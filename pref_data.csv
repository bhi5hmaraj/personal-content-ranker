,chosen,rejected
0,"

Why Planning is Hard: A Multifaceted Model
 - LessWrong 2.0 viewerArchiveSequencesAboutSearchLog InQuestionsEventsShortformAlignment ForumAF CommentsHomeFeaturedAllTagsRecent CommentsWhy Planning is Hard: A Multifaceted Model
Ruby31 Mar 2019 2:33 UTC29 points9 commentsLW linkPlanning & Decision-MakingPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsContentsContentsWhat is plan­ning? Plan­ning is a Pre­dic­tion/​In­for­ma­tion ProblemPlan­ning is a Com­pu­ta­tion ProblemPlan­ning is a Self-Knowl­edge & Self-Mastery ProblemPre­dict­ing yourselfKnow­ing what you wantSelf-mas­tery of your hu­man brainHeuris­tics and biasesUs­ing Sys­tem 1 (in­tu­ition) and Sys­tem 2 (ex­plicit rea­son) in harmonyEmo­tional MasteryPlan­ning is a Re­cur­sive ProblemSum­mary: What It Takes to Be a Great PlannerEndnotesEpistemic con­fi­dence: Highly con­fi­dent. This post doesn’t cover ev­ery­thing rele­vant to the topic, but I am con­fi­dent that ev­ery­thing pre­sented is solid.You may have no­ticed that plan­ning can be rather difficult. Granted, not all plans are difficult, plan­ning what you’re go­ing to have for din­ner isn’t too bad usu­ally; how­ever se­ri­ous plan­ning can range from merely daunt­ing to seem­ingly in­tractable. Think of the challenge of plan­ning to­wards a satis­fy­ing ca­reer, a fulfilling re­la­tion­ship, the suc­cess of your startup, or the sim­ple preser­va­tion and flour­ish­ing of hu­man civ­i­liza­tion. I pre­sent here a gears-level, re­duc­tion­ist ac­count of plan­ning which makes it starkly clear why plan­ning is hard. The point not be­ing that we should give up be­cause plan­ning is fu­tile. Far from it. With a de­tailed model of the fac­tors which make plan­ning hard, we can de­rive a unified roadmap for how to get bet­ter at plan­ning and ul­ti­mately have a unified, pow­er­ful ap­proach to mak­ing bet­ter plans.ContentsWhat is plan­ning?Plan­ning is a pre­dic­tion/​in­for­ma­tion problemPlan­ning is a com­pu­ta­tion problemPlan­ning is a self-knowl­edge and self-mas­tery problemPlan­ning is a re­cur­sive problemSum­mary: what it takes to be a good plannerAp­pendix: math­e­mat­i­cal for­mal­ism of the com­pu­ta­tion problemWhat is plan­ning? We can’t talk about why plan­ning is hard be­fore clar­ify what plan­ning is. It’s pretty sim­ple:Plan­ning is the se­lec­tion of ac­tions in or­der to achieve de­sired out­comes. [1] The need for plan­ning arises when the fol­low­ing con­di­tions hold:There are mul­ti­ple states that the world can be in.You have prefer­ences over which states the world is in.There are mul­ti­ple ac­tions available to you, each of which might cause the world to more likely be in some states rather than oth­ers.You can­not take all of the ac­tions, ei­ther be­cause you lack the re­sources to do so or be­cause the ac­tions are in­her­ently ex­clu­sive.Th­ese con­di­tions lead to a re­fine­ment of the defi­ni­tion above: plan­ning is the se­lec­tion of ac­tions from among com­pet­ing al­ter­na­tives. One must pri­ori­tize among one’s available op­tions and al­lo­cate re­sources to whichever op­tions rank most highly. Often one is al­lo­cat­ing re­sources among mul­ti­ple op­tions in pro­por­tion to their pri­or­ity, but I think we can still rightly call it pri­ori­ti­za­tion even when one is only de­cid­ing be­tween al­lo­cat­ing 100% of their re­sources to only one op­tion of out two. In other words, all of plan­ning is pri­ori­ti­za­tion. [2]Plan­ning is a Pre­dic­tion/​In­for­ma­tion ProblemOn what ba­sis should we se­lect or pri­ori­tize one ac­tion over an­other? Of course, we should se­lect whichever ac­tions we most ex­pect to lead us to the wor­lds we most pre­fer. We should choose ac­tions based on the Ex­pected Value (EV) we as­sign to them.And therein lies one of the core challenges of plan­ning. Ex­pect­ing is just an­other word for pre­dict­ing. And pre­dict­ing is just a word which means as­sign­ing each ac­tion a prob­a­bil­ity dis­tri­bu­tion over the world states it will re­sult in. That’s hard to do well.Plan­ning is hard be­cause pre­dict­ing is hard. Of course, pre­dict­ing is a lot eas­ier when you have more in­for­ma­tion, but usu­ally we have far less than we’d like, so plan­ning is hard be­cause of limited in­for­ma­tion. Plan­ning is a pre­dic­tion prob­lem and an in­for­ma­tion prob­lem. Peo­ple of think of plan­ning as be­ing about “do­ing”, but in truth plan­ning is just as much about “know­ing”. One thing this amounts to is that the in­stru­men­tal ra­tio­nal­ity in­volved in plan­ning is in­sep­a­rable from the epistemic ra­tio­nal­ity of hav­ing true be­liefs, good mod­els, and mak­ing ac­cu­rate pre­dic­tions. Any plan­ner’s abil­ity is go­ing to be capped by their epistemic skill.What we can do with this re­al­iza­tion is that in any situ­a­tion where we’re plan­ning, we can pay de­liber­ate at­ten­tion to the pre­dic­tions we need to make, the in­for­ma­tion we have, and ways in which might be able to make bet­ter pre­dic­tions. (I call this the In­for­ma­tion Con­text of a plan.) Alter­na­tively stated, one can be­gin to ap­proach plan­ning with an un­cer­tainty re­duc­tion mind­set [3]. Rather than al­lo­cat­ing all of one’s re­sources from the out­set, one in­stead al­lo­cates a por­tion of the re­sources to­wards “pur­chas­ing” in­for­ma­tion which im­proves the sub­se­quent al­lo­ca­tion of the rest via im­proved pre­dic­tion. We do this already in many cases (read­ing re­views, ask­ing friends, tri­als), but the oc­ca­sions where we fail to do this can cost us big time: the stu­dent who didn’t re­search be­fore start­ing law school, the com­pany that spent years de­vel­op­ing a product be­fore test­ing with users, the cou­ple who rushed into mar­riage, etc. (Not the only cause, but emo­tions of­ten make us im­pa­tient to go all out with an op­tion with­out ad­e­quate con­sid­er­a­tion.)Un­for­tu­nately, while it’s easy to say that get­ting more in­for­ma­tion and re­duc­ing un­cer­tainty is a good thing, there’s still a lot of com­plex­ity in man­ag­ing un­cer­tainty. Know­ing how much is okay and how to effi­ciently re­duce it. Still, always re­mem­ber­ing that pre­dic­tion is a core challenge of plan­ning is a start.Plan­ning is a Com­pu­ta­tion ProblemUn­for­tu­nately, even hav­ing all the right in­for­ma­tion and perfect pre­dic­tion would not be enough to make plan­ning easy. Even when one can perfectly pre­dict the out­come of all al­ter­na­tive op­tions, plan­ning of­ten gives rise to in­tractably large com­pu­ta­tional prob­lems which are NP-Com­plete. (I provide a small math­e­mat­i­cal treat­ment in this com­ment be­low.)We are usu­ally al­lo­cat­ing a finite set of re­sources (our time, our money) amount a set of op­tions in or­der to ac­com­plish a va­ri­ety of goals. Ex­am­ple: I might care about my health, en­ter­tain­ment, ca­reer, friend­ships, ro­mance, art, and ed­u­ca­tion. Towards these val­ues, I could in a given week: sleep, play ten­nis, go to the gym, watch Net­flix, work late, call up by bestie, go on Tin­der, draw a pic­ture, read a text­book, etc. I get to al­lo­cate my time to some com­bi­na­tion of ac­tivi­ties, but the thing is, the num­ber of pos­si­ble com­bi­na­tions for al­lo­cat­ing my time in a sin­gle week is mind-bog­gling. A su­per-sim­plified ex­am­ple: I have 30 dis­cre­tionary hours in a week and 10 pos­si­ble ac­tivi­ties I could spend each of those hours on, but I only spend time on ac­tivi­ties in two-hour blocks. This re­sults in 10^(30/​2) = 10^15 = one thou­sand trillion differ­ent com­bi­na­tions of how I could spend my time that week. Even if I could perfectly pre­dict how much I’d like each com­bi­na­tion of time us­age, I could never con­sider them all ex­plic­itly. This com­pu­ta­tional com­plex­ity was very salient to me in my last job as a Product Man­ager where ev­ery six weeks I would choose which pro­jects my team of Data Scien­tists would work on. Un­der ideal cir­cum­stances, I might get to choose ten pro­jects out of a can­di­date twenty pro­jects. Choos­ing was hard, es­pe­cially since in ad­di­tion to pure value of each pro­ject, I had to jug­gle the facts that: a) pro­jects were of­ten not in­de­pen­dent, b) pro­jects of­ten have fu­ture costs, e.g. main­te­nance and tech debt, c) there are costs to not do­ing or de­lay­ing some pro­jects, d) the benefits of differ­ent pro­jects are spread over differ­ent time hori­zons, e) pro­jects aren’t com­men­su­rate, e.g. pro­tect­ing down­side risk vs build­ing new func­tion­al­ity, f) some pro­jects are nec­es­sary to pre­serve fu­ture op­tion­al­ity, g) bud­get con­straints were soft, I could some­times steal time from el­se­where it meant I could se­lect a bet­ter set of pro­jects.Even if I’d had perfect in­for­ma­tion and pre­dic­tion, which I cer­tainly didn’t, the num­ber of pos­si­bil­ities was large enough to be in­tractable by any an­a­lyt­i­cal method of pri­ori­ti­za­tion.I am least cer­tain about how to best tackle the com­pu­ta­tion prob­lem of plan­ning, but my cur­rent guess is that very much re­lies on effec­tively us­ing our in­stinc­tive, in­tu­itive, Sys­tem 1 think­ing in con­junc­tion with our Sys­tem 2 think­ing. Sys­tem 1 is bet­ter at han­dling prob­lems too large to be con­sciously con­sid­ered, while Sys­tem 2 can en­sure that Sys­tem 1 is pay­ing at­ten­tion to all the rele­vant con­sid­er­a­tions. I like cost-benefit analy­ses and de­ci­sion-ma­tri­ces not be­cause I think they should make the fi­nal de­ci­sion, but be­cause I think the ex­er­cise of cre­at­ing them loads the right in­for­ma­tion into Sys­tem 1.Plan­ning is a Self-Knowl­edge & Self-Mastery ProblemIn a three-fold man­ner, plan­ning is a prob­lem of self-knowl­edge and self-mas­tery.Pre­dict­ing yourselfThe first prob­lem of plan­ning stated was that of pre­dic­tion and in­for­ma­tion. With the ex­cep­tion of plans made for teams, or­ga­ni­za­tions, coun­tries, and the like, your plans will con­cern your­self and your ac­tions. That means that in­vari­ably one of the most im­por­tant things to be able to pre­dict is your­self. Some­times this is hard be­cause we lack in­for­ma­tion. It might be hard to pre­dict how you will be­have or feel in novel cir­cum­stances. Other times it can be hard to pre­dict our­selves be­cause we are averse to mak­ing the best pos­si­ble pre­dic­tions we could. We re­fuse to ad­mit that re­al­is­ti­cally we are not go­ing to get up at 6am to go to the gym.A par­tic­u­lar as­pect of your­self which it is key to be able to pre­dict is your mo­ti­va­tion. Plans which rely on un­re­al­is­tic pre­dic­tions about how mo­ti­vated you will feel while ex­e­cut­ing the plans are prob­a­bly go­ing to be un­suc­cess­ful plans.Ad­vice I have for this as­pect of plan­ning is a) start pay­ing at­ten­tion to your­self and to pre­dict­ing your­self, for­tu­nately usu­ally you have a lot of data to work with, b) adopt a policy of rad­i­cal-self hon­esty, what is true is already so and good pre­dic­tions are the ba­sis of good plans.Know­ing what you wantThis post started with the defi­ni­tion that plan­ning is the se­lec­tion of ac­tions to­wards de­sired out­comes. It is rather im­por­tant that you cor­rectly iden­tify your ac­tual de­sired out­comes, oth­er­wise, any ac­tions you se­lect aren’t likely to be worth much. “I thought I wanted X ac­tu­ally but I didn’t” is too of­ten the foil of a sup­pos­edly suc­cess­ful plan where the plan­ner ev­i­dently lacked good self-pre­dic­tion.I’m of the be­lief that one’s deep-down de­sired out­comes (per­sonal and moral) are con­tained in­side one’s brain, re­gard­less of whether one has achieved good con­scious ac­cess to them or in­ferred all the con­se­quences of them. Much of good plan­ning is re­duc­ing un­cer­tainty around what is that you ac­tu­ally want and value. Self-pre­dic­tion is a spe­cial case of the gen­eral pre­dic­tion/​in­for­ma­tion prob­lem, but it re­quires differ­ent tech­niques of un­cer­tainty re­duc­tion than out­side-world pre­dic­tion does. In­tro­spec­tive meth­ods such as med­i­ta­tion, Fo­cus­ing, In­ter­nal Fam­ily Sys­tems, etc. are helpful for hav­ing bet­ter knowl­edge of which out­comes you ac­tu­ally want.Of course, com­pli­cat­ing the dis­cov­ery of what your de­sired out­comes are is that the no­tion of you may be a lit­tle com­pli­cated. I have been per­suaded over the years that it can be very use­ful to think of your­self as be­ing made up of parts or sub-agents, each with their own par­tic­u­lar de­sires. Mas­ter­ing your­self means com­ing to un­der­stand your parts and their de­sires lead­ing into an abil­ity to make plans that satisfy all your­self. This can be cru­cial as mak­ing plans which parts of you are not on­board of­ten causes those parts to un­der­mine the plan.Re­lat­edly, “mak­ing your­self do some­thing” should always be a warn­ing sign that some part of you is be­ing dis­re­garded. Some­times that’s le­gi­t­i­mate, but re­ally only if you’re ac­count­ing for it in your self-pre­dic­tions and how it’s go­ing to af­fect over­all suc­cess. Self-mas­tery of your hu­man brainHu­man brains are re­ally, re­ally good, but they’re also re­ally com­pli­cated with a lot of differ­ent mov­ing pieces and no user man­ual. Most of us man­age, but there are gains to be had from bet­ter at op­er­at­ing our own minds.Heuris­tics and biasesTo get bet­ter at pre­dict­ing, it helps to un­der­stand when our brains na­tively do and don’t make good pre­dic­tions of their own ac­cord. That leads di­rectly into heuris­tics and bi­ases and at­tempts to be­come a lens which sees its flaws.Us­ing Sys­tem 1 (in­tu­ition) and Sys­tem 2 (ex­plicit rea­son) in harmonyHu­man brains run on both and each sys­tem has its ad­van­tages and dis­ad­van­tages. The hu­man who is lev­er­ag­ing their brain to its ful­lest ex­tent is us­ing all of their mind in har­mony, not priv­ileg­ing or dis­re­gard­ing one kind of think­ing in­ap­pro­pri­ately. This hard though, but re­ally a key part of get­ting good at plan­ning. Emo­tional MasteryAr­guably emo­tions could be lumped un­der Sys­tem 1, but it’s worth call­ing them out sep­a­rately. While emo­tions serve mul­ti­ple pur­poses, one of the things they do is carry an im­por­tant sig­nal of in­for­ma­tion from your sub­con­scious mind. If you feel anx­ious about some­thing or hav­ing a nig­gling doubt or what­ever, that’s be­cause there’s a part of your mind has been pro­cess­ing raw data and find­ing it sig­nifi­cant. The skil­led plan­ner and pre­dic­tor will be some­one who can ex­tract that valuable in­for­ma­tion from their emo­tions.How­ever, emo­tional mas­tery is per­haps even more im­por­tant to plan­ning in an­other way. Many, many plans are bad plans be­cause peo­ple lapse into op­ti­miz­ing for their emo­tions rather than ac­tual long-term de­sired out­comes. A per­son who rushes into a plan with in­ad­e­quate in­for­ma­tion be­cause they dis­like be­ing in a state of in­de­ci­sion is likely choos­ing a worse plan be­cause they were un­able to han­dle their un­pleas­ant emo­tions. Or a per­son who only ex­e­cutes ex­tremely con­ser­va­tive plans be­cause any­thing bold makes them too feel anx­ious. I be­lieve schools of thought which help with emo­tional mas­tery such as ACT, CBT, and DBT have a place among the train­ing ma­te­ri­als for great plan­ners.Plan­ning is a Re­cur­sive ProblemWe’ve cov­ered that plan­ning re­quires iden­ti­fy­ing your op­tions, pre­dict­ing their out­comes, eval­u­at­ing the good­ness of said out­comes, and then some­how crunch­ing through all the differ­ent com­bi­na­tions of pos­si­ble ac­tions to se­lect the best over­all set. Ex­cept plan­ning is re­cur­sive. Any difficult plan is go­ing to be com­posed of mul­ti­ple sub-plans and for each one of those sub-plans the whole pro­cess needs to re­peated again: iden­ti­fy­ing op­tions, mak­ing pre­dic­tions, se­lect­ing com­bi­na­tions, etc.First, this adds to the already ex­ten­sive amount of com­pu­ta­tion in­volved in plan­ning. Se­cond, it can of­ten cause plans to span mul­ti­ple do­mains strain­ing all but the most ver­sa­tile gen­er­al­ists. Have pity for the as­piring baker whose plan was to make the best muffins in town, but now has to figure out dou­ble-en­try ac­count­ing and US tax code. Or the physi­cist-cum-founder who wanted to cre­ate clean en­ergy tech­nolo­gies but now needs to figure out a sales and mar­ket­ing strat­egy too.At a base level, the challenges of good plan­ning re­main con­stant be­tween do­mains, e.g. mak­ing good pre­dic­tions, as well the ad­vis­able steps, e.g. re­duc­ing un­cer­tainty. Yet the con­crete steps for do­ing these might look very differ­ent. The physi­cist-founder might have been very good at re­duc­ing un­cer­tainty in the lab­o­ra­tory with ex­per­i­ments, but the feed­back loop for iter­at­ing on sales strat­egy might be com­pletely differ­ent, even if in both cases you’re try­ing to re­duce un­cer­tainty.Shift­ing be­tween do­mains is prob­a­bly a rea­son why peo­ple who are good at re­duc­ing un­cer­tainty and plan­ning in some do­mains are neg­li­gent in oth­ers. Ar­guably, the skil­led plan­ner is go­ing to figure out the right spe­cific tech­niques to achieve good plans across all the do­mains they touch. In some do­mains, you might have lots of ex­perts you can poll; in oth­ers it might be cheap to run ex­per­i­ments; in some you can build good ex­plicit mod­els; in oth­ers it’s all about train­ing in­tu­ition; and yet in fur­ther cases it might not be easy at all and you’re left try­ing to draw ten­u­ous in­fer­ences from his­tor­i­cal ex­am­ple.Sum­mary: What It Takes to Be a Great PlannerWe can neatly sum­ma­rize why plan­ning can be so difficult with a list of all the traits one should have in or­der to over­come the difficul­ties.One must be ex­cel­lent at mak­ing pre­dic­tions across do­mains, helped by their mas­tery of epistemic skill and virtue.One must be skil­led at iden­ti­fy­ing which in­for­ma­tion one needs yet lacks and at ex­e­cut­ing sub-plans to effi­ciently ob­tain this in­for­ma­tion across differ­ent do­mains and the dis­parate tech­niques re­quired to do so.One must be skil­led at mak­ing choices within com­bi­na­to­ri­ally ex­plo­sive prob­lems in­volv­ing the se­lec­tion of com­bi­na­tions of mul­ti­ple op­tions to­wards mul­ti­ple goals via the use of heuris­tics, ex­cel­lent in­tu­ition/​in­stinct, or some other re­li­able method.One must be able to make use of their Sys­tem 1 and Sys­tem 2 to think in har­mony.One must have knowl­edge of their true val­ues, goals, and de­sired states of the world.One must have knowl­edge of their sub-parts and al­ign­ment be­tween them so that one does not un­der­mine one­self.One must be able to pre­dict one’s own be­hav­ior in­clud­ing the be­hav­ior of one’s mo­ti­va­tions and emo­tions so that one can plan effec­tively around them.One must be able to re­duce un­cer­tainty about one’s self-model via ex­per­i­ments or in­tro­spec­tive tools.One must be able to listen to the in­for­ma­tion con­tained in their emo­tions.One must not let their emo­tions co­erce them into plans which sate their emo­tions while sac­ri­fic­ing their over­all goals.One must be able to plan effec­tively up and down the re­cur­sive tree of their plans: enu­mer­at­ing, eval­u­at­ing, and re­duc­ing un­cer­tainty wher­ever needed.Endnotes[1] This broad defi­ni­tion stands in op­po­si­tion to a com­mon defi­ni­tion which uses plan­ning pri­mar­ily in con­texts of schedul­ing, e.g. plan your day, plan your week. The broader defi­ni­tion here is es­sen­tially syn­ony­mous with de­ci­sion-mak­ing, per­haps differ­ing only in con­no­ta­tion. De­ci­sions some­what im­ply a one-off choice be­tween op­tions whereas plan­ning im­plies se­lec­tion of mul­ti­ple ac­tions to be taken over time. The term plan­ning also some­what more than de­ci­sion-mak­ing high­lights that there is a goal one wishes to achieve.[2] I of­ten hear peo­ple faced with clear pri­ori­ti­za­tion tasks plead that they need more re­source, that re­source scarcity is the prob­lem. Some­times it is, some­times the best ac­tion is to get more re­sources. But of­ten it is a fal­lacy to think that if you can’t solve pri­ori­ti­za­tion now, it will some­how be eas­ier when you have more re­sources. How­ever many re­sources you have, they will be finite and you will still be able to think of things you’d like to do with even more re­sources, things which feel just as nec­es­sary. In short, you should get used to choos­ing sooner rather than later. [3] To be tech­ni­cal, un­cer­tainty re­duc­tion is about con­cen­trat­ing the prob­a­bil­ity mass of your prior dis­tri­bu­tion into nar­rower bands. What links here?On the Na­ture of Agency
 by Ruby (1 Apr 2019 1:32 UTC; 31 points)How to make plans? by Pee Doom (23 Apr 2019 8:29 UTC; 24 points)Ruby31 Mar 2019 2:33 UTC29 points9 commentsLW linkPlanning & Decision-MakingPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsRuby 31 Mar 2019 2:23 UTC 9 pointsAp­pendix: For­mal­ism of the Com­pu­ta­tion ProblemA sim­ple for­mal­ism illus­trates that plan­ning quickly be­comes com­pu­ta­tion­ally in­tractable. Bor­row­ing from Lee Merkhofer’s  Math­e­mat­i­cal The­ory for Pri­ori­tiz­ing Pro­jects and Op­ti­mally Allo­cat­ing Cap­i­tal. As­sume there are m po­ten­tial pro­jects. For now, as­sume that the pro­jects are in­de­pen­dent; that is, it is rea­son­able to se­lect any com­bi­na­tion of pro­jects and the cost and value of any pro­ject do not de­pend on what other pro­jects are se­lected. Define, for each pro­ject i = 1, 2,..., m the zero-one vari­able xi. The vari­able xi is one if the pro­ject is ac­cepted and zero if it is re­jected. Let ci be the in­cre­men­tal value (b for “benefit”) of the i’th pro­ject and ci be its cost. Let C be the to­tal available bud­get. The goal is to se­lect from the available pro­jects the sub­set of pro­jects with a to­tal cost less than or equal to C that pro­duces the great­est pos­si­ble to­tal value.The prob­lem may be ex­pressed math­e­mat­i­cally as:Max­i­mize m∑i=1bixiSub­ject to: m∑i=1cixi≤C and xi= 0 or 1 for i = 1, 2,...,m.This is a zero-one in­te­ger op­ti­miza­tion prob­lem. It is NP-Com­plete, i.e. the time re­quired to solve such a prob­lem us­ing any cur­rently known al­gorithm in­creases rapidly as the size of the pro­gram grows. Nat­u­rally, be­cause al­lo­cat­ing re­sources/​plan­ning is in­volves com­bi­na­tions of ac­tions and com­bi­na­tions tend to ex­plode. It can be okay if there the num­ber of pos­si­ble ac­tions/​pro­jects is rel­a­tively small, but re­mem­ber that even 10! is already 3.6 mil­lion. The equa­tion above isn’t com­pre­hen­sive enough to cap­ture the full de­tail of real-world plan­ning, but it should suffice to in­di­cate that plan­ning is of­ten of the com­bi­na­to­ri­ally ex­plo­sive class. (If you want to see how more fac­tors can be in­cluded, see the rest of Merkhofer’s pa­per where he mod­els mu­tu­ally ex­clu­sive/​se­quen­tial pro­jects, multi-pe­riod plan­ning, and sen­si­tivity to de­lay of pro­jects.)Note how­ever that this treat­ment as­sumes that the benefits and costs are perfectly known when perform­ing the op­ti­miza­tion. In the real world, we only have dis­tri­bu­tions over the benefits and costs. A true for­mal­ism of real-world pri­ori­ti­za­tion would be couched in statis­ti­cal terms. Plus, the benefits and costs in the above for­mal­ism are scalars which can be added and com­pared, e.g. dol­lars. In the real world, the benefits and costs we weigh are of dis­parate types which at best have vague con­ver­sion rates be­tween them. So you might imag­ine that a com­pre­hen­sive for­mal­ism would deal in vec­tors and would in­clude a com­pli­cated func­tion for com­par­ing those vec­tors.The point here is not that we should at­tempt to cre­ate or use math­e­mat­i­cal mod­els in our plan­ning, but to rec­og­nize that it is pre­cisely this math which our brains must find some way of crunch­ing. Un­der­stand­ing that this is the im­mense prob­lem we are tasked with, we can start to look for ways to han­dle it bet­ter than our de­fault. And, you know, also give our­selves a bit of break when we find plan­ning hard.What links here?Why Plan­ning is Hard: A Mul­ti­faceted Model
 by Ruby (31 Mar 2019 2:33 UTC; 29 points)Rohin Shah 31 Mar 2019 22:08 UTC 6 pointsParentThis is a zero-one in­te­ger op­ti­miza­tion prob­lem. It is NP-CompleteNit­pick: Just be­cause a prob­lem can be for­mal­ized as a zero-one in­te­ger op­ti­miza­tion prob­lem doesn’t mean it’s NP-com­plete; you need to show that some NP-com­plete prob­lem re­duces to the plan­ning prob­lem. For ex­am­ple, the prob­lem of find­ing the largest num­ber in a set {ni} is a zero-one op­ti­miza­tion prob­lem (it can be for­mal­ized as max∑ixini sub­ject to ∑ixi=1 with each xi be­ing zero or one) but it isn’t NP-com­plete.That said, the prob­lem you speci­fied is iden­ti­cal to the knap­sack prob­lem, which is known to be NP-com­plete, so your point stands.Ruby 31 Mar 2019 22:11 UTC 6 pointsParentThat’s a fair nit­pick, thanks. I was aware it was iden­ti­cal to the knap­sack prob­lem, though I do see that my phras­ing im­plied that be­ing a zero-one in­te­ger op­ti­miza­tion prob­lem au­to­mat­i­cally makes it NP-Com­plete. That was sloppy of me.Swapna Rao 23 Dec 2019 4:46 UTC 5 pointsThis is so helpful. Thank you.
Rohin Shah 31 Mar 2019 22:10 UTC 3 pointsOne other as­pect I would in­clude is that even figur­ing out the set of ac­tions available to you can be difficult. This post seems to be ar­gu­ing that “think­ing in­side the box” is already hard; I think figur­ing out how to “think out­side the box” is also both im­por­tant and hard.Also, you’ll prob­a­bly en­joy Re­search as a Stochas­tic De­ci­sion Pro­cess.What links here?On the Na­ture of Agency
 by Ruby (1 Apr 2019 1:32 UTC; 31 points)Ruby 31 Mar 2019 23:25 UTC 1 pointParentThat’s very true. I need to think through that more and figure out how to in­cor­po­rate into my mod­els. I think there’s a lot there which is miss­ing from here.NaiveTortoise 31 Mar 2019 18:15 UTC 2 pointsThe dis­cus­sion of plan­ning across do­mains seems to ig­nore the fact that of­ten the best solu­tion to plan­ning a pro­ject in a do­main with which you’re not fa­mil­iar is to hire/​get help from an­other per­son. Of course, once you turn plan­ning into a multi-per­son ac­tivity, which I think any com­pre­hen­sive model of plan­ning should treat it as, you also need to fac­tor in un­cer­tainty about oth­ers’ plans, which com­pli­cates the model quite a bit.
avturchin 31 Mar 2019 10:58 UTC 1 pointEach plan re­al­i­sa­tion con­sists of sev­eral stages, which are similar in very differ­ent types of tasks:1) Plan­ning and data gath­er­ing about differ­ent tasks.2) Prepa­ra­tion: buy­ing in­stru­ments, col­lect­ing data for this task, reg­ister on Tin­der etc.3) Creat­ing “first draft”. Writ­ing the draft, build­ing a pro­to­type, try­ing to go to the first date.4) Perfect­ing the product. Edit­ing, test­ing with users, many dates.5) “Sel­ling it” and get­ting the out­put. E.g: get­ting ar­ti­cle pub­lished and cited, startup be­comes uni­corn, or sta­ble re­la­tion. Sel­ling means that the out­put of the pro­ject be­comes use­ful for some other pro­jects, not nec­es­sary money or re­la­tion with other peo­ple. 6) End­ing. It is the mo­ment when you press stop but­ton (or you be­come in­ter­nally “peper­clipy” by pro­duc­ing more and more thing which you do not need already). For ex­am­ple, you need to stop dat­ing if you get wife. Stop­ping is not easy, as we tend to do the same things again and again. Stop­ping is es­pe­cially difficult if the pro­ject fail and I have an op­tion: try more or stop try­ing.  The most difficult here is the 5th step, sel­l­ing – this is there plans tend to fail. Be­cause on the first 4 lev­els I just spend re­sources and mea­sure the progress by my in­ter­nal met­rics. At the end I fi­nally com­pare it with the out­side world, which could be just my big­ger pro­ject. avturchin 31 Mar 2019 10:35 UTC 1 pointPlans (or tasks) could be pre­sented as black­boxes, which con­sume X and out­put Y.X is:moneytimeother ma­te­rial or im­ma­te­rial re­sources (con­tacts, ob­jects)in­for­ma­tion about situationplan of actionop­por­tu­nity costY is a de­sired out­put, which con­sists of:The di­rect planned out­put which is a re­source for other tasks (money, ca­pa­bil­ities, knowl­edge, effect on other peo­ple opinion about my product). There is im­por­tant differ­ence here be­tween 90 per cent of re­sult and 100 per cent. 90 per cent is a situ­a­tion when you wrote an email but didn’t send it. The best out­come of a plan is 120 per cent re­sult. My emo­tions and stress in the pro­cess of the im­ple­men­ta­tion of the plan.Col­lat­eral out­put: what I will learn while im­ple­ment­ing the plan, and which other use­ful things I can do dur­ing it. 120-per-cent-re­sult is partly based on get­ting many good sec­ondary goals achieved si­mul­ta­neously. For ex­am­ple, I will not only go to a con­fer­ence, but also will visit my friends in the same city, prac­tice my com­mu­ni­ca­tion skills and find the next con­fer­ence on the topic. Risks: what very bad things could hap­pen? Ac­ci­dents, theft. Risks are not de­sired, but they are part of the out­put.Back to top","






A research workflow with Zotero and Org mode | mkbehr.com



















Skip to main content





Toggle navigation





mkbehr.com






About me


Archive


Tags


RSS feed


Github














Source










A research workflow with Zotero and Org mode


                    Michael Behr
            
September 19, 2015

Comments

Source






Any research project is going to involve a literature search: reading
through a bunch of papers that might be relevant to your topic in
order to get a sense of what the field already knows. Now, maybe
there's some magic technique for picking out the information that
matters, passing over the rest, and writing out a single, coherent
story in one pass through all the papers you can find. If that
technique exists, I have no idea what it is.
So when every paper brings up ten new questions and twenty papers to
start answering them, I need a system to keep my notes organized. I
need notes that let me jump back and forth between papers without
losing my place, draw links between papers, and store lists of
citations to come back to. Here's how I do it.

Storing papers with Zotero
The first tool I use is Zotero, a reference
manager. Zotero's job is to store all the actual papers I come across,
along with information like data on how to cite the papers and any
tags they might have been published with. It can grab that information
from my web browser, whether from a journal's website or someplace
like Google Scholar or PubMed. It's also great for quickly putting
together a bibliography, using bibtex or similar programs, when I want
to write up some results.



Zotero stores the papers I want to
read and reference. I scaled up the font size here to make it readable
in a tiny blog image.
Zotero isn't the only choice for reference managers.
Mendeley is another popular choice, and
there are a
whole bunch more
out there. I picked Zotero arbitrarily a few years ago, but it's
working out well because of its emacs integration.
Keeping notes with Emacs and Org mode
You see, Zotero has some note-taking functions, and I used to keep my
notes there, but there were some problems. Notes are stored as
separate files for each paper, but I want to cross-reference notes
from a lot of different papers at once. And while the editor has some
rich-text capabilities (e.g. bold and italic text), it's missing
important things I need in my notes, like the ability to typeset
equations.
That's where Emacs and its
extension Org mode come in. To borrow a term
from Perl enthusiasts, Org mode is the swiss army chainsaw of text
document formats. Org mode documents have a lot of features, and it's
way beyond this post's scope to describe them all. For the purpose of
research notes, the most useful things it lets me do are:

I can store my notes in a hierarchical tree structure, and I can
  hide parts of the tree from view in order to focus on other parts.
I can put hyperlinks into my notes, including links to papers,
  websites, or other parts of the file.
I can put math in my notes using Latex, and view the typeset
  equations right in my Emacs buffer.




A sample from my notes file. You
can see the tree structure of the file, some links to papers, and a
little bit of inline math, using Latex.
Gluing it all together with zotxt
Now, see those links to papers in my notes buffer? I didn't have to
copy and paste them from anywhere. I inserted them with just three
keystrokes each. So far, I've just described some useful pieces of
software, but the interesting part of my workflow is how they fit
together.
zotxt is an extension that lets
other programs talk to Zotero, and Emacs has a package to talk to it.
It's even structured specifically to work with Org mode documents.
With zotxt, my workflow looks like this:

I find a paper I want to look at somewhere on the internet.
I use Zotero's browser plugin to save it to Zotero. Hopefully it
  grabs the paper itself and this happens in one click; if the site
  doesn't play along, I spend a minute grabbing a pdf and feeding it
  to Zotero.
I insert a link to the Zotero entry into my notes file in Emacs. I
  can do this with the key chords C-c "" "". I don't need to further
  specify what paper I want to grab: the browser plugin leaves the
  paper selected in Zotero, and zotxt can grab the selected paper.
When I want to read the paper, I go to the link and tell Emacs to
  open the paper in my system PDF viewer. The key chords for this are
  C-c "" a, and then selecting the PDF attachment from the Helm
  window that appears (usually I just type pdf RET).
When I'm reading a paper and see a citation that might be useful, I
  look it up on the internet and repeat this process to store a note
  linking to it.

It took me a while to get it set up to my liking, so here's how I did
it:

First, install zotxt. If you're
  using Zotero as a firefox extension, you just need to install zotxt
  as another extension. If you're using the standalone Zotero client,
  you can still do it: download the extension file from that link,
  then go to the Add-Ons Manager under the Tools menu and find the
  option to install an add-on from a file.




The menu option looks like
this.

Next, install the zotxt package in emacs. If your
  package manager is set up, you
  can just type M-x package-install RET zotxt RET.
Now, when org-zotxt-mode is active, you can use its functions in
  your org-mode buffers. You can search for papers and insert them
  with C-c "" i, insert the currently-selected paper in Zotero with
  C-u C-c "" i, and open a paper's PDF or other related files by
  moving the cursor to a link and typing C-c "" a. However, you might
  want a little bit more setup to deal with some annoyances.
You probably want to have org-zotxt-mode automatically activated
  in all your org-mode documents. To make that happen, you can add
  some code to your .emacs file to start up this mode on all your
  org-mode buffers - see below this list for the .emacs
  configuration I use.
If you want to insert a link to the currently-selected item a lot,
  C-u C-c "" i is an awkward sequence to type. I rebound it to C-c ""
  "".
You might notice that when you insert a link to a paper, the text of
  that link is a full citation. That might be what you want, but I
  just want the authors, paper name, and year. It took me a bit of
  hacking to get around that: it's possible to tell the emacs zotxt
  interface to use a different citation format than the default, but I
  had to throw together a little XML file to give it a shorter format
  than a full citation. (This may not be the easiest or cleanest way
  to do it, but it works!)
  That XML file is here. To use it, go into
  your Zotero preferences and select Cite -> Styles, and add the file.
  It should appear in the menu as ""mkbehr's short reference format"".
  Then add the last two lines in the .emacs snippet below, and you
  should get shorter citations.
You probably want to install the
  Helm package, to make zotxt's
  search interface easier to navigate. That link should tell you
  everything you need to know.

Here's that .emacs setup code:
;; Activate org-zotxt-mode in org-mode buffers
(add-hook 'org-mode-hook (lambda () (org-zotxt-mode 1)))
;; Bind something to replace the awkward C-u C-c "" i
(define-key org-mode-map
  (kbd ""C-c \"" \"""") (lambda () (interactive)
                      (org-zotxt-insert-reference-link '(4))))
;; Change citation format to be less cumbersome in files.
;; You'll need to install mkbehr-short into your style manager first.
(eval-after-load ""zotxt""
'(setq zotxt-default-bibliography-style ""mkbehr-short""))

Of course, I'm not done tinkering to make my workflow better. I hear
good things about the org-ref
and helm-bibtex
packages - if only I can keep an up-to-date bibtex file as I add papers
to my library, I can associate links with not only a paper's pdf, but
also that paper's section of my notes file. And I haven't found a
smooth way to take a paper and pull up the papers it cites in my
browser. But until then, I'm pretty happy with this setup.
Happy researching!



emacs
research



Previous post


Next post

Comments

Please enable JavaScript to view the comments powered by Disqus.

Comments powered by Disqus




            Contents © 2015         Michael Behr - Powered by         Nikola






"
1,"  Synchronous and Asynchronous Data Transmission | Computer Science                                            Skip to content      Menu  Home Membership Sign up Contact  About   Login           Teach Computer Science         Menu  KS3 Resources  KS3 Theory Topics KS3 Algorithms Topics KS3 Programming KS3 Networking KS3 Data Representation KS3 Connecting clients & server KS3 Databases Resources All KS3   GCSE Resources  GCSE Python Course  Resources  Python Basic Output Tutorial Python Variables Tutorial Python Basic Input Tutorial Python Calculations Tutorial Python Control Flow Tutorial Python Readability Tutorial Python Loops (For) Tutorial Python Lists Tutorial Python Functions Tutorial Python While Loops Tutorial Python Strings Tutorial Python Files Handling Tutorial     Theory Topics Computer Architecture Computer Hardware Data Representation Memory and Data Storage Networking Computer Security and Ethics Algorithms Programming Databases All GCSE   A-Level Resources  A Level Python Course A Level Contemporary Processes A Level Software & Development A Level Exchanging Data A Level Data Types & Structures A Level Legal & Moral Issues A Level Computational Thinking A Level Problem Solving  All A-Level    Exam Board Mapping  GCSE  AQA CIE 2020 CIE 2022-24 (0984) CIE 0478 Edexcel IGCSE Edexcel OCR Eduqas WJEC National 5   A Level  AQA CIE 2022 (9618) OCR IB Computing Eduqas WJEC SQA Higher Level SQA Advanced Higher     Revision  Past Papers Mock Exams Data Representation Data Storage Databases Ethics Hardware & Software Internet Networks Programming Security         Synchronous and Asynchronous Data Transmission        KS3 Computer Science 11-14 Years Old 48 modules covering EVERY Computer Science topic needed for KS3 level.  View KS3 Resources →       GCSE Computer Science 14-16 Years Old 45 modules covering EVERY Computer Science topic needed for GCSE level.  View GCSE Resources →      A-Level Computer Science 16-18 Years Old 66 modules covering EVERY Computer Science topic needed for A-Level.  View A-Level Resources →          Home / Networks / Synchronous and Asynchronous Data Transmission          KS3 Data Representation (14-16 years) An editable PowerPoint lesson presentationEditable revision handoutsA glossary which covers the key terminologies of the moduleTopic mindmaps for visualising the key conceptsPrintable flashcards to help students engage active recall and confidence-based repetitionA quiz with accompanying answer key to test knowledge and understanding of the module  View KS3 Data Representation Resources    A-Level Exchanging Data Resources (16-18 years) An editable PowerPoint lesson presentationEditable revision handoutsA glossary which covers the key terminologies of the moduleTopic mindmaps for visualising the key conceptsPrintable flashcards to help students engage active recall and confidence-based repetitionA quiz with accompanying answer key to test knowledge and understanding of the module  View A-Level Exchanging Data Resources       Table of Contents  hide      1 KS3 Data Representation (14-16 years)    2 A-Level Exchanging Data Resources (16-18 years)    3 Synchronous Data Transmission    3.1 Characteristics of Synchronous Transmission    3.2 Examples of Synchronous Transmission    4 Asynchronous Transmission    4.1 Characteristics of Asynchronous Transmission    4.2 Examples of Asynchronous Transmission    5 Synchronous and Asynchronous Transmission    6 Synchronous vs. Asynchronous Transmission    6.1 Further Reading    Synchronous Data Transmission In synchronous data transmission, data moves in a completely paired approach, in the form of chunks or frames.  The synchronisation between the source and target is required so that the source knows where the new byte begins since there are no spaces included between the data. Synchronous transmission is effective, dependable, and often utilised for transmitting a large amount of data.  It offers real-time communication between linked devices. An example of synchronous transmission would be the transfer of a large text file.  Before the file is transmitted, it is first dissected into blocks of sentences.  The blocks are then transferred over the communication link to the target location. Because there are no beginning and end bits, the data transfer rate is quicker but there’s an increased possibility of errors occurring.  Over time, the clocks will get out of sync, and the target device would have the incorrect time, so some bytes could become damaged on account of lost bits.  To resolve this issue, it’s necessary to regularly re-synchronise the clocks, as well as to make use of check digits to ensure that the bytes are correctly received and translated.  Characteristics of Synchronous Transmission There are no spaces in between characters being sent.Timing is provided by modems or other devices at the end of the transmission.Special ’syn’ characters goes before the data being sent.The syn characters are included between chunks of data for timing functions. Examples of Synchronous Transmission ChatroomsVideo conferencingTelephonic conversationsFace-to-face interactions Asynchronous Transmission In asynchronous transmission, data moves in a half-paired approach, 1 byte or 1 character at a time.  It sends the data in a constant current of bytes.  The size of a character transmitted is 8 bits, with a parity bit added both at the beginning and at the end, making it a total of 10 bits.  It doesn’t need a clock for integration—rather, it utilises the parity bits to tell the receiver how to translate the data. It is straightforward, quick, cost-effective, and doesn’t need 2-way communication to function. Characteristics of Asynchronous Transmission Each character is headed by a beginning bit and concluded with one or more end bits.There may be gaps or spaces in between characters. Examples of Asynchronous Transmission EmailsForumsLettersRadiosTelevisions Synchronous and Asynchronous Transmission Point of ComparisonSynchronous TransmissionAsynchronous TransmissionDefinitionTransmits data in the form of chunks or framesTransmits 1 byte or character at a timeSpeed of TransmissionQuickSlowCostExpensiveCost-effectiveTime IntervalConstantRandomGaps between the data?Does not existExistExamplesChat Rooms, Telephonic Conversations, Video ConferencingEmail, Forums, Letters Synchronous vs. Asynchronous Transmission In synchronous transmission data is transmitted in the form of chunks, while in asynchronous transmission data is transmitted one byte at a time.Synchronous transmission needs a clock signal between the source and target to let the target know of the new byte.  In comparison, with asynchronous transmission, a clock signal is not needed because of the parity bits that are attached to the data being transmitted, which serves as a start indicator of the new byte.The data transfer rate of synchronous transmission is faster since it transmits in chunks of data, compared to asynchronous transmission which transmits one byte at a time.Asynchronous transmission is straightforward and cost-effective, while synchronous transmission is complicated and relatively pricey.Synchronous transmission is systematic and necessitates lower overhead figures compared to asynchronous transmission.  Both synchronous and asynchronous transmission have their benefits and limitations.  Asynchronous transmission is used for sending a small amount of data while the synchronous transmission is used for sending bulk amounts of data.  Thus, we can say that both synchronous and asynchronous transmission are essential for the overall process of data transmission. Further Reading Data communication    Post navigation Programming Data TypesUniform Resource Locator (URL)               About this site Teach Computer Science provides detailed and comprehensive teaching resources for the new 9-1 GCSE specification, KS3 & A-Level. Equally suitable for International teachers and students. Over 5,000 teachers have signed up to use our materials in their classroom.    Search for:       What do we provide? In short: everything you need to teach GCSE, KS3 & A-Level Computer Science:  Condensed revision notes Exam question booklets Mind maps Interactive quizzes PowerPoint presentations Flashcards      Exam boards Our materials cover both UK and international exam board specifications:  AQA CIE Edexcel OCR Eduqas WJEC National 5           © 2023 Teach Computer Science  This site uses cookies to improve your experience. To find out more, see our cookie policy.                                ","

Generalized Efficient Markets in Political Power - LessWrong 2.0 viewerArchiveSequencesAboutSearchLog InQuestionsEventsShortformAlignment ForumAF CommentsHomeFeaturedAllTagsRecent CommentsGeneralized Efficient Markets in Political Powerjohnswentworth1 Aug 2020 4:49 UTC42 points6 commentsLW linkWorld ModelingWorld OptimizationPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsContentsSchel­ling PointsGover­nance as Schel­ling PointPoli­ti­cal PowerCom­pe­ti­tion and Gen­er­al­ized Mar­ket EfficiencyDemoc­racy’s Seedy UnderbellySchel­ling PointsIn Thomas Schel­ling’s clas­sic ex­per­i­ment, we imag­ine try­ing to meet up with some­one in New York City, but we haven’t speci­fied a time or place in ad­vance and have no way to com­mu­ni­cate. Where do we go, and when, to max­i­mize the chance of meet­ing? There are some “nat­u­ral” choices—places and times which stand out, like the top of the Em­pire State Build­ing at noon. Th­ese are called Schel­ling points.More gen­er­ally, Schel­ling points are rele­vant when­ever two or more peo­ple need to make “match­ing” choices with limited abil­ity to com­mu­ni­cate in ad­vance. For in­stance, cer­tain mar­kets, like Ebay or Uber, serve as “meet­ing points” for buy­ers and sel­l­ers. Schel­ling him­self wrote a fair bit about ne­go­ti­a­tions, where peo­ple need to agree on how to di­vide some spoils, or where to draw a bound­ary, or … They can talk to each other, but ac­tu­ally com­mu­ni­cat­ing is hard be­cause both par­ties have no in­cen­tive to be hon­est—and there­fore no rea­son to trust each other when e.g. when one per­son says “I just can’t af­ford to sell it be­low $10”. Schel­ling points be­come nat­u­ral out­comes for the ne­go­ti­a­tions—e.g. split the spoils evenly, draw the bound­ary at the river, etc.In prac­tice, it’s of­ten use­ful to cre­ate Schel­ling points. In the New York City ex­per­i­ment, one could put up a gi­ant billboard that says “meet­ing point”, and place signs all over the city point­ing to­ward the meet­ing point, mak­ing that point a nat­u­ral place for peo­ple to meet. Some air­ports ac­tu­ally do this:Ebay and Uber are of course also ex­am­ples of pur­pose-built Schel­ling points.One in­ter­est­ing fea­ture of cre­at­ing a Schel­ling point is that we may have some de­grees of free­dom available, and we can use those de­grees of free­dom to ex­tract value.In our meetup ex­am­ple, we could imag­ine putting the meetup point in­side a build­ing, and charg­ing peo­ple to get in—much like Ebay or Uber charge fees for their ser­vices. Or, we could imag­ine lo­cal busi­nesses want­ing to put the meetup point nearby, in hopes of at­tract­ing busi­ness from meeters—one could imag­ine a gimicky air­port restau­rant with a bunch of “MEET HERE!” signs hop­ing to sell peo­ple over­priced na­chos and drinks while they wait to meet up with friends or fam­ily. Alter­na­tively, we could imag­ine users of the meetup point want­ing it in lo­ca­tions con­ve­nient to them—e.g. in the New York City ex­am­ple, peo­ple in a par­tic­u­lar neigh­bor­hood might cam­paign to es­tab­lish the meetup point there for their own con­ve­nience.How­ever, the Schel­ling point cre­ator/​con­trol­ler only has so many de­grees of free­dom. Charge too high a fee, and peo­ple will go to some other Schel­ling point. Move the meetup point to a neigh­bor­hood in the out­skirts of town, and it will be too in­con­ve­nient for peo­ple from other neigh­bor­hoods. By de­fault, peo­ple will usu­ally stick to Schel­ling points which ev­ery­one is already us­ing—if ev­ery­body has always met up un­der this par­tic­u­lar sign, then that’s the ob­vi­ous place to keep meet­ing up—so the con­trol­ler of the origi­nal Schel­ling point can ex­tract more value than “new” points. But there are always limits.We can think of a Schel­ling-point-con­trol­ler’s “power” as their range of free­dom in mov­ing the point around, or as the amount of value they can ex­tract with­out los­ing out to some other Schel­ling point. Just be­cause some­one nom­i­nally “con­trols” the Schel­ling point does not mean they can ac­tu­ally do any­thing with­out los­ing it! It may be that even a small fee will drive ev­ery­one to switch to a differ­ent Schel­ling point. It may be that the Schel­ling point is in the dead cen­ter of the city and peo­ple will keep meet­ing in the dead cen­ter even if the signs move (e.g. maybe some­one will just put up new signs for the “city cen­ter” and peo­ple will meet there). It may be that main­tain­ing all the signs costs roughly as much as one can earn from the Schel­ling point (oth­er­wise a com­peti­tor would come along and put up more signs of their own). There are many ways to ex­tract value from a Schel­ling point, but if there’s some mechanism for open com­pe­ti­tion over con­trol of the point, then the net value one can ex­tract may be driven to near-zero.That’s roughly how I think poli­tics works.Gover­nance as Schel­ling PointWhen peo­ple are op­er­at­ing in a group, it’s use­ful to have stan­dard­ized Schel­ling points for a wide va­ri­ety of in­ter­per­sonal con­flicts, so that we don’t need a bunch of ex­pen­sive ne­go­ti­a­tion/​con­flict to re­solve each one. Th­ese Schel­ling points are things like “rules” and “lead­ers”.An ex­am­ple: Alice likes to rock out to loud mu­sic af­ter sun­down, while her neigh­bor Bob likes to go to bed early. They have con­flict­ing prefer­ences for when quiet hours should be. But nei­ther of them wants to get in a fight about it, or spend a bunch of time and effort ne­go­ti­at­ing. So, the build­ing/​neigh­bor­hood has a rule: “quiet hours run from 10pm to 6am”. The main pur­pose of the rule is to act as a Schel­ling point: by de­fault, those are the quiet hours which ev­ery­one re­spects and ex­pects ev­ery­one else to re­spect. They might be en­forced if needed, but usu­ally that doesn’t ac­tu­ally hap­pen. Of course, Alice and Bob could still work out a sep­a­rate deal—e.g. maybe Alice talks to all her neigh­bors and gets their ok to play loud mu­sic on Fri­day night—but that would re­quire a bunch of ex­tra ne­go­ti­a­tion. The offi­cial rule is the Schel­ling point ev­ery­body co­or­di­nates on, by de­fault.More gen­er­ally, laws and courts serve as Schel­ling points in ne­go­ti­a­tions. Where does my prop­erty end and yours be­gin? The gov­ern­ment land records provide a Schel­ling point an­swer, so we don’t need to fight/​ne­go­ti­ate over it our­selves. In a dis­agree­ment over land, the po­lice and mil­i­tary all want to co­or­di­nate and back the same per­son, so the gov­ern­ment’s land records tell them all who the “right­ful” owner is. Since the po­lice and mil­i­tary co­or­di­nate around that Schel­ling point, it be­comes the nat­u­ral Schel­ling point for oth­ers as well.In prin­ci­ple, the legally-rec­og­nized Schel­ling point could sim­ply be ig­nored—e.g. if a chunk of land is legally rec­og­nized as your prop­erty, and I build some­thing on it with­out per­mis­sion, the two of us could just agree that this is fine, effec­tively ne­go­ti­at­ing a differ­ent Schel­ling point. Some non-gov­ern­ment group could even have mechanisms to en­force the al­ter­na­tive Schel­ling point. But the le­gal Schel­ling point is the one which po­lice and the mil­i­tary are will­ing to en­force.Poli­ti­cal PowerThe Death Eaters don’t always agree on when, where or how to launch an at­tack, but they know that any at­tack will go bet­ter if they’re all in it to­gether. So, they band to­gether be­hind a leader, and at­tack when, where and how the leader di­rects. The leader’s or­ders be­come the Schel­ling point ac­tion for the group.The Death Eaters ex­am­ple illus­trates the no­tion of “poli­ti­cal power” par­tic­u­larly well: the leader’s “power” is roughly the set of or­ders he could give with­out his or­ders ceas­ing to be a Schel­ling point for group ac­tivity. If the Death Eaters are mostly in agree­ment on some course of ac­tion, and the leader di­rects against it, then his or­ders be­come less of a Schel­ling point, and mul­ti­ple such or­ders will likely see him re­moved from nom­i­nal power. He has some de­gree of free­dom in which or­ders to give, but only to the ex­tent that the Death eaters are, on av­er­age, mostly in agree­ment with his choices.There’s a gen­eral prin­ci­ple of “power” here: a leader’s power is the set of or­ders they could give with­out their or­ders ceas­ing to be Schel­ling points for the group’s ac­tivi­ties. A leader’s power is high when group mem­bers all want to co­or­di­nate their choices, but care much less about which choice is made, so long as ev­ery­one “matches”. Then the leader can just choose any­thing they please, and ev­ery­one will go along with it. (In­ter­est­ingly, this sug­gests that a leader can get high value from a group whose prefer­ences are or­thog­o­nal to their own; pur­sue power in groups which care about differ­ent things than you!) Con­versely, a leader’s power can be low in two ways:Group mem­bers care a lot about which choice is made. In this case, the leader has lit­tle free­dom to choose, and is mostly just a figure­head.Group mem­bers only weakly care about co­or­di­nat­ing. In this case, the group is in­her­ently un­sta­ble; lots of deals and con­ces­sions are needed just to keep it to­gether.Key thing to keep in mind: both of these con­di­tions are rel­a­tive-to-the-next-best-op­tion. Group mem­bers may care a lot about co­or­di­nat­ing and only have weak prefer­ences about which choice is made, but if there’s a com­peti­tor who could co­or­di­nate just as well and satisfy the weak prefer­ences bet­ter, then that com­peti­tor’s or­ders may be­come the new Schel­ling point.That’s poli­tics, in a nut­shell: peo­ple try to turn their own or­ders/​poli­cies/​sug­ges­tions into Schel­ling points for group ac­tivity. They do this mainly by offer­ing con­ces­sions and fa­vors to group mem­bers/​sub­groups, in ex­change for those mem­bers’ sup­port for the new Schel­ling point.Com­pe­ti­tion and Gen­er­al­ized Mar­ket EfficiencyIn demo­cratic coun­tries/​groups, we have a built-in mechanism for com­pe­ti­tion be­tween would-be lead­ers. In other words, there’s a Schel­ling point for when and how to switch Schel­ling points. That im­me­di­ately sug­gests a gen­er­al­ized effi­cient mar­kets-style hy­poth­e­sis: lead­ers’ power in such groups is driven by com­pe­ti­tion to near-zero.What does that look like?Well, most peo­ple want to co­or­di­nate; re­gard­less of what the rules are, we want to agree on what the rules are, oth­er­wise we end up in ex­pen­sive fights. But most peo­ple also have some prefer­ences about the rules—i.e. poli­ti­cal poli­cies. Would-be lead­ers make promises: they pre­com­mit to cer­tain poli­cies, thereby cut­ting off cer­tain op­tions if they win (i.e. sac­ri­fic­ing po­ten­tial power), but gain­ing more sup­port for their Schel­ling point in the pro­cess. To max­i­mize power, a would-be leader wants to just barely “out­bid” all the other would-be lead­ers—i.e. promise just a bit more to just a few more par­ties, keep­ing as much power as pos­si­ble while still win­ning the po­si­tion.Of course, the other com­peti­tors are try­ing to do the same thing. Solve for the equil­ibrium: the com­peti­tors ei­ther bid away any de­gree of free­dom which any con­stituency cares about, or lose to some­one who bids more. Gen­er­al­ized effi­cient mar­kets kick in; the leader ends up with near-zero power. They’re mostly just a figure­head im­ple­ment­ing all the poli­cies they had to pre­com­mit to in or­der to win the elec­tion.Now, con­sider the re­verse—a dic­ta­tor or sin­gle-party state or the like. How do they max­i­mize power?To max­i­mize power, they want to avoid gen­er­al­ized effi­cient mar­kets—i.e. they want to min­i­mize com­pe­ti­tion over the Schel­ling point. Elec­tions en­courage com­pe­ti­tion by pro­vid­ing a Schel­ling point for when and how to switch Schel­ling points; the power-hun­gry leader wants ex­actly the op­po­site of that. They want to make sure that there is no Schel­ling point for when and how to switch Schel­ling points.If a new Schel­ling point does show up (e.g. an op­po­si­tion group), there won’t be any agree­ment on when and how to switch, so there will prob­a­bly be some ex­pen­sive con­flict (i.e. civil war). That ex­pen­sive con­flict it­self cre­ates a big po­ten­tial en­ergy bar­rier for any po­ten­tial com­peti­tor: for the peo­ple sup­port­ing a switch to the new Schel­ling point, the ex­pected gains from the switch must ex­ceed costs of the con­flict. So from the dic­ta­tor’s stand­point, the worse a civil war would be, the fewer con­ces­sions and hand­outs they need to make and the broader their power.(Of course, a dic­ta­tor can use other strate­gies to max­i­mize power as well—e.g. threat­en­ing to kill peo­ple/​de­stroy things if a new Schel­ling point comes along. But that’s a sym­met­ric strat­egy: the dic­ta­tor’s en­e­mies can just as eas­ily threaten to kill peo­ple/​de­stroy things if no new Schel­ling point is adopted. The dic­ta­tor may have an ad­van­tage in re­sources, but that gap can in prin­ci­ple be closed by other means. It’s mainly the lack of a Schel­ling point for switch­ing Schel­ling points which con­fers an asym­met­ric ad­van­tage to the in­cum­bent.)Democ­racy’s Seedy UnderbellyBased on the pre­vi­ous sec­tion, some­one ac­cus­tomed to a “democ­racy=good, dic­ta­tor=bad” wor­ld­view might think that lead­ers be­ing forced to bar­gain away all their po­ten­tial power is good news. “Lead­ers are just figure­heads” and “lead­ers are just im­ple­ment­ing the poli­cies which won the elec­tion” both say the same thing. This is “good”, yes?The failure modes of democ­racy are baked-in here too.Con­sider a would-be leader figur­ing out the perfect mix of promises and con­ces­sions to make, in or­der to win an elec­tion. From their point of view, differ­ent peo­ple want­ing op­po­site things is a prob­lem. Mov­ing the meetup point closer to one neigh­bor­hood means mov­ing it fur­ther from an­other. But di­men­sion­al­ity is a ma­jor boon for the leader: there’s thou­sands of di­men­sions along which policy can change. If Alice cares strongly about one par­tic­u­lar di­men­sion—like, say, gov­ern­ment sup­port for her pro­fes­sion—which no­body else cares about very much, then that promise can be made to Alice with­out los­ing the sup­port of some­body else. It’s spe­cial-in­ter­est poli­tics: look for poli­cies with fo­cused benefits and diffuse costs. Pile many such poli­cies to­gether, and you have a win­ning coal­i­tion.That out­come may be “effi­cient” in the sense that no other bun­dle of poli­cies can beat it in an elec­tion, but that’s very differ­ent from “effi­cient” in the sense of “not wast­ing ridicu­lous amounts of re­sources on pork-bar­rel pro­jects and reg­u­la­tory bar­ri­ers to en­try”.Now, sup­pose we’re un­happy with this out­come. We want to build a bet­ter world. What can we do?Ob­vi­ously “run for office” is not a work­able an­swer here. Gen­er­al­ized effi­cient mar­kets mean we can’t win an elec­tion with­out trad­ing away any abil­ity to en­act our preferred poli­cies.If we have some ex­ter­nal re­sources—e.g. a gi­ant pile of money—then we could po­ten­tially use that to “force” the poli­ti­cal equil­ibrium in a differ­ent di­rec­tion. This could look like old-fash­ioned bribery, where we just pay some stake­hold­ers to back our preferred Schel­ling point. It could look like a pay­ment to the poli­ti­cal sys­tem as a whole, e.g. offer­ing a pri­vate sub­sidy for road re­pair. It could in­volve re­sources other than money, as in a celebrity or me­dia out­let offer­ing an en­dorse­ment, or a na­tion offer­ing some con­ces­sion in ex­change for lower tar­iffs. We could change the op­tions available to the group via tech­nol­ogy, e.g. bit­coin. We could sim­ply try to con­vince peo­ple to sup­port our preferred poli­cies—though this means com­pet­ing in memes­pace, which has gen­er­al­ized effi­cient mar­kets of its own. The gen­eral pat­tern: we use our re­sources to change the set of op­tions available or to di­rectly in­fluence the prefer­ences of group mem­bers.Point is: there are no hun­dred-dol­lar bills ly­ing on the ground. If we want to change the poli­ti­cal equil­ibrium in a highly-poli­ti­cally-com­pet­i­tive en­vi­ron­ment, we need to change the un­der­ly­ing op­tions available to the group and/​or the prefer­ences of in­di­vi­d­ual group mem­bers.johnswentworth1 Aug 2020 4:49 UTC42 points6 commentsLW linkWorld ModelingWorld OptimizationPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsZack_M_Davis 1 Aug 2020 19:07 UTC 6 pointsIt gets worse. We also face co­or­di­na­tion prob­lems on the con­cepts we use to think with. In or­der for lan­guage to work, we need shared word defi­ni­tions, so that the prob­a­bil­is­tic model in my head when I say a word matches up with the model in your head when you heard the word. A leader isn’t just in a po­si­tion to co­or­di­nate what the group does, but also which as­pects of re­al­ity the group is able to think about.Raemon 4 Aug 2020 2:00 UTC 4 pointsI feel obli­gated to link CGP Grey’s The Rules for Rulers, which makes some similar points through a some­what differ­ent lens with a bunch of cute graph­ics.betulaster 4 Aug 2020 1:21 UTC 3 pointsI’m prob­a­bly miss­ing some­thing ob­vi­ous, but I don’t triv­ially see how this In­ter­est­ingly, this sug­gests that a leader can get high value from a group whose prefer­ences are or­thog­o­nal to their own; pur­sue power in groups which care about differ­ent things than you! fol­lows from thisA leader’s power is high when group mem­bers all want to co­or­di­nate their choices, but care much less about which choice is made, so long as ev­ery­one “matches”. Then the leader can just choose any­thing they please, and ev­ery­one will go along with it.  Could you please elab­o­rate?Also, I have an out­sider’s view of Amer­i­can (or, in­deed, Western in gen­eral) poli­tics, so I can be wrong, but I think an ar­gu­ment from em­pirics could be made against this:  It’s spe­cial-in­ter­est poli­tics: look for poli­cies with fo­cused benefits and diffuse costs. Pile many such poli­cies to­gether, and you have a win­ning coal­i­tion. At least in the two most re­cent Amer­i­can elec­tions (2016 and then the 2018 midterms) it seems like it was very much not the case of peo­ple rac­ing for the most fo­cused benefits and most diffuse cost, but rather for the most effi­cient way to gal­va­nize their vot­ers, cost be damned. Think of the wall on the Mex­i­can bor­der—it would prob­a­bly be ex­or­bitantly ex­pen­sive, in­clud­ing to those that voted for it, but it was a very pow­er­ful sym­bol that peo­ple who felt strongly about the is­sue could rally be­hind. 538 here do a kind of liter­a­ture re­view—and find, amongst other things, that racial at­ti­tudes mat­tered more in 2016 than in any re­cent elec­tion — even 2008, when the pres­ence of an Afri­can-Amer­i­can can­di­date shaped the poli­ti­cal con­ver­sa­tion. Un­less I mi­s­un­der­stand the idea, I don’t think is­sues of race have a nar­row fo­cused scope of benefits and costs diffuse enough not to be no­ticed by other vot­ers. I also think this point Would-be lead­ers make promises: they pre­com­mit to cer­tain poli­cies, thereby cut­ting off cer­tain op­tions if they win (i.e. sac­ri­fic­ing po­ten­tial power), but gain­ing more sup­port for their Schel­ling point in the pro­cess. makes an as­sump­tion of vot­ers be­ing more-or-less perfectly in­formed about what the Schel­ling point (poli­cies and laws) ac­tu­ally is. What if a leader could get elected by pre-com­mit­ing to cer­tain poli­cies, but then ac­tu­ally not act on them, while man­ag­ing to con­vince the vot­ers that they, in fact, are do­ing their best to im­ple­ment these poli­cies, but are failing to for a cer­tain (prob­a­bly not a very falsifi­able) rea­son? Or does the model already sup­port this in a way that I don’t no­tice?johnswentworth 4 Aug 2020 2:53 UTC 3 pointsParentOn “group with prefer­ences or­thog­o­nal to your own”: the idea is you can give the mem­bers ex­actly what they want, and then in­de­pen­dently get what­ever you want as well. Since they’re in­differ­ent to the things you care about, you can choose those things how­ever you please.At least in the two most re­cent Amer­i­can elec­tions (2016 and then the 2018 midterms) it seems like it was very much not the case of peo­ple rac­ing for the most fo­cused benefits and most diffuse cost, but rather for the most effi­cient way to gal­va­nize their vot­ers, cost be damned.I ex­pect that poli­tics in most places, and US Con­gres­sional poli­tics es­pe­cially, is usu­ally much more heav­ily fo­cused on spe­cial in­ter­ests than the over­all me­dia nar­ra­tive would sug­gest. For in­stance, vot­ers in Kansas care a lot about farm sub­sidies, but the news will mostly not talk about that be­cause most of us find the sub­ject rather bor­ing. The me­dia wants to talk about the things ev­ery­one is in­ter­ested in, which is ex­actly the op­po­site of spe­cial in­ter­ests.Also I am ex­tremely skep­ti­cal that racial is­sues played more than a minor role in the elec­tion, even as­sum­ing that they played a larger role in 2016 than in other elec­tions. Every me­dia out­let in the coun­try (in­clud­ing 538) wanted to run sto­ries about how race was su­per-im­por­tant to the elec­tion, be­cause those sto­ries got tons of clicks, but that’s very differ­ent from ac­tu­ally play­ing a role.Or does the model already sup­port this in a way that I don’t no­tice?Nope, you are com­pletely right on that front, poor in­for­ma­tion/​straight-up ly­ing were is­sues I ba­si­cally ig­nored for pur­poses of this post. That said, most of the post still ap­plies once we add in ly­ing/​bul­lshit; the main change is that, when­ever they can get away with it, lead­ers will lie/​bul­lshit in or­der to si­mul­ta­neously satisfy two groups with con­flict­ing goals. As long as at least some peo­ple in each con­stituency see through the lies/​bul­lshit, there will still be pres­sure to ac­tu­ally do what those peo­ple want. On the other hand, peo­ple who can be fooled by lies/​bul­lshit are es­sen­tially “neu­tral” for pur­poses of in­fluenc­ing the poli­ti­cal equil­ibrium; there’s no par­tic­u­lar rea­son to worry about their prefer­ences at all. So we just ig­nore the gullible peo­ple, and ap­ply the dis­cus­sion from the post to ev­ery­body else.betulaster 14 Aug 2020 21:13 UTC 3 pointsParentThanks for the re­ply and sorry I couldn’t get to this for some time! Hope you’re still in­ter­ested in the dis­cus­sion. I ex­pect that poli­tics in most places, and US Con­gres­sional poli­tics es­pe­cially, is usu­ally much more heav­ily fo­cused on spe­cial in­ter­ests than the over­all me­dia nar­ra­tive would sug­gest This is re­ally in­ter­est­ing and you prob­a­bly have a good point. Do you think there’s a more re­li­able way (for an out­sider like my­self, who’s not able to, I dunno, go and ask peo­ple in a dive bar what they think) to get the lay of the poli­ti­cal land in a par­tic­u­lar point in space? (And time?) Maybe some cen­tral­ized kind of poll repos­i­tory? Every me­dia out­let in the coun­try (in­clud­ing 538) wanted to run sto­ries about how race was su­per-im­por­tant to the elec­tion, be­cause those sto­ries got tons of clicks, but that’s very differ­ent from ac­tu­ally play­ing a role. On a side note, I can imag­ine this kind of per­spec­tive, when taken to an un­miti­gated ex­treme, lead­ing to a very carte­sian-de­mon view of the world. Most peo­ple that pub­lish their thoughts are in­cen­tivized to make you, the reader, like it or be in­ter­ested in it. Mass-me­dia ob­vi­ously so, blog­gers or an­a­lysts or think tanks less ob­vi­ously so, but still. If, when faced with a choice of writ­ing about (a) things that are real but dull vs (b) things that are not real but get clicks, no one has an in­cen­tive to do (a), how do you form a view of the world? (Had I not known about pub­lish-or-per­ish and read Gel­man/​Falkovich on p-hack­ing, I my­self would give the tra­di­tion­ally carte­sian an­swer of “by read­ing sci­en­tific pa­pers in peer-re­viewed jour­nals”, but… yeah).  So we just ig­nore the gullible peo­ple, and ap­ply the dis­cus­sion from the post to ev­ery­body else. I think we’ve made an im­por­tant move in ar­gu­men­ta­tion here—we’ve started to in­tro­duce the pos­si­bil­ity of the vot­ers differ­ing by whether they be­lieve the lies/​bul­lshit or not. But if we do—that is, we in­tro­duce the pos­si­bil­ity of the voter con­sid­er­ing some of the poli­ti­cian’s com­mit­ment to a fu­ture policy Schel­ling point not gen­uine—we also open the pos­si­bil­ity for the voter to spec­u­late on what the poli­ti­cian’s true poli­cies are. Say, Alice runs for pres­i­dent on a con­ser­va­tive/​jobs-cen­tric plat­form, com­mits to out­law work visas and ex­punge all for­eign work­ers, and wins the race, and says that she’s work­ing hard to achieve that goal. Bob is a to­tal sup­porter and he’s sure that she does ex­actly that and thinks that de­por­ta­tions are only a cou­ple days away. Char­lie may be skep­ti­cal about the promise, be­cause sounded very rad­i­cal and cam­paign-y, but thinks she’s prob­a­bly go­ing to cut work visas, but not be able to ex­punge for­eign­ers already in the coun­try. Dave agrees with Char­lie that the promise was rad­i­cal and will not be fol­lowed through on fully, but not on what will ac­tu­ally hap­pen—he thinks Alice will be able to ex­punge already pre­sent for­eign­ers, but never risk the poli­ti­cal tur­moil of re­mov­ing work visas. Erin is a to­tal skep­tic and thinks Alice is merely ex­ploit­ing the vot­ers, and is ac­tu­ally not do­ing any­thing about the for­eign­ers, and fi­nally Frank is a con­spir­acy the­o­rist and thinks that Alice is se­cretly work­ing with a ca­bal of global­ists to bring even more for­eign­ers (maybe even ille­gally!) in while bul­lshit­ting him.All of these peo­ple have differ­ent Schel­ling points! If Zack, a for­eigner, asks Bob to loan him money, Bob is go­ing to re­fuse, be­cause he thinks Zack will be kicked out of the coun­try to­mor­row and he’s not get­ting his money back. If he asks Erin, she’s likely to agree, be­cause she doesn’t be­lieve Zack is go­ing any­where.Now, sure, there’s only so many ways to in­ter­pret a sin­gle cam­paign promise, and there are bound to be groups within the voter base that will agree on what Alice will ac­tu­ally do, the Schel­ling point will work for them—but since Alice is in­cen­tivized to make a lot of fo­cused-benefit-dis­perse-cost promises, vot­ers who agree on what her ac­tions on a cer­tain policy are, may dis­agree on what her ac­tions re­gard­ing a differ­ent policy are. So… when no­body agrees on what the Schel­ling point is, does it, for all in­tents and pur­poses, ex­ist?johnswentworth 14 Aug 2020 21:55 UTC 2 pointsParentGreat points!Do you think there’s a more re­li­able way (for an out­sider like my­self, who’s not able to, I dunno, go and ask peo­ple in a dive bar what they think) to get the lay of the poli­ti­cal land in a par­tic­u­lar point in space?This is wayyyy out­side my zone of ex­per­tise, but I would look for spe­cial­ist-ori­ented pub­li­ca­tions—e.g. newslet­ters speci­fi­cally tar­geted at lob­by­ists/​poli­cy­mak­ers, or poli­ti­cal in­for­ma­tion in the in­dus­try pub­li­ca­tions of spe­cial-in­ter­est in­dus­tries.If, when faced with a choice of writ­ing about (a) things that are real but dull vs (b) things that are not real but get clicks, no one has an in­cen­tive to do (a), how do you form a view of the world?I’d say the key is to gen­er­ate your own ques­tions, then proac­tively look for the an­swers rather than wait­ing around for what­ever in­for­ma­tion comes to you. There’s plenty of good in­for­ma­tion out there, it just isn’t su­per-viral, so you have to go look­ing for it.All of these peo­ple have differ­ent Schel­ling points!Im­por­tant point here: these peo­ple don’t ac­tu­ally have differ­ent Schel­ling points. They pre­sum­ably all agree that if Alice wins the elec­tion, then what­ever Alice signs into law will be the new Schel­ling point. What these peo­ple dis­agree on is their ex­pec­ta­tions for what the fu­ture Schel­ling point will be.Back to top"
2,"



MAT337. Introduction to Real Analysis









MAT337. Introduction to Real Analysis
Fall 2018

Web page: http://www.math.toronto.edu/ilia/MAT337.2018/. 
Class Location & Time: Tue, 1:00PM - 2:00 PM; Thu, 11:00 AM - 1:00 PM; NE2190
Instructor: Ilia Binder (ilia@math.toronto.edu),  DH3026.
  
Office Hours: Tue 2:00 PM - 3:00 PM and Thu 10:00 AM-11:00 AM
Teaching Assistant:  Belal Abuelnasr, (belal.abuelnasr@mail.utoronto.ca ).
Office Hours:   Fri, 10-11 AM, DH3050.

Textbooks: Understanding Analysis, Second Edition, by Stephen Abbott.
  This book is provided as a free electronic resource to all UofT students through the library website.
  Click on the following link to access the textbook (you may be required to enter your UTORid and password): http://myaccess.library.utoronto.ca/login?url=http://books.scholarsportal.info/viewdoc.html?id=/ebooks/ebooks3/springer/2015-07-09/1/9781493927128 

Prerequisites:  MAT102H5, MAT224H5/MAT240H5, MAT212H5/MAT244H5, MAT232H5/MAT233H5/MAT257Y5
Exclusions:  MAT337H1, MAT357H1,MATB43H3, MATC37H3
  
Prerequisites will be checked, and students not meeting them will be removed from the course by the end of the second week of classes. If a student believes that s/he does have the necessary background material, and is able to prove it (e.g., has a transfer credit from a different university), then s/he should submit a 'Prerequisite/Corequisite Waiver Request Form'.

Topics. 
The course is the rigorous introduction to Real  Analysis. We start with the careful discussion of The Axiom of Completeness and proceed to the study of the basic  concepts of limits, continuity, Riemann integrability, and differentiability.  

Topics covered in class.
September 6: An introduction. Real numbers and the Axiom of Completeness. Section 1.3.
September 11: The Axiom of Completeness. Nested Interval property. Sections 1.3, 1.4.
September 13: Nested Interval property. Archimedean property. Definitions of the limit of a sequence (including an alternative definition). Limits and algebraic operations. Sections 1.4, 2.2, 2.3.
September 18: Limits and algebraic operations. Limits and order. Squeezed sequence lemma.Section 2.3.
September 20: The Monotone Convergence Theorem. Iterated sequences. Positive series. Liminf and limsup. Section 2.4.
September 25:  Liminf and limsup. Subsequences and their limits. Bolzano-Weierstrass Theorem.  Section 2.5.
September 27: Bolzano-Weierstrass Theorem.  Cauchy Criterion. Series. Sections 2.5, 2.6, 2.7.
October 2: Open and closed sets. Interrior, exterior, and border points. Section 3.2.
October 4: Interrior, exterior, and border points. Compact sets. Heine-Borel Theorem. Sections 3.2, 3.3.
October 16:  Heine-Borel Theorem. Baire's Theorem. Sections 3.3, 3.5.
October 18:  Functional limits. Sequential criterion. Continuity. Sections 4.2, 4.3.
October 23:  Continuity and compact sets. Uniform continuity. Section 4.4.
October 25:  Uniform continuity and compact sets. The Intermediate value Theorem. Differentiability (including an alternative definition). Darboux's Theorem. Sections 4.4, 4.5, 5.2.
October 30:  Rolle's theorem. The Mean Value Theorem. L'Hospital rule. Pointwise and Uniform convergence. Sections 5.3, 6.2.
November 1:   Uniform convergence. Continuity of uniform limit. Uniform convergence and differentiation. Sections 6.2, 6.3.
November 6:   Midterm review.
November 8:   Midterm.
November 13:   Uniform convergence and differentiation. Uniform convergence of series. Sections 6.3, 6.4.
November 15:   Power series. Section 6.5.
November 20:   Riemann Integration. Section 7.2.
November 22:   Riemann Integration: criterion of integrability, non-integrable functions integrability of continuous functions, additivity and algebraic properties of Riemann integral. Sections 7.2, 7.3, 7.4.
November 27:   Algebraic properties of Riemann Integral. Integrability of Uniform limit. Section 7.4.
November 29:   The Fundamental Theorem  of Calculus. Integration by parts. Riemann integrability criterion. Sections 7.5, 8.1.
December 4:   Final review. 


Homework. The assignments should be submitted through Quercus. To submit, you can scan or take a photo of your work (or write your work electronically). Please make sure that the images are clear and easy to read before you submit them.

Assignment #1, due September 13: The assignment is based on the material you have learned in MAT102.

Please do the following exercises from the textbook:  1.2.3, 1.2.4, 1.2.5, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.2.11, 1.2.12, 1.2.13.


Assignment #2, due September 20.


Assignment #3, due October 4.


Assignment #4, due October 18.


Assignment #5, due October 25.


Assignment #6, due November 1.


Assignment #7, due November 8.


Assignment #8, due November 15.


Assignment #9, due November 22.


Assignment #10, due November 29.

Tutorials and presentations. Each student must be registered in one of the tutorials (on ROSI). The attendance of tutorials is mandatory. Based on the homework assignments, the students will be selected to present some of the homework problems at the tutorials. An unexcused absence at the tutorial on the day you are selected for the presentation will result in zero credit for the presentation. 
Tutorials will begin on Friday of the second week of classes. 
Quiz. There  will be a one-hour in-tutorial quiz on Friday, September 28, or Monday, October 1, depending on your tutorial section. No aides are allowed for this quiz. The quiz will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4.
Recommended preparation (do not hand in): problems 1.3.2, 1.3.3, 1.3.6, 1.3.8, 1.4.8, 2.2.2, 2.2.4, 2.3.2, 2.3.7, 2.4.1, 2.4.6, 2.4.8.

Midterm Test. There  will be a two-hour in-class midterm test on Thursday, November 8. No aides are allowed for this test. The test will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 3.2, 3.3, 3.5, 4.2, 4.3, .4.4, 4.5, 5.2, 5.3.
  Recommended preparation: assignment #7, and (do not hand in):  all the quiz review problems, 2.5.9, 2.6.4, 2.7.7, 3.2.8, 3.3.8, 3.5.9, 4.2.4, 4.3.6, 4.4.11, 4.5.6, 5.2.10, 5.3.4. 
Final exam.   The final exam will be held on Wednesday, December 12, 5-8pm, at KN137. No aides are allowed for this test. 
The exam will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 3.2, 3.3, 3.5, 4.2, 4.3, .4.4, 4.5, 5.2, 5.3, 6.2, 6.3, 6.4, 6.5, 6.6 (up to Theorem 6.6.2), 7.2, 7.3, 7.4, 7.5, 8.1 (up to Theorem 8.1.2).
You will be required to state and prove in detail one of the following Theorems from the textbook: 2.4.2, 2.5.5, 3.3.4, 4.2.3, 4.3.9, 4.4.1, 4.4.2, 4.4.7, 5.2.7, 5.3.2, 6.2.6, 6.4.4, 7.2.8, 7.5.1.
Recommended preparation (do not hand in): all the quiz and midterm review problems, 6.2.3, 6.2.13, 6.2.14, 6.2.15, 6.3.1, 6.3.6, 6.4.2, 6.4.4, 6.4.10, 6.5.2, 6.5.8, 7.2.3, 7.3.2, 7.3.5, 7.4.3, 7.4.10, 7.5.2, 7.5.4.
Additional office hours: Tuesday, December 11, 12 - 1. Location: DH3000 .

 Grading. Grades  will be based on the best eight out of ten homework assignements (10%), an in-tutorial quiz (10%), an in-lecture midterm test (25%), tutorial presentations (15%), attendance of tutorials and active participation in the discussions (5%), and Final exam (35%). I will also  occasionally assign bonus problems.
Late  work. No late work will be accepted. Special consideration for late assignments or missed exams must be submitted via e-mail within a week of the original due date. There will be no make-up quiz, midterm test, or final. Justifiable absences must be declared on ROSI, undocumented absences will result in zero credit.
E-mail policy.
E-mails must originate from a utoronto.ca address and contain the course code MAT337 in the subject line. Please include your full name and student number in your e-mail.

Academic Integrity.
    Honesty and fairness are fundamental to the University of Toronto’s mission. Plagiarism is a form of academic fraud and is treated
    very seriously. The work that you submit must be your own and cannot contain anyone elses work or ideas without proper
    attribution. You are expected to read the handout How not to plagiarize (http://www.writing.utoronto.ca/advice/using-sources/how-not-to-plagiarize) and to be familiar with the Code of behaviour on academic matters, which is linked from the UTM calendar under
  the link Codes and policies.

Maintained by:   Ilia Binder (ilia@math.toronto.edu)


","



Stellar : Message of the Day








Search: 










MIT Course Management System





Home
Course Guide
@Stellar
Updates





Message of the day
Continue








Stellar CMS
Information Services & Technology


W92 . 304 Vassar Street
Cambridge . MA . 02139



Get Help


FAQ


User Guide


Contact the Help Desk
Request a Stellar site



Resources

Supported Browsers

Certificates

Library E-Reserves

WebSIS



Updates


What's new?

Subscribe






"
3,"






A research workflow with Zotero and Org mode | mkbehr.com



















Skip to main content





Toggle navigation





mkbehr.com






About me


Archive


Tags


RSS feed


Github














Source










A research workflow with Zotero and Org mode


                    Michael Behr
            
September 19, 2015

Comments

Source






Any research project is going to involve a literature search: reading
through a bunch of papers that might be relevant to your topic in
order to get a sense of what the field already knows. Now, maybe
there's some magic technique for picking out the information that
matters, passing over the rest, and writing out a single, coherent
story in one pass through all the papers you can find. If that
technique exists, I have no idea what it is.
So when every paper brings up ten new questions and twenty papers to
start answering them, I need a system to keep my notes organized. I
need notes that let me jump back and forth between papers without
losing my place, draw links between papers, and store lists of
citations to come back to. Here's how I do it.

Storing papers with Zotero
The first tool I use is Zotero, a reference
manager. Zotero's job is to store all the actual papers I come across,
along with information like data on how to cite the papers and any
tags they might have been published with. It can grab that information
from my web browser, whether from a journal's website or someplace
like Google Scholar or PubMed. It's also great for quickly putting
together a bibliography, using bibtex or similar programs, when I want
to write up some results.



Zotero stores the papers I want to
read and reference. I scaled up the font size here to make it readable
in a tiny blog image.
Zotero isn't the only choice for reference managers.
Mendeley is another popular choice, and
there are a
whole bunch more
out there. I picked Zotero arbitrarily a few years ago, but it's
working out well because of its emacs integration.
Keeping notes with Emacs and Org mode
You see, Zotero has some note-taking functions, and I used to keep my
notes there, but there were some problems. Notes are stored as
separate files for each paper, but I want to cross-reference notes
from a lot of different papers at once. And while the editor has some
rich-text capabilities (e.g. bold and italic text), it's missing
important things I need in my notes, like the ability to typeset
equations.
That's where Emacs and its
extension Org mode come in. To borrow a term
from Perl enthusiasts, Org mode is the swiss army chainsaw of text
document formats. Org mode documents have a lot of features, and it's
way beyond this post's scope to describe them all. For the purpose of
research notes, the most useful things it lets me do are:

I can store my notes in a hierarchical tree structure, and I can
  hide parts of the tree from view in order to focus on other parts.
I can put hyperlinks into my notes, including links to papers,
  websites, or other parts of the file.
I can put math in my notes using Latex, and view the typeset
  equations right in my Emacs buffer.




A sample from my notes file. You
can see the tree structure of the file, some links to papers, and a
little bit of inline math, using Latex.
Gluing it all together with zotxt
Now, see those links to papers in my notes buffer? I didn't have to
copy and paste them from anywhere. I inserted them with just three
keystrokes each. So far, I've just described some useful pieces of
software, but the interesting part of my workflow is how they fit
together.
zotxt is an extension that lets
other programs talk to Zotero, and Emacs has a package to talk to it.
It's even structured specifically to work with Org mode documents.
With zotxt, my workflow looks like this:

I find a paper I want to look at somewhere on the internet.
I use Zotero's browser plugin to save it to Zotero. Hopefully it
  grabs the paper itself and this happens in one click; if the site
  doesn't play along, I spend a minute grabbing a pdf and feeding it
  to Zotero.
I insert a link to the Zotero entry into my notes file in Emacs. I
  can do this with the key chords C-c "" "". I don't need to further
  specify what paper I want to grab: the browser plugin leaves the
  paper selected in Zotero, and zotxt can grab the selected paper.
When I want to read the paper, I go to the link and tell Emacs to
  open the paper in my system PDF viewer. The key chords for this are
  C-c "" a, and then selecting the PDF attachment from the Helm
  window that appears (usually I just type pdf RET).
When I'm reading a paper and see a citation that might be useful, I
  look it up on the internet and repeat this process to store a note
  linking to it.

It took me a while to get it set up to my liking, so here's how I did
it:

First, install zotxt. If you're
  using Zotero as a firefox extension, you just need to install zotxt
  as another extension. If you're using the standalone Zotero client,
  you can still do it: download the extension file from that link,
  then go to the Add-Ons Manager under the Tools menu and find the
  option to install an add-on from a file.




The menu option looks like
this.

Next, install the zotxt package in emacs. If your
  package manager is set up, you
  can just type M-x package-install RET zotxt RET.
Now, when org-zotxt-mode is active, you can use its functions in
  your org-mode buffers. You can search for papers and insert them
  with C-c "" i, insert the currently-selected paper in Zotero with
  C-u C-c "" i, and open a paper's PDF or other related files by
  moving the cursor to a link and typing C-c "" a. However, you might
  want a little bit more setup to deal with some annoyances.
You probably want to have org-zotxt-mode automatically activated
  in all your org-mode documents. To make that happen, you can add
  some code to your .emacs file to start up this mode on all your
  org-mode buffers - see below this list for the .emacs
  configuration I use.
If you want to insert a link to the currently-selected item a lot,
  C-u C-c "" i is an awkward sequence to type. I rebound it to C-c ""
  "".
You might notice that when you insert a link to a paper, the text of
  that link is a full citation. That might be what you want, but I
  just want the authors, paper name, and year. It took me a bit of
  hacking to get around that: it's possible to tell the emacs zotxt
  interface to use a different citation format than the default, but I
  had to throw together a little XML file to give it a shorter format
  than a full citation. (This may not be the easiest or cleanest way
  to do it, but it works!)
  That XML file is here. To use it, go into
  your Zotero preferences and select Cite -> Styles, and add the file.
  It should appear in the menu as ""mkbehr's short reference format"".
  Then add the last two lines in the .emacs snippet below, and you
  should get shorter citations.
You probably want to install the
  Helm package, to make zotxt's
  search interface easier to navigate. That link should tell you
  everything you need to know.

Here's that .emacs setup code:
;; Activate org-zotxt-mode in org-mode buffers
(add-hook 'org-mode-hook (lambda () (org-zotxt-mode 1)))
;; Bind something to replace the awkward C-u C-c "" i
(define-key org-mode-map
  (kbd ""C-c \"" \"""") (lambda () (interactive)
                      (org-zotxt-insert-reference-link '(4))))
;; Change citation format to be less cumbersome in files.
;; You'll need to install mkbehr-short into your style manager first.
(eval-after-load ""zotxt""
'(setq zotxt-default-bibliography-style ""mkbehr-short""))

Of course, I'm not done tinkering to make my workflow better. I hear
good things about the org-ref
and helm-bibtex
packages - if only I can keep an up-to-date bibtex file as I add papers
to my library, I can associate links with not only a paper's pdf, but
also that paper's section of my notes file. And I haven't found a
smooth way to take a paper and pull up the papers it cites in my
browser. But until then, I'm pretty happy with this setup.
Happy researching!



emacs
research



Previous post


Next post

Comments

Please enable JavaScript to view the comments powered by Disqus.

Comments powered by Disqus




            Contents © 2015         Michael Behr - Powered by         Nikola






","









Edge.org



















Skip to main content


Copyright © 2023 By Edge Foundation, Inc. All Rights Reserved.



 

Edge.org







 



To arrive at the edge of the world's knowledge, seek out the most complex and sophisticated minds, put them in a room together, and have them ask each other the questions they are asking themselves.




https://www.edge.org/response-detail/26557Printed On Fri November 17th 2023 



Fri, Nov 17, 2023HOMECONVERSATIONSVIDEOAUDIOANNUAL QUESTIONEVENTSNEWSLIBRARYABOUTPEOPLE 


















2016 : WHAT DO YOU CONSIDER THE MOST INTERESTING RECENT [SCIENTIFIC] NEWS? WHAT MAKES IT IMPORTANT?



 In the News [ 22 ] 
   |   
 Contributors [ 199 ] 
   |   
 View All Responses [ 199 ]  




  
 Pamela McCorduck 
 Author, Machines Who Think, The Universal Machine, Bounded Rationality, This Could Be Important; Co-author (with Edward Feigenbaum), The Fifth Generation  

 


 Identifying The Principles, Perhaps The Laws, Of Intelligence 
 The most important news for me came in mid-2015, when three scientists, Samuel J. Gershman, Eric J. Horvitz, and Joshua Tenenbaum published “Computational rationality: A converging paradigm for intelligence in brains, minds, and machines” in Science, 17 July 2015. They announced that they and their colleagues had something new underway: an effort to identify the principles, perhaps the laws, of intelligence, just as Newton once discovered the laws of motion.
Formerly, any commonalities among a stroll in the park, the turbulence of a river, the revolution of a carriage wheel, the trajectory of a cannon ball, or the paths of the planets, seemed preposterous. It was Newton who found the underlying generalities that explained each of them (and so much more) at a fundamental level.
Now comes a similarly audacious pursuit to subsume under general principles, perhaps even laws, the essence of intelligence wherever it’s found. “Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things,” Newton said.
So far as intelligence goes, we are pre-Newtonian. Commonalities of intelligence shared by cells, dolphins, plants, birds, robots and humans seem, if not preposterous, at least far-fetched.
Yet rich exchanges among artificial intelligence, cognitive psychology, and the neurosciences, for a start, aim exactly toward Newton’s “truth in simplicity,” those underlying principles (maybe laws) that will connect these disparate entities together. The pursuit’s formal name is computational rationality. What is it exactly, we ask? Who, or what, exhibits it?
 The pursuit is inspired by the general agreement in the sciences of mind that intelligence arises not from the medium that embodies it—whether biological or electronic—but the way interactions among elements in the system are arranged. Intelligence begins when a system identifies a goal, learns (from a teacher, a training set, or an experience) and then moves on autonomously, adapting to a complex, changing environment. Another way of looking at this is that intelligent entities are networks, often hierarchies of intelligent systems, humans certainly among the most complex, but congeries of humans even more so.
The three scientists postulate that three core ideas characterize intelligence. First, intelligent agents have goals, and form beliefs and plan actions that will best reach those goals. Second, calculating ideal best choices may be intractable for real-world problems, but rational algorithms can come close enough (“satisfice” in Herbert Simon’s term) and incorporate the costs of computation. Third, these algorithms can be rationally adapted to the entity’s specific needs, either off-line through engineering or evolutionary design, or online through meta-reasoning mechanisms that select the best strategy on the spot for a given situation.
Though barely begun, the inquiry into computational rationality is already large and embraces multitudes. For example, biologists now talk easily about cognition, from the cellular to the symbolic level. Neuroscientists can identify computational strategies shared by both humans and animals. Dendrologists can show that trees communicate with each other (slowly) to warn of nearby enemies, like wood beetles: activate the toxins, neighbor.
The humanities themselves are comfortably at home here too, though it’s taken many years for most of us to see that. And of course here belongs artificial intelligence, a key illuminator, inspiration, and provocateur.
It’s news now; it will stay news because it’s so fundamental; its evolving revelations will help us see our world, our universe, in a completely new way. And for those atremble at the perils of super-intelligent entities, surely understanding intelligence at this fundamental level is one of our best defenses. 
 
 Return to Table of Contents  

 



  










  
 2018 : WHAT IS THE LAST QUESTION?  

  
 2017 : WHAT SCIENTIFIC TERM OR CONCEPT OUGHT TO BE MORE WIDELY KNOWN?  

  
 2016 : WHAT DO YOU CONSIDER THE MOST INTERESTING RECENT [SCIENTIFIC] NEWS? WHAT MAKES IT IMPORTANT?  

  
 2015 : WHAT DO YOU THINK ABOUT MACHINES THAT THINK?  

  
 2014 : WHAT SCIENTIFIC IDEA IS READY FOR RETIREMENT?  

  
 2013 : WHAT *SHOULD* WE BE WORRIED ABOUT?  

  
 2012 : WHAT IS YOUR FAVORITE DEEP, ELEGANT, OR BEAUTIFUL EXPLANATION?  

  
 2011 : WHAT SCIENTIFIC CONCEPT WOULD IMPROVE EVERYBODY'S COGNITIVE TOOLKIT?  

  
 2010 : HOW IS THE INTERNET CHANGING THE WAY YOU THINK?  

  
 2009 : WHAT WILL CHANGE EVERYTHING?  

  
 2008 : WHAT HAVE YOU CHANGED YOUR MIND ABOUT? WHY?  

  
 2007 : WHAT ARE YOU OPTIMISTIC ABOUT?  

  
 2006 : WHAT IS YOUR DANGEROUS IDEA?  

  
 2005 : WHAT DO YOU BELIEVE IS TRUE EVEN THOUGH YOU CANNOT PROVE IT?  

 2004 : WHAT'S YOUR LAW?  

 2003 : WHAT ARE THE PRESSING SCIENTIFIC ISSUES FOR THE NATION AND THE WORLD, AND WHAT IS YOUR ADVICE ON HOW I CAN BEGIN TO DEAL WITH THEM? - GWB  

 2002 : WHAT IS YOUR QUESTION? ... WHY?  

 2001 : WHAT NOW?  

 2001 : WHAT QUESTIONS HAVE DISAPPEARED?  

 2000 : WHAT IS TODAY'S MOST IMPORTANT UNREPORTED STORY?  

  
 1999 : WHAT IS THE MOST IMPORTANT INVENTION IN THE PAST TWO THOUSAND YEARS?  

 1998 : WHAT QUESTIONS ARE YOU ASKING YOURSELF?  

 











John Brockman, Editor and Publisher
Contact Info:[email protected]
In the News
Get Edge.org by email

Edge.org is a nonprofit private operating foundation under Section 501(c)(3) of the Internal Revenue Code.
Copyright © 2023 By Edge Foundation, Inc All Rights Reserved.






 














"
4,"



90% of all claims about the problems with medical studies are wrong | Slate Star Codex
































































Home

About / Top Posts
Archives
Top Posts

Comments Feed
RSS Feed





Slate Star Codex









Blogroll 
Economics

Artir Kel
Bryan Caplan
David Friedman
Pseudoerasmus
Scott Sumner
Tyler Cowen


Effective Altruism

80000 Hours Blog
Effective Altruism Forum
GiveWell Blog


Rationality

Alyssa Vance
Beeminder
Elizabeth Van Nostrand
Gwern Branwen
Jacob Falkovich
Jeff Kaufman
Katja Grace
Kelsey Piper
Less Wrong
Paul Christiano
Robin Hanson
Sarah Constantin
Zack Davis
Zvi Mowshowitz


Science

Andrew Gelman
Greg Cochran
Michael Caton
Razib Khan
Scott Aaronson
Stephan Guyenet
Steve Hsu


SSC Elsewhere

SSC Discord Server
SSC Podcast
SSC Subreddit
Unsong


Archives

January 2021
September 2020
June 2020
May 2020
April 2020
March 2020
February 2020
January 2020
December 2019
November 2019
October 2019
September 2019
August 2019
July 2019
June 2019
May 2019
April 2019
March 2019
February 2019
January 2019
December 2018
November 2018
October 2018
September 2018
August 2018
July 2018
June 2018
May 2018
April 2018
March 2018
February 2018
January 2018
December 2017
November 2017
October 2017
September 2017
August 2017
July 2017
June 2017
May 2017
April 2017
March 2017
February 2017
January 2017
December 2016
November 2016
October 2016
September 2016
August 2016
July 2016
June 2016
May 2016
April 2016
March 2016
February 2016
January 2016
December 2015
November 2015
October 2015
September 2015
August 2015
July 2015
June 2015
May 2015
April 2015
March 2015
February 2015
January 2015
December 2014
November 2014
October 2014
September 2014
August 2014
July 2014
June 2014
May 2014
April 2014
March 2014
February 2014
January 2014
December 2013
November 2013
October 2013
September 2013
August 2013
July 2013
June 2013
May 2013
April 2013
March 2013
February 2013

Full Archives
 




90% of all claims about the problems with medical studies are wrong

Posted on February 17, 2013 by Scott Alexander 

I have frequently heard people cite John Ioannidis’ apparent claim that “90% of medical research is false”.
I think John Ioannidis is a brilliant person and I love his work and I think this statement points at a correct and important insight. But as phrased, I think this particular formulation when not paired with any caveats creates just a little more panic than is warranted.
Before I go further, Ioannidis’ evidence:
He starts with simple statistics. Most studies are judged to have “discovered” a result if they reach p < 0.05, that is, if there is 5% probability or less the findings are due to mere chance (this is the best case scenario, where the study is totally free from bias or methodological flaws).
Suppose you throw a dart at the Big Chart O’ Human Metabolic Pathways and supplement your experimental group with the chemical you hit. Then ten years later you come back and see how many of them died of heart attacks.
Most chemicals on the Big Chart probably don’t prevent heart attacks. Let’s say only one in a thousand do. Maybe your study will successfully find that 1/1000. But the 999 inactive chemicals will also throw up about 50 (999 * 5%) false positives significant at the 5% level. Therefore, even if you conduct your study perfectly, and it shows a significant decrease in heart attacks, there’s about a 98% chance it’s false.
One would hope medical scientists plan their studies with a little more care than throwing a dart at a metabolic chart. Yet many don’t; a lot of genetic research is conducted by checking every single gene against the characteristic of interest and seeing if any stick. And even when scientists have well-thought out theories, the inherent difficulty of medicine means they probably have less than a 50-50 chance of being right the first time, which means a 5% significance level has a less than 5% predictive value.
And this isn’t even counting publication bias or poor methodology or conflicts of interest or anything like that.
Disturbingly, this problem seems to be borne out in empirical tests. Amgen Pharmaceuticals says it repeated experiments in 53 important papers and was only able to confirm 6. And Ioannidis himself did a re-analysis which is quoted as finding that “41% of the most influential studies in medicine have been convincingly shown to be wrong or significantly exaggerated.”
So I don’t at all disagree with the general consensus that this is a huge problem. But I do disagree with the following statements:
1. 90% of all medical research is wrong
2. A given study you read, or your doctor reads, is 90% likely to be wrong.
3. 90% of the things doctors believe, presumably based on these medical findings, is wrong.
4. This proves the medical establishment is clueless and hopelessly irrational and that two smart people working in a basement for five minutes can discover a new medical science far better than what all doctors could have produced in seventy years.
Is 90% of all medical research wrong?
As far as I can tell, there is no source at all for the 90% figure. I can’t find it in any of Ioannidis’ studies and indeed they contradict it. His table of predictive values of different studies doesn’t have any entries that correspond to 90% (“underpowered exploratory epidemiological study” is relatively close with 88%, but this is just for that one type of study, which is known to be especially bad). The Atlantic sums it up as:
His model predicted, in different fields of medical research, rates of wrongness roughly corresponding to the observed rates at which findings were later convincingly refuted: 80 percent of non-randomized studies (by far the most common type) turn out to be wrong, as do 25 percent of supposedly gold-standard randomized trials, and as much as 10 percent of the platinum-standard large randomized trials.
Notice which number is conspicuously missing from that excerpt.
Now another study of his did show that in 90% of studies with very large effect sizes, later research eventually found the effect size to be smaller, but this was out of a pool of studies specifically selected for being surprising and likely to be false. I don’t think it’s the source of the number and if it were that would be terrible.
As far as I can tell, this started from a quote in an Atlantic article on Ioannidis which included the line “he charges that as much as 90 percent of the published medical information that doctors rely on is flawed”. This then got turned into the title of a Time article “A Researcher’s Claim: 90% of Medical Research Is Wrong”, which itself got perverted to 90% of Medical Research Is Completely False.
So an unsourced quote that up to 90% of studies are flawed has somehow turned into a rallying cry that it has been proven that at least 90% of studies are false. To take this seriously we would have to believe that the numbers for all research are the same as the numbers for the poorly conducted epidemiological studies or the studies specifically selected for surprising results. I guess having a nice round number is good insofar as it makes the public pay attention to this field, but as far as actual numbers go, it’s kind of made up.
Is any given study you read, or your doctor reads, 90% likely to be wrong?
But let’s take the above number at face value and say that 90% of medical studies are wrong. Fine. Does that mean the last medical study you read about in Scientific American, or that your doctor used to recommend you a new drug, is wrong?
No. Let’s look at the Medical Evidence Pyramid.
The medical evidence pyramid is much like all pyramids, in that the bottom levels are infested with snakes and booby traps and vengeful medical evidence mummies. It’s only after you reach the top few levels that you get the gold and jewels and precious, precious mummy powder.
This plays out in the same table of Ioannidis’ speculations we saw before. While an in vitro study of the type used to identify possible drug targets might have a positive predictive value of 0.1%, a good meta-analysis or great RCT has a positive predictive value of 85%; that is, it’s 85% likely to be true.
There are only two reasons someone might hear about the studies on the snake-infested bottom levels of the pyramid. Number one, that person is a specialist in the field who is valiantly trying to read through the entire niche medical journal the paper was published in. Or number two, the study found something incredible like DONUTS CURE CANCER IN A SAMPLE OF THREE LAB RATS!!! and the media decided to pick up on it. Hopefully everyone already ignores studies of the DONUTS CURE CANCER IN A SAMPLE OF THREE LAB RATES!!! type studies; if not, there’s really not much I can say to you.
But most of the medical results that you hear about are the ones that get published in important journals and are trumpeted far and wide as important medical results. These are closer to the top of the pyramid than to the bottom. They’re usually big expensive studies on thousands of people. Since the universities, hospitals, and corporations sponsoring them aren’t idiots, they usually hire a decent statistician or two to make sure that they don’t spend $300,000 testing something only to have a letter to the editor of the NEJM point out that they forgot to blind their subjects so it’s totally worthless. And finally, in many cases you would only run a study that big and expensive if you had something plausible to test – you’re not going to spend $300,000 just to throw a dart at the Big Chart O’ Human Metabolic Pathways and see what happens.
So these studies that people actually hear about are bigger, they have more incentives to get their methodology right, and they’re testing propositions with high plausibility. How do they do?
I said above that one of Ioannidis’ studies was frequently quoted as saying that “41% of the most influential studies in medicine have been convincingly shown to be wrong or significantly exaggerated.”
This is from a great study I totally endorse, but the 41% number was maximized for scariness. If I wanted to bias my reporting the other direction, I could equally well report the same results as “Only about 5% of influential medical experiments with adequate sample size have later been contradicted.”
How? Ioannidis got his result by taking all medical studies with over 1000 citations in the ’90s, of which there were 49. Of these, 4 were negative results (ie “X doesn’t work”) so he threw them out. This is the first part I think is kind of unfair. Yes, negative results aren’t as sexy as positive results, but they’re still influential medical research, and if Ioannidis is quoted as saying that X% of medical findings are later contradicted when he means that X% of positive medical findings are, that’s not quite fair.
Annnnyway, of the 45 famous studies with positive findings, 11 didn’t really get tested and so we don’t know if they’re right or wrong. Eliminating these is also a potential bias, because we expect that studies which seem sketchy are more likely to be replicated so people can find out if they’re actually right. Ioannidis quite rightly set himself a higher bar by not eliminating them, but the quote about 41% of studies being wrong does seem to have gone back eliminated them – at least that’s the only way I can make the study numbers add up to 41% (the numbers given in the study actually say 32% of these studies failed to replicate).
So our 41% number is based off of 34 studies, best described as “34 famous medical studies that found positive findings ie the least believable kind of finding, plus were suspicious enough that someone wanted to replicate them”.
Of these 34 studies, 7 were outright contradicted. Bad? Definitely. But for example, one of them was a study with a sample size of nine patients. Another study may well have been correct, but the results were interpreted wrongly (it said that estrogen decreased lipoprotein levels which everyone assumed meant decreased heart disease, but in fact later studies found increased heart disease without necessarily disproving the lipoprotein levels). Five of the six others were epidemiological trials, firmly on the middle of the pyramid. Only two of these contradicted studies were a true experiment with a sample size of >10.
(even here, I am sort of skeptical. Three of these disproven studies, two epidemiologicals and an experimental, purported to show Vitamin E decreased heart disease. Then a single better trial showed that Vitamin E did not decrease heart disease. While recognizing the last trial was better, it does seem like something more complicated is going on here than “all three of the earlier trials were just wrong”, and I’ve recently been convinced antioxidant research is a huge minefield where tiny differences in protocol can cause big differences in results. But fine, let’s grant this one and say there were two outright-contradicted experiments.)
So aside from the seven that were outright wrong, another seven were listed as “overstating their results”.
There are a couple of problems that bothered me here. One of them was that Ioannidis decided to count studies as contradicting each other if relative risk in one study was half or less than in the other study, “regardless of whether confidence intervals might overlap or not”. So even if a study effectively said “Here is a wide range of possible results, we think it’s about here in the middle but our research is consistent with it being anywhere in this range”, if another study got somewhere else in that range, the first study was marked as “exaggerated”.
The second problem is, once again, poor studies versus poor interpretations. Ioannidis cites as an example of an exaggerated study one lasting a year and showing that the drug zidovudine helped slow the progression of HIV to AIDS. It concluded that giving HIV patients long-term zidovudine was probably a good idea. A later study lasted longer, and said that yes, zidovudine worked for a year, but then it stopped working. Because the earlier study had suggested longer-term zidovudine, it was marked as “exaggerated results”, even though the results of both studies were totally consistent with one another (both found that zidovudine worked for the first year). This is probably of little consolation to AIDS patients who were treated with a useless drug, but it seems pretty important if we’re investigating study methodology.
So the way I got my 5% figure was to take the two experimental studies with decent sample sizes which were actually contradicted and compare them to the 38 large experimental studies total that started the experiment.
So this suggests that if you see a large experimental study being trumpeted in the medical literature, the chance that it will be found to be totally false (as opposed to true but exaggerated) within ten years or so is only about 5% – which if you understand p-values is about what you should have believed already.
(I think. This requires quite a few assumptions, not the least of which is that my calculations above are correct!)
Also worth noting: Ioannidis’ experiment did not investigate the absolute highest level of the medical pyramid, systematic reviews and meta-analyses. I expect the best of these to be better than any individual study.
3. Are 90% of the things doctors believe, presumably based on medical findings, wrong?
After going through the steps above, it should be pretty obvious that the answer is no, because doctors are mostly reading famous influential studies like the ones mentioned above, which are at worst 40% and at best 5% wrong.
But there’s another factor to be taken into account, which is that why would you only read one study on something when lots of important findings have been investigated multiple times?
Suppose that you’re throwing darts at the Big Chart O’ Human Metabolic Pathways, with your 1/1000 base rate of true hypotheses. You run a very good methodologically sound study and find p = .05. But now there’s still only a 1/50 chance your hypothesis is correct.
But another team in China runs the same study, and they also find p = .05. We expect the Chinese to get false to true results at a rate of one to two (because the 1 in the 1/50 stays 1, but the 50 is divided by 20 to produce approximately 2. Wow, I’m even worse at explaining math than I am at doing it.)
Now a team in, oh, let’s say Turkey runs the same study, and they also find p = .05. We expect the Turks to get false to true results at a rate of one to ten, for, uh, the same math reasons as the Chinese. When the, um, Icelanders repeat the study, our odds go to one to two hundred.
So we started with 1000:1 odds, the first study brought us up to 50:1 odds, the second study to 2:1 odds, the third study to 1:10 odds, and the fourth study to 1:200 odds, ie we are now 99.5% sure we’re right.
Real medicine is both better and worse than this. It’s better in that we often have dozens of studies rather than just four. It’s worse in that the studies are not all so methodologically sound that we can multiply our odds by 20 each time (to put it lightly).
But some of them are, and once we get enough of them, the base rate problems which plague individual medical findings go away very quickly. Even if only one of the studies is methodologically sound, if the reason they’re studying their topic is because a bunch of other less believable studies all got positive results, that’s a much better base rate than “because I hit it with my dart”.
When doctors say that, for example, iron supplements help anaemia, it’s not because they hit iron on their Big Chart O’ Human Metabolic Pathways, then ran a single study, got p = .05, and rushed off to publish a medical textbook. It’s because they knew hemoglobin had iron in it, there are at least 21 randomized controlled studies, probably some had p-values closer to .001 than to .05 even though I don’t have any of them in front of me to check, and eventually some really really smart statisticians at the Cochrane Collaboration gave it their seal of approval. Most doctors’ beliefs aren’t on quite this high a level, but most doctors’ beliefs aren’t on the “Someone threw a dart, then did one study” level either.
4. Does this prove the medical establishment is clueless and hopelessly irrational and that two smart people working in a basement for five minutes can discover a new medical science far better than what all doctors could have produced in seventy years?
A lot of people seem to go from Ioannidis’ experiment to something like “So I guess everyone in medicine is just clueless about how science and statistics work. I’ll go read a couple of medical studies and then be able to outperform everyone in this totally flawed field.”
(important note: I’m not accusing MetaMed of this! They seem pretty sane. I am accusing some people I come across in the community who are much more enthusiastic than the relatively sober MetaMed people of doing something like this.)
But the problem isn’t that no one in medicine is familiar with Ioannidis’ research. It’s that they’re not really sure what to do about it and figuring out a plan and implementing it will take time and effort.
Ioannidis’ work isn’t exactly secret. I’ve hung out with groups of residents (ie trainee doctors) who have discussed Ioannidis’ findings over the dinner table. According to The Atlantic
To say that Ioannidis’s work has been embraced would be an understatement. His PLoS Medicine paper is the most downloaded in the journal’s history, and it’s not even Ioannidis’s most-cited work—that would be a paper he published in Nature Genetics on the problems with gene-link studies. Other researchers are eager to work with him: he has published papers with 1,328 different co-authors at 538 institutions in 43 countries, he says. Last year he received, by his estimate, invitations to speak at 1,000 conferences and institutions around the world, and he was accepting an average of about five invitations a month until a case last year of excessive-travel-induced vertigo led him to cut back.
So if so many people are aware of this, why isn’t the problem getting fixed more quickly?
An optimist could say the problem isn’t getting fixed because there is no problem. A vast volume of embarassingly wrong medical literature gets published, inflates the publishers’ resumes, and everyone else ignores it and concentrates on the not-really-so-bad large randomized trials. To the post-cynic it is all a smooth, well-functioning machine.
A pessimist might say that the problem isn’t getting fixed because it’s impossible. The average medical hypothesis is always going to have a low base rate of being true – in fact, if we force scientists to only study high base-rate hypotheses, by definition everything we discover will be boring. There will never be enough resources to apply huge rigorous trials to every one of the millions of things worth studying. So we’re always going to have weak studies about low-base rate hypotheses, which is what Ioannidis is attacking as the recipe for failure.
A realist might point out there are some things we can do, but it involves coordinating a huge and complicated system with many moving parts. Journals can force trials to register before they conduct their experiments to avoid publication bias. The scientific community can give more status to people who perform important replications and especially important negative replications. Study authors and the media can come up with better ways to report their results to doctors and the public without blowing them out of proportion. Statisticians can…actually, anything I say statisticians can do is just going to be a mysterious answer, along the lines of “do better statistics stuff”, so I’m not going to embarass myself by completing this sentence except to postulate that I’ll bet there’s some recommendation that could complete it usefully.
But all these things involve vague entities who aren’t really actors (“the scientific community”, “the media”) acting in ways that are kind of against their immediate incentives. This is hard to make people do and usually involves a lot of grassroots coordination effort. Which is going on. But it takes time.
But no matter what happens, I think a useful epistemic habit is to be very skeptical of individual studies, and skeptical but not too skeptical of large randomized trials, good meta-analyses, and general medical consensus when supported by an evidence base.


						This entry was posted in Uncategorized. Bookmark the permalink.											


← Google Correlate does not imply Google Causation
Typical mind and gender identity →


26 Responses to 90% of all claims about the problems with medical studies are wrong
Reverse order




 gwern says: 

			February 17, 2013 at 7:17 pm 
> It’s only after you reach the top few levels that you get the gold and jewels and precious, precious mummy powder. This plays out in the same table of Ioannides’ speculations we saw before.
The hyperlink in both seems to be the same.
> Also worth noting: Ioannides’ experiment did not investigate the absolute highest level of the medical pyramid, systematic reviews and meta-analyses. I expect the best of these to be better than any individual study.
Isn’t that covered in http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.0020124.t004&representation=PNG_M ? Second row, ‘Confirmatory meta-analysis of good-quality RCTs’, PPV=0.85
(Hm, I wonder why that meta-analysis has the same PPV as a ‘Adequately powered RCT with little bias and 1:1 pre-study odds’… Maybe adequately powered here means a study sample size equivalent to that of meta-analyses pooling many underpowered studies.)
Speaking of which, I can’t believe I missed that chart while reading that paper originally. That changes everything: it is our priors for medical research!
> After going through the steps above, it should be pretty obvious that the answer is no, because doctors are mostly reading famous influential studies like the ones mentioned above, which are at worst 40% and at best 5% wrong.
I don’t think this is obvious at all. If doctors did not take a step without consulting a Cochran Review, then yeah, any individual therapy or treatment will have that nice 5-40% chance of being wrong. But is that the case? I was under the impression that the evidence-based medicine folks had made lists and examined old standard treatments with actual RCTs and found that many treatments or medicines had never been genuinely tested, and when they were, often failed.








 Scott Alexander says: 

			February 17, 2013 at 8:22 pm 
Table link fixed.
When I said he doesn’t include meta-analyses, I meant in that particular study of the top 49 most cited medical studies. I agree he certainly considers them from a theoretical perspective.
“I was under the impression that the evidence-based medicine folks had made lists and examined old standard treatments with actual RCTs and found that many treatments or medicines had never been genuinely tested, and when they were, often failed.”
I am under the impression that most medicine now is evidence-based medicine.








 gwern says: 

			February 17, 2013 at 8:42 pm 
> I am under the impression that most medicine now is evidence-based medicine.
One would certainly like to think that, but given the long track record of medicine, it’s not something I would believe until I saw a paper asserting that the majority of operations performed in some sample had evidence-based medicine backing…








 Michael vassar says: 

			February 18, 2013 at 9:59 am 
Is that itself a medical claim? An EBM claim?
How did you conclude this?
I really really think you should talk with some of the doctors MetaMed works with about this.  I certainly don’t think anu of them agree












 Andrew Hunter says: 

			February 17, 2013 at 7:25 pm 
Side note: How many times has MetaMed changed its name?
I mean, they seem like smart people and the basic idea is good, but constant renames is one of those classic signs of a Silicon Valley startup that has no idea what they’re doing and is stuck bikeshedding–it doesn’t give me a particularly good feeling about it.








 Scott Alexander says: 

			February 17, 2013 at 8:19 pm 
They’ve had legitimate reasons for most of the times they’ve changed their name, and they’re not really public yet so it doesn’t really count.










 Sarah says: 

			February 17, 2013 at 7:56 pm 
1.  I, for one, don’t go around claiming that 90% of research studies are false; I believe Ioannides only found a little more than half of medical studies were disconfirmed by later experiments.  90% of all *pre-clinical cancer studies* are later disconfirmed, which does mean that if you see a study that says “X cures cancer” but X hasn’t made it to clinical trials yet, X probably *doesn’t* cure cancer.
2.  I don’t expect most things doctors believe to be wrong.  I don’t even expect most things *people* believe to be wrong; after all, most things people believe are of the form “water is wet,” so uncontroversial that we barely notice them as beliefs.  What I *do* expect to be wrong are beliefs that aren’t based on a model of the world.  
“Iron supplements cure anemia” is a belief that depends on knowing how hemoglobin works, knowing how digestion works, having observed iron supplements cure anemia…lots of different kinds of evidence, at different scales of complexity, confirm the prediction. 
“Antioxidants reduce cancer risk” is an example of the kind of belief we should be skeptical about.  Free radical damage may lead to cancer; antioxidants stabilize free radicals; so one might think antioxidants prevent cancer. Some early clinical trials found that taking antioxidants reduced cancer risk; but further study found that they probably don’t.  And there’s some evidence that “free radicals” (or reactive oxygen species) are the mechanism by which the immune system and chemotherapy drugs attack cancers, so antioxidants, if anything, protect cancer cells.  We don’t have a clear model of what antioxidants do, in the same way that we have a clear model of what role iron plays in the blood, so we ought to be skeptical of any conclusions about “antioxidants are good for you” or “antioxidants are bad for you.” And we ought to be *especially* skeptical of anything that assumes a cause (eating antioxidants) will result in a health outcome (less cancer) merely because the cause affects an intermediate biochemical step (stabilizing free radicals).
If there’s a prevailing theory in the biomedical sciences that a.) relies on complex chains of genetic and biochemical causation, and b.) hasn’t shown measurable results in the form of lower death rates, then I’m going to tag it as improbable.








 Scott Alexander says: 

			February 17, 2013 at 8:31 pm 
1. I didn’t mean to “accuse” you of saying this. I meant other, less reputable sources like Time Magazine and most of the rest of the media.
2. I’m not sure to what degree I agree with your emphasis on understanding mechanism. There are a lot of things that work in biology without us having any idea how. Natural selection was a pretty good example until we discovered genes. Another good example would be digitalis, which was used to treat heart failure since the 1700s but whose full mechanism of action was only discovered in the last few decades. I prefer good experimental results to good theoretical explanation, but the caveat which I think you’re trying to point out is that they had better be *good* experimental results, as opposed to marginal experimental results.
3. The last part wasn’t meant as an attack on MetaMed, just on some of the people who try to talk about this on Less Wrong. I’ve edited the post to try to make this slightly clearer.










 Sarah says: 

			February 17, 2013 at 8:15 pm 
A defense of MetaMed:
1.  We are not a Silicon Valley startup, we aren’t even based in Silicon Valley, we don’t belong to that culture, we’re not a consumer web app, there’s really no sense in which that’s the right reference class.
2.  We are only doing a major publicity launch at the end of this *month*, so name changes are not really a practical issue.  The name changes were in response to market research — people are bad at intuiting what kinds of names appeal to customers, and we finally settled on MetaMed after our marketing team put a lot of effort into finding out what made the best impression. It’s just window dressing; the internal structure of the company has stayed the same.
3.  It’s useless to promise we can fix a hard problem until we succeed in fixing it.  But we’re not claiming to be able to waltz in and fix medicine with no effort.  One of the main things we’re doing is a bit more modest: just *assembling* a coherent model out of the research that already exists.  
We know, for example, that statistical prediction rules work very well for medical diagnosis and risk prediction; these are just simple little combinations of a few known risk factors for, say, heart attacks, that give each patient a score rating their hart attack risk.  But these rules only exist for a few special cases in medicine.  For most diseases, nobody has gone around combining all the risk factors and all the signs and symptoms into a single statistical model that says “if we know X, Y, and Z about you, here’s how likely you are to have disease A.” 
One way of looking at MetaMed’s job is that we combine, reorganize, and quantify the existing scientific literature.  
Now, in a sense, you could say that medical culture already does this; doctors pretty much know, from some combination of their clinical experience, their med school education, and whatever research they have time to read, how to diagnose diseases and choose treatments.  But this is, one has to admit, an imperfect process.  Human minds are very bad at intuitively putting disparate pieces of information together.  That’s *why* things like checklists and statistical prediction rules can outperform clinicians; using intuitive judgment, you’ll forget things.  Formally organizing the research literature into prediction models is a kind of safeguard on expert judgment, and I think it’s quite likely to catch things doctors miss.








 Deiseach says: 

			February 18, 2013 at 1:53 am 
(1) Statistics are much more complicated than people (even smart, educated, knowledgeable in their field people) think.
(2) Medicine is an art more than a science.
(3) Confusion reigns.  For family reasons, I’ve been scouring online resources for information on diabetes diets, and I’m getting confusing recommendations even on the very same website; e.g. carbohydrates increase blood sugar – well and good; starch as well as sugar needs to be watched – fine, tell me more; eat vegetables rather than fruit – okay, what vegetables; eat carrots, they’re low-GI – no, don’t eat carrots, they’re loaded with sugar! Eat peas – no, don’t eat peas, they’re full of starch and starch is bad!
The conclusion I am left with is that the only safe diet (for anything) is rainwater and moss 🙁








 Michael vassar says: 

			February 18, 2013 at 10:06 am 
This is exactly what MetaMed is for.   It’s definitely the case that 1, 2, and 3 are true, but 1 is largely a consequence of accepting too low a standard when seeing people as smatt and knowledgeable










 Sniffnoy says: 

			February 18, 2013 at 2:22 am 
So this suggests that if you see a large experimental study being trumpeted in the medical literature, the chance that it will be found to be totally false (as opposed to true but exaggerated) within ten years or so is only about 5% – which if you understand p-values is about what you should have believed already.
(I think. This requires quite a few assumptions, not the least of which is that my calculations above are correct!)
Not really.  P-values are not how likely something is to be wrong or invalid.  Rather, they’re how likely this data was to show up if you were wrong, i.e., they’re P(E | not H) rather than P(not H | E).  (Except they’re not really that, either — they’re just how likely this data was to show up if you were wrong in a particular way, i.e. the null hypothesis.)
And yes this is very counterintuitive and hence why everybody gets them wrong.








 Scott Alexander says: 

			February 18, 2013 at 1:03 pm 
The 5% number comes not from p-values but from the empirical observation that of 40 studies analyzed, 2 were wrong. This matches the number of studies that would be wrong merely by chance if we only took the p-value into account rather than the base rate.








 Sniffnoy says: 

			February 19, 2013 at 5:58 am 
Yes, but you say that the empirical 5% matches what you’d expect from a p-value of 5%, and I don’t think that’s correct.  Unless you just mean “hey look these numbers are the same!” which doesn’t really mean anything by itself.
I mean, you talk about just taking the p-value into account rather than the base rate, but it’s not at all clear to me that the way you do so is meaningful.  Just considering the equation P(H|E)=P(E|H)*P(H)/P(E), you’re suggesting that we “don’t take into account base rate” by assuming P(H)/P(E) is about 1?  I really don’t see what makes such an assumption reasonable.
Now if you want to say, “Let’s not worry about what P(H) is, and so just assume P(H)/P(E) is some constant”, that might make more sense.  But then you can’t get any particular number out of it.












 Elissa says: 

			February 18, 2013 at 3:59 am 
Thanks for looking into the 90% thing. Ioannidis misspelled as “Ioannides” throughout.








 jason says: 

			February 18, 2013 at 8:08 am 
What percentage of studies that contradict famous studies with positive findings are false?








 gwern says: 

			February 18, 2013 at 10:26 am 
I believe, although I haven’t checked, that the studies Ioannidis looked at were always larger or otherwise better than the original studies being tested (since this would be the only sensible approach); hence, if there’s a disagreement, either way….










 Alyssa Vance says: 

			February 18, 2013 at 11:31 am 
For what it’s worth, I don’t know of anyone at MetaMed who has ever claimed that 90% of studies are false, so I think your first sentence might be straw manning. Me and several others have claimed that 80% are false, but that’s much more in line with his actual results, as you note.








 Scott Alexander says: 

			February 18, 2013 at 1:08 pm 
Do feel like I’ve heard the 90% number somewhere, possibly somewhere nonofficial in private conversation with someone, but unless I can track down a source I’ll edit it out with apologies. I still think “80% of non-experimental studies” is a pretty big caveat compared to “80% of research” but I have no idea how this was phrased and for all I know you said it that way. Sorry about that.








 SGr says: 

			December 5, 2015 at 8:34 am 
Your link to the 90% claim:
http://therefusers.com/refusers-newsroom/90-of-peer-reviewed-clinical-research-is-completely-false-greenmedinfo/#.VmLbj_mrTNN












 Nancy Lebovitz says: 

			February 18, 2013 at 7:06 pm 
I thought a lot of the point of MetaMed was to find sound but neglected research– at least as much that as debunking bad research.






Pingback: Future tense | Slate Star Codex


Pingback: MetaMed launch day | Slate Star Codex




 DanielLC says: 

			April 22, 2014 at 12:49 am 
> Of these, 4 were negative results (ie “X doesn’t work”) so he threw them out. This is the first part I think is kind of unfair.
I disagree. Negative results don’t show that an effect isn’t there. They just show that it’s too small to see with that sample size. A negative result being later disproven does not show a flaw in the original study.
If you show that the effect is there and is large enough that the first study shouldn’t have missed it, that’s a problem, but it makes this way more complicated so it’s easier just to ignore those studies.
Thinking about this more, I guess they’d have to do something like this either way, to show that the study didn’t just fail to replicate because the second study had a false negative. Either they’d have to look at ones where the study where it fails is much more powerful, or they’d have to use a two-tailed T-test and show that the two studies shouldn’t result from the same effect.






Pingback: The Problem With Connection Theory | The Rationalist Conspiracy




 Q says: 

			April 28, 2014 at 4:10 pm 
Here is a 90% number: “He (Ioannidis) charges that as much as 90 percent of the published medical information that doctors rely on is flawed.”
http://www.theatlantic.com/magazine/archive/2010/11/lies-damned-lies-and-medical-science/308269/












Meta

Register Log in
Entries feed
Comments feed
WordPress.org


B4X is a free and open source developer tool that allows users to write apps for Android, iOS, and more.
80,000 Hours researches different problems and professions to help you figure out how to do as much good as possible. Their free career guide show you how to choose a career that's fulfilling and maximises your contribution to solving the world's most pressing problems.
The Effective Altruism newsletter provides monthly updates on the highest-impact ways to do good and help others.
Altruisto is a browser extension so that when you shop online, a portion of the money you pay goes to effective charities (no extra cost to you). Just install an extension and when you buy something, people in poverty will get medicines, bed nets, or financial aid.
AISafety.com hosts a Skype reading group Wednesdays at 19:45 UTC, reading new and old articles on different aspects of AI Safety. We start with a presentation of a summary of the article, and then discuss in a friendly atmosphere.
Norwegian founders with an international team on a mission to offer the equivalent of a Norwegian social safety net globally available as a membership. Currently offering travel medical insurance for nomads, and global health insurance for remote teams.
The COVID-19 Forecasting Project at the University of Oxford is making advanced pandemic simulations of 150+ countries available to the public, and also offer pro-bono forecasting services to decision-makers.

Seattle Anxiety Specialists are a therapy practice helping people overcome anxiety and related mental health issues (eg GAD, OCD, PTSD) through evidence based interventions and self-exploration. Check out their free anti-anxiety guide here.
MealSquares is a ""nutritionally complete"" food that contains a balanced diet worth of nutrients in a few tasty easily measurable units. Think Soylent, except zero preparation, made with natural ingredients, and looks/tastes a lot like an ordinary scone. 
Dr. Laura Baur is a psychiatrist with interests in literature review, reproductive psychiatry, and relational psychotherapy; see her website for more.  Note that due to conflict of interest she doesn't treat people in the NYC rationalist social scene.
Substack is a blogging site that helps writers earn money and readers discover articles they'll like.
Giving What We Can is a charitable movement promoting giving some of your money to the developing world or other worthy causes. If you're interested in this, consider taking their Pledge as a formal and public declaration of intent.
Metaculus is a platform for generating crowd-sourced predictions about the future, especially science and technology. If you're interested in testing yourself and contributing to their project, check out their questions page

Beeminder's an evidence-based willpower augmention tool that collects quantifiable data about your life, then helps you organize it into commitment mechanisms so you can keep resolutions. They've also got a blog about what they're doing here
Support Slate Star Codex on Patreon. I have a day job and SSC gets free hosting, so don't feel pressured to contribute. But extra cash helps pay for contest prizes, meetup expenses, and me spending extra time blogging instead of working.
Jane Street is a quantitative trading firm with a focus on technology and collaborative problem solving. We're always hiring talented programmers, traders, and researchers and have internships and fulltime positions in New York, London, and Hong Kong. No background in finance required. 




 















","




Off the Convex Path – Off the convex path






























About

Subscribe









Off the Convex Path


Contributors

Sanjeev Arora
Nisheeth Vishnoi
Nadav Cohen

Former contributors:

Moritz Hardt

Mission statement
The notion of convexity underlies a lot of beautiful mathematics. When combined with computation, it gives rise to the area of convex optimization that has had a huge impact on understanding and improving the world we live in. However, convexity does not provide all the answers. Many procedures in statistics, machine learning and nature at large—Bayesian inference, deep learning, protein folding—successfully solve non-convex problems that are NP-hard, i.e., intractable on worst-case instances. Moreover, often nature or humans choose methods that are inefficient in the worst case to solve problems in P.
Can we develop a theory to resolve this mismatch between reality and the predictions of worst-case analysis?  Such a theory could identify structure in natural inputs that helps sidestep worst-case complexity.
This blog is dedicated to the idea that optimization methods—whether created by humans or nature, whether convex or nonconvex—are exciting objects of study and, often lead to useful algorithms and insights into nature. This study can be seen as an extension of classical mathematical fields such as dynamical systems and differential equations among others, but with the important addition of the notion of computational efficiency.
We will report on interesting research directions and open problems, and highlight progress that has been made. We will write articles ourselves as well as encourage others to contribute. In doing so, we hope to generate an active dialog between theorists, scientists and practitioners and to motivate a generation of young researchers to work on these important problems.
Contributing an article
If you’re writing an article for this blog, please follow these guidelines.








      Theme available on Github.
    








"
5,"

AI Safety Fundamentals Course















































        Your browser doesn't support iframes
    

","



The Control Group Is Out Of Control | Slate Star Codex
































































Home

About / Top Posts
Archives
Top Posts

Comments Feed
RSS Feed





Slate Star Codex









Blogroll 
Economics

Artir Kel
Bryan Caplan
David Friedman
Pseudoerasmus
Scott Sumner
Tyler Cowen


Effective Altruism

80000 Hours Blog
Effective Altruism Forum
GiveWell Blog


Rationality

Alyssa Vance
Beeminder
Elizabeth Van Nostrand
Gwern Branwen
Jacob Falkovich
Jeff Kaufman
Katja Grace
Kelsey Piper
Less Wrong
Paul Christiano
Robin Hanson
Sarah Constantin
Zack Davis
Zvi Mowshowitz


Science

Andrew Gelman
Greg Cochran
Michael Caton
Razib Khan
Scott Aaronson
Stephan Guyenet
Steve Hsu


SSC Elsewhere

SSC Discord Server
SSC Podcast
SSC Subreddit
Unsong


Archives

January 2021
September 2020
June 2020
May 2020
April 2020
March 2020
February 2020
January 2020
December 2019
November 2019
October 2019
September 2019
August 2019
July 2019
June 2019
May 2019
April 2019
March 2019
February 2019
January 2019
December 2018
November 2018
October 2018
September 2018
August 2018
July 2018
June 2018
May 2018
April 2018
March 2018
February 2018
January 2018
December 2017
November 2017
October 2017
September 2017
August 2017
July 2017
June 2017
May 2017
April 2017
March 2017
February 2017
January 2017
December 2016
November 2016
October 2016
September 2016
August 2016
July 2016
June 2016
May 2016
April 2016
March 2016
February 2016
January 2016
December 2015
November 2015
October 2015
September 2015
August 2015
July 2015
June 2015
May 2015
April 2015
March 2015
February 2015
January 2015
December 2014
November 2014
October 2014
September 2014
August 2014
July 2014
June 2014
May 2014
April 2014
March 2014
February 2014
January 2014
December 2013
November 2013
October 2013
September 2013
August 2013
July 2013
June 2013
May 2013
April 2013
March 2013
February 2013

Full Archives
 




The Control Group Is Out Of Control

Posted on April 28, 2014 by Scott Alexander 

I.
Allan Crossman calls parapsychology the control group for science.
That is, in let’s say a drug testing experiment, you give some people the drug and they recover. That doesn’t tell you much until you give some other people a placebo drug you know doesn’t work – but which they themselves believe in – and see how many of them recover. That number tells you how many people will recover whether the drug works or not. Unless people on your real drug do significantly better than people on the placebo drug, you haven’t found anything.
On the meta-level, you’re studying some phenomenon and you get some positive findings. That doesn’t tell you much until you take some other researchers who are studying a phenomenon you know doesn’t exist – but which they themselves believe in – and see how many of them get positive findings. That number tells you how many studies will discover positive results whether the phenomenon is real or not. Unless studies of the real phenomenon do significantly better than studies of the placebo phenomenon, you haven’t found anything.
Trying to set up placebo science would be a logistical nightmare. You’d have to find a phenomenon that definitely doesn’t exist, somehow convince a whole community of scientists across the world that it does, and fund them to study it for a couple of decades without them figuring it out.
Luckily we have a natural experiment in terms of parapsychology – the study of psychic phenomena – which most reasonable people believe don’t exist, but which a community of practicing scientists believes in and publishes papers on all the time.
The results are pretty dismal. Parapsychologists are able to produce experimental evidence for psychic phenomena about as easily as normal scientists are able to produce such evidence for normal, non-psychic phenomena. This suggests the existence of a very large “placebo effect” in science – ie with enough energy focused on a subject, you can always produce “experimental evidence” for it that meets the usual scientific standards. As Eliezer Yudkowsky puts it:
Parapsychologists are constantly protesting that they are playing by all the standard scientific rules, and yet their results are being ignored – that they are unfairly being held to higher standards than everyone else. I’m willing to believe that. It just means that the standard statistical methods of science are so weak and flawed as to permit a field of study to sustain itself in the complete absence of any subject matter.
These sorts of thoughts have become more common lately in different fields. Psychologists admit to a crisis of replication as some of their most interesting findings turn out to be spurious. And in medicine, John Ioannides and others have been criticizing the research for a decade now and telling everyone they need to up their standards.
“Up your standards” has been a complicated demand that cashes out in a lot of technical ways. But there is broad agreement among the most intelligent voices I read (1, 2, 3, 4, 5) about a couple of promising directions we could go:
1. Demand very large sample size.
2. Demand replication, preferably exact replication, most preferably multiple exact replications.
3. Trust systematic reviews and meta-analyses rather than individual studies. Meta-analyses must prove homogeneity of the studies they analyze.
4. Use Bayesian rather than frequentist analysis, or even combine both techniques.
5. Stricter p-value criteria. It is far too easy to massage p-values to get less than 0.05. Also, make meta-analyses look for “p-hacking” by examining the distribution of p-values in the included studies.
6. Require pre-registration of trials.
7. Address publication bias by searching for unpublished trials, displaying funnel plots, and using statistics like “fail-safe N” to investigate the possibility of suppressed research.
8. Do heterogeneity analyses or at least observe and account for differences in the studies you analyze.
9. Demand randomized controlled trials. None of this “correlated even after we adjust for confounders” BS.
10. Stricter effect size criteria. It’s easy to get small effect sizes in anything.
If we follow these ten commandments, then we avoid the problems that allowed parapsychology and probably a whole host of other problems we don’t know about to sneak past the scientific gatekeepers.
Well, what now, motherfuckers?
II.
Bem, Tressoldi, Rabeyron, and Duggan (2014), full text available for download at the top bar of the link above, is parapsychology’s way of saying “thanks but no thanks” to the idea of a more rigorous scientific paradigm making them quietly wither away.
You might remember Bem as the prestigious establishment psychologist who decided to try his hand at parapsychology and to his and everyone else’s surprise got positive results. Everyone had a lot of criticisms, some of which were very very good, and the study failed replication several times. Case closed, right?
Earlier this month Bem came back with a meta-analysis of ninety replications from tens of thousands of participants in thirty three laboratories in fourteen countries confirming his original finding, p < 1.2 * -1010, Bayes factor 7.4 * 109, funnel plot beautifully symmetrical, p-hacking curve nice and right-skewed, Orwin fail-safe n of 559, et cetera, et cetera, et cetera.
By my count, Bem follows all of the commandments except [6] and [10]. He apologizes for not using pre-registration, but says it’s okay because the studies were exact replications of a previous study that makes it impossible for an unsavory researcher to change the parameters halfway through and does pretty much the same thing. And he apologizes for the small effect size but points out that some effect sizes are legitimately very small, this is no smaller than a lot of other commonly-accepted results, and that a high enough p-value ought to make up for a low effect size.
This is far better than the average meta-analysis. Bem has always been pretty careful and this is no exception. Yet its conclusion is that psychic powers exist.
So – once again – what now, motherfuckers?
III.
In retrospect, that list of ways to fix science above was a little optimistic.
The first nine items (large sample sizes, replications, low p-values, Bayesian statistics, meta-analysis, pre-registration, publication bias, heterogeneity) all try to solve the same problem: accidentally mistaking noise in the data for a signal.
We’ve placed so much emphasis on not mistaking noise for signal that when someone like Bem hands us a beautiful, perfectly clear signal on a silver platter, it briefly stuns us. “Wow, of the three hundred different terrible ways to mistake noise for signal, Bem has proven beyond a shadow of a doubt he hasn’t done any of them.” And we get so stunned we’re likely to forget that this is only part of the battle.
Bem definitely picked up a signal. The only question is whether it’s a signal of psi, or a signal of poor experimental technique.
None of these commandments even touch poor experimental technique – or confounding, or whatever you want to call it. If an experiment is confounded, if it produces a strong signal even when its experimental hypothesis is true, then using a larger sample size will just make that signal even stronger. 
Replicating it will just reproduce the confounded results again. 
Low p-values will be easy to get if you perform the confounded experiment on a large enough scale.
Meta-analyses of confounded studies will obey the immortal law of “garbage in, garbage out”.
Pre-registration only assures that your study will not get any worse than it was the first time you thought of it, which may be very bad indeed.
Searching for publication bias only means you will get all of the confounded studies, instead of just some of them.
Heterogeneity just tells you whether all of the studies were confounded about the same amount. 
Bayesian statistics, alone among these first eight, ought to be able to help with this problem. After all, a good Bayesian should be able to say “Well, I got some impressive results, but my prior for psi is very low, so this raises my belief in psi slightly, but raises my belief that the experiments were confounded a lot.”
Unfortunately, good Bayesians are hard to come by, and the researchers here seem to be making some serious mistakes. Here’s Bem:
An opportunity to calculate an approximate answer to this question emerges from a Bayesian critique of Bem’s (2011) experiments by Wagenmakers, Wetzels, Borsboom, & van der Maas (2011). Although Wagenmakers et al. did not explicitly claim psi to be impossible, they came very close by setting their prior odds at 10^20 against the psi hypothesis. The Bayes Factor for our full database is approximately 10^9 in favor of the psi hypothesis (Table 1), which implies that our meta-analysis should lower their posterior odds against the psi hypothesis to 10^11
Let me shame both participants in this debate.
Bem, you are abusing Bayes factor. If Wagenmakers uses your 10^9 Bayes factor to adjust from his prior of 10^-20 to 10^-11, then what happens the next time you come up with another database of studies supporting your hypothesis? We all know you will, because you’ve amply proven these results weren’t due to chance, so whatever factor produced these results – whether real psi or poor experimental technique – will no doubt keep producing them for the next hundred replication attempts. When those come in, does Wagenmakers have to adjust his probability from 10^-11 to 10^-2? When you get another hundred studies, does he have to go from 10^-2 to 10^7? If so, then by conservation of expected evidence he should just update to 10^+7 right now – or really to infinity, since you can keep coming up with more studies till the cows come home. But in fact he shouldn’t do that, because at some point his thought process becomes “Okay, I already know that studies of this quality can consistently produce positive findings, so either psi is real or studies of this quality aren’t good enough to disprove it”. This point should probably happen well before he increases his probability by a factor of 10^9. See Confidence Levels Inside And Outside An Argument for this argument made in greater detail.
Wagenmakers, you are overconfident. Suppose God came down from Heaven and said in a booming voice “EVERY SINGLE STUDY IN THIS META-ANALYSIS WAS CONDUCTED PERFECTLY WITHOUT FLAWS OR BIAS, AS WAS THE META-ANALYSIS ITSELF.” You would see a p-value of less than 1.2 * 10^-10 and think “I bet that was just coincidence”? And then they could do another study of the same size, also God-certified, returning exactly the same results, and you would say “I bet that was just coincidence too”? YOU ARE NOT THAT CERTAIN OF ANYTHING. Seriously, read the @#!$ing Sequences.
Bayesian statistics, at least the way they are done here, aren’t gong to be of much use to anybody.
That leaves randomized controlled trials and effect sizes.
Randomized controlled trials are great. They eliminate most possible confounders in one fell swoop, and are excellent at keeping experimenters honest. Unfortunately, most of the studies in the Bem meta-analysis were already randomized controlled trials.
High effect sizes are really the only thing the Bem study lacks. And it is very hard to experimental technique so bad that it consistently produces a result with a high effect size.
But as Bem points out, demanding high effect size limits our ability to detect real but low-effect phenomena. Just to give an example, many physics experiments – like the ones that detected the Higgs boson or neutrinos – rely on detecting extremely small perturbations in the natural order, over millions of different trials. Less esoterically, Bem mentions the example of aspirin decreasing heart attack risk, which it definitely does and which is very important, but which has an effect size lower than that of his psi results. If humans have some kind of very weak psionic faculty that under regular conditions operates poorly and inconsistently, but does indeed exist, then excluding it by definition from the realm of things science can discover would be a bad idea.
All of these techniques are about reducing the chance of confusing noise for signal. But when we think of them as the be-all and end-all of scientific legitimacy, we end up in awkward situations where they come out super-confident in a study’s accuracy simply because the issue was one they weren’t geared up to detect. Because a lot of the time the problem is something more than just noise.
IV.
Wiseman & Schlitz’s Experimenter Effects And The Remote Detection Of Staring is my favorite parapsychology paper ever and sends me into fits of nervous laughter every time I read it.
The backstory: there is a classic parapsychological experiment where a subject is placed in a room alone, hooked up to a video link. At random times, an experimenter stares at them menacingly through the video link. The hypothesis is that this causes their galvanic skin response (a physiological measure of subconscious anxiety) to increase, even though there is no non-psychic way the subject could know whether the experimenter was staring or not. 
Schiltz is a psi believer whose staring experiments had consistently supported the presence of a psychic phenomenon. Wiseman, in accordance with nominative determinism is a psi skeptic whose staring experiments keep showing nothing and disproving psi. Since they were apparently the only two people in all of parapsychology with a smidgen of curiosity or rationalist virtue, they decided to team up and figure out why they kept getting such different results.
The idea was to plan an experiment together, with both of them agreeing on every single tiny detail. They would then go to a laboratory and set it up, again both keeping close eyes on one another. Finally, they would conduct the experiment in a series of different batches. Half the batches (randomly assigned) would be conducted by Dr. Schlitz, the other half by Dr. Wiseman. Because the two authors had very carefully standardized the setting, apparatus and procedure beforehand, “conducted by” pretty much just meant greeting the participants, giving the experimental instructions, and doing the staring.
The results? Schlitz’s trials found strong evidence of psychic powers, Wiseman’s trials found no evidence whatsoever.
Take a second to reflect on how this makes no sense. Two experimenters in the same laboratory, using the same apparatus, having no contact with the subjects except to introduce themselves and flip a few switches – and whether one or the other was there that day completely altered the result. For a good time, watch the gymnastics they have to do to in the paper to make this sound sufficiently sensical to even get published. This is the only journal article I’ve ever read where, in the part of the Discussion section where you’re supposed to propose possible reasons for your findings, both authors suggest maybe their co-author hacked into the computer and altered the results.
While it’s nice to see people exploring Bem’s findings further, this is the experiment people should be replicating ninety times. I expect something would turn up. 
As it is, Kennedy and Taddonio list ten similar studies with similar results. One cannot help wondering about publication bias (if the skeptic and the believer got similar results, who cares?). But the phenomenon is sufficiently well known in parapsychology that it has led to its own host of theories about how skeptics emit negative auras, or the enthusiasm of a proponent is a necessary kindling for psychic powers.
Other fields don’t have this excuse. In psychotherapy, for example, practically the only consistent finding is that whatever kind of psychotherapy the person running the study likes is most effective. Thirty different meta-analyses on the subject have confirmed this with strong effect size (d = 0.54) and good significance (p = .001).
Then there’s Munder (2013), which is a meta-meta-analysis on whether meta-analyses of confounding by researcher allegiance effect were themselves meta-confounded by meta-researcher allegiance effect. He found that indeed, meta-researchers who believed in researcher allegiance effect were more likely to turn up positive results in their studies of researcher allegiance effect (p < .002). 

It gets worse. There's a famous story about an experiment where a scientist told teachers that his advanced psychometric methods had predicted a couple of kids in their class were about to become geniuses (the students were actually chosen at random). He followed the students for the year and found that their intelligence actually increased. This was supposed to be a Cautionary Tale About How Teachers’ Preconceptions Can Affect Children.
Less famous is that the same guy did the same thing with rats. He sent one laboratory a box of rats saying they were specially bred to be ultra-intelligent, and another lab a box of (identical) rats saying they were specially bred to be slow and dumb. Then he had them do standard rat learning tasks, and sure enough the first lab found very impressive results, the second lab very disappointing ones.
This scientist – let’s give his name, Robert Rosenthal – then investigated three hundred forty five different studies for evidence of the same phenomenon. He found effect sizes of anywhere from 0.15 to 1.7, depending on the type of experiment involved. Note that this could also be phrased as “between twice as strong and twenty times as strong as Bem’s psi effect”. Mysteriously, animal learning experiments displayed the highest effect size, supporting the folk belief that animals are hypersensitive to subtle emotional cues.
Okay, fine. Subtle emotional cues. That’s way more scientific than saying “negative auras”. But the question remains – what went wrong for Schlitz and Wiseman? Even if Schlitz had done everything short of saying “The hypothesis of this experiment is for your skin response to increase when you are being stared at, please increase your skin response at that time,” and subjects had tried to comply, the whole point was that they didn’t know when they were being stared at, because to find that out you’d have to be psychic. And how are these rats figuring out what the experimenters’ subtle emotional cues mean anyway? I can’t figure out people’s subtle emotional cues half the time!
I know that standard practice here is to tell the story of Clever Hans and then say That Is Why We Do Double-Blind Studies. But first of all, I’m pretty sure no one does double-blind studies with rats. Second of all, I think most social psych studies aren’t double blind – I just checked the first one I thought of, Aronson and Steele on stereotype threat, and it certainly wasn’t. Third of all, this effect seems to be just as common in cases where it’s hard to imagine how the researchers’ subtle emotional cues could make a difference. Like Schlitz and Wiseman. Or like the psychotherapy experiments, where most of the subjects were doing therapy with individual psychologists and never even saw whatever prestigious professor was running the study behind the scenes.
I think it’s a combination of subconscious emotional cues, subconscious statistical trickery, perfectly conscious fraud which for all we know happens much more often than detected, and things we haven’t discovered yet which are at least as weird as subconscious emotional cues. But rather than speculate, I prefer to take it as a brute fact. Studies are going to be confounded by the allegiance of the researcher. When researchers who don’t believe something discover it, that’s when it’s worth looking into.
V.
So what exactly happened to Bem?
Although Bem looked hard to find unpublished material, I don’t know if he succeeded. Unpublished material, in this context, has to mean “material published enough for Bem to find it”, which in this case was mostly things presented at conferences. What about results so boring that they were never even mentioned?
And I predict people who believe in parapsychology are more likely to conduct parapsychology experiments than skeptics. Suppose this is true. And further suppose that for some reason, experimenter effect is real and powerful. That means most of the experiments conducted will support Bem’s result. But this is still a weird form of “publication bias” insofar as it ignores the contrary results of hypotheticaly experiments that were never conducted.
And worst of all, maybe Bem really did do an excellent job of finding every little two-bit experiment that no journal would take. How much can we trust these non-peer-reviewed procedures?
I looked through his list of ninety studies for all the ones that were both exact replications and had been peer-reviewed (with one caveat to be mentioned later). I found only seven:
Batthyany, Kranz, and Erber: .268
Ritchie 1: 0.015
Ritchie 2: -0.219
Richie 3: -0.040
Subbotsky 1: 0.279
Subbotsky 2: 0.292
Subbotsky 3: -.399
Three find large positive effects, two find approximate zero effects, and two find large negative effects. Without doing any calculatin’, this seems pretty darned close to chance for me.
Okay, back to that caveat about replications. One of Bem’s strongest points was how many of the studies included were exact replications of his work. This is important because if you do your own novel experiment, it leaves a lot of wiggle room to keep changing the parameters and statistics a bunch of times until you get the effect you want. This is why lots of people want experiments to be preregistered with specific committments about what you’re going to test and how you’re going to do it. These experiments weren’t preregistered, but conforming to a previously done experiment is a pretty good alternative.
Except that I think the criteria for “replication” here were exceptionally loose. For example, Savva et al was listed as an “exact replication” of Bem, but it was performed in 2004 – seven years before Bem’s original study took place. I know Bem believes in precognition, but that’s going too far. As far as I can tell “exact replication” here means “kinda similar psionic-y thing”. Also, Bem classily lists his own experiments as exact replications of themselves, which gives a big boost to the “exact replications return the same results as Bem’s original studies” line. I would want to see much stricter criteria for replication before I relax the “preregister your trials” requirement.
(Richard Wiseman – the same guy who provided the negative aura for the Wiseman and Schiltz experiment – has started a pre-register site for Bem replications. He says he has received five of them. This is very promising. There is also a separate pre-register for parapsychology trials in general. I am both extremely pleased at this victory for good science, and ashamed that my own field is apparently behind parapsychology in the “scientific rigor” department)
That is my best guess at what happened here – a bunch of poor-quality, peer-unreviewed studies that weren’t as exact replications as we would like to believe, all subject to mysterious experimenter effects.
This is not a criticism of Bem or a criticism of parapsychology. It’s something that is inherent to the practice of meta-analysis, and even more, inherent to the practice of science. Other than a few very exceptional large medical trials, there is not a study in the world that would survive the level of criticism I am throwing at Bem right now.
I think Bem is wrong. The level of criticism it would take to prove a wrong study wrong is higher than that almost any existing study can withstand. That is not encouraging for existing studies.
VI.
The motto of the Royal Society – Hooke, Boyle, Newton, some of the people who arguably invented modern science – was nullus in verba, “take no one’s word”.
This was a proper battle cry for seventeenth century scientists. Think about the (admittedly kind of mythologized) history of Science. The scholastics saying that matter was this, or that, and justifying themselves by long treatises about how based on A, B, C, the word of the Bible, Aristotle, self-evident first principles, and the Great Chain of Being all clearly proved their point. Then other scholastics would write different long treatises on how D, E, and F, Plato, St. Augustine, and the proper ordering of angels all indicated that clearly matter was something different. Both groups were pretty sure that the other had make a subtle error of reasoning somewhere, and both groups were perfectly happy to spend centuries debating exactly which one of them it was.
And then Galileo said “Wait a second, instead of debating exactly how objects fall, let’s just drop objects off of something really tall and see what happens”, and after that, Science.
Yes, it’s kind of mythologized. But like all myths, it contains a core of truth. People are terrible. If you let people debate things, they will do it forever, come up with horrible ideas, get them entrenched, play politics with them, and finally reach the point where they’re coming up with theories why people who disagree with them are probably secretly in the pay of the Devil. 
Imagine having to conduct the global warming debate, except that you couldn’t appeal to scientific consensus and statistics because scientific consensus and statistics hadn’t been invented yet. In a world without science, everything would be like that.
Heck, just look at philosophy.
This is the principle behind the Pyramid of Scientific Evidence. The lowest level is your personal opinions, no matter how ironclad you think the logic behind them is. Just above that is expert opinion, because no matter how expert someone is they’re still only human. Above that is anecdotal evidence and case studies, because even though you’re finally getting out of people’s heads, it’s still possible for the content of people’s heads to influence which cases they pay attention to. At each level, we distill away more and more of the human element, until presumably at the top the dross of humanity has been purged away entirely and we end up with pure unadulterated reality.

The Pyramid of Scientific Evidence
And for a while this went well. People would drop things off towers, or see how quickly gases expanded, or observe chimpanzees, or whatever.
Then things started getting more complicated. People started investigating more subtle effects, or effects that shifted with the observer. The scientific community became bigger, everyone didn’t know everyone anymore, you needed more journals to find out what other people had done. Statistics became more complicated, allowing the study of noisier data but also bringing more peril. And a lot of science done by smart and honest people ended up being wrong, and we needed to figure out exactly which science that was.
And the result is a lot of essays like this one, where people who think they’re smart take one side of a scientific “controversy” and say which studies you should believe. And then other people take the other side and tell you why you should believe different studies than the first person thought you should believe. And there is much argument and many insults and citing of authorities and interminable debate for, if not centuries, at least a pretty long time.
The highest level of the Pyramid of Scientific Evidence is meta-analysis. But a lot of meta-analyses are crap. This meta-analysis got p < 1.2 * 10^-10 for a conclusion I'm pretty sure is false, and it isn’t even one of the crap ones. Crap meta-analyses look more like this, or even worse. 
How do I know it’s crap? Well, I use my personal judgment. How do I know my personal judgment is right? Well, a smart well-credentialed person like James Coyne agrees with me. How do I know James Coyne is smart? I can think of lots of cases where he’s been right before. How do I know those count? Well, John Ioannides has published a lot of studies analyzing the problems with science, and confirmed that cases like the ones Coyne talks about are pretty common. Why can I believe Ioannides’ studies? Well, there have been good meta-analyses of them. But how do I know if those meta-analyses are crap or not? Well…

The Ouroboros of Scientific Evidence
Science! YOU WERE THE CHOSEN ONE! It was said that you would destroy reliance on biased experts, not join them! Bring balance to epistemology, not leave it in darkness! 

I LOVED YOU!!!!
Edit: Conspiracy theory by Andrew Gelman


						This entry was posted in Uncategorized and tagged long post is long, science, statistics, studies. Bookmark the permalink.											


← Stop Confounding Yourself! Stop Confounding Yourself!
Links For May 2014 →


197 Responses to The Control Group Is Out Of Control
Reverse order




 suntzuanime says: 

			April 28, 2014 at 9:20 pm 
Imagine the global warming debate, but you couldn’t appeal to scientific consensus or statistics because you didn’t really understand the science or the statistics, and you just had to take some people who claimed to know what was going on at their verba.
“Take no one’s word” sounds like a good rallying cry when it comes to dropping a bowling ball and a feather and seeing which hits the ground first, but I don’t have my own global temperature monitoring stations that I’ve been running for the past fifty years, and even if I did I probably wouldn’t be smart enough to know if a climate model based on them was bullshit or not.
I guess this is sort of the point you were making? But it’s weird to cite climate change as a counterexample.
Now I’m wondering if you were just doing the thing where you subtly undercut your own points as you make them out of sheer uncontrollable perversity. This edit button is a curse, not a blessing.








 Oligopsony says: 

			April 28, 2014 at 11:07 pm 
If some of the weirder psi suppression theories are right, psi should actually be easier to study by conducting personal experiments than by trying to study or do public science, especially if you precommit yourself to not telling anyone about the results.








 McGuire says: 

			April 29, 2014 at 12:10 pm 
I myself have taken this a step farther: I precommitted to not performing the experiments at all.
So far, preliminary results are promising.








 Will Newsome says: 

			April 30, 2014 at 1:11 am 
Yeah not testing the Lord seems like a good idea.








 Multiheaded says: 

			May 4, 2014 at 11:11 am 
Will: damn straight! Shudder.












 Douglas Knight says: 

			April 29, 2014 at 2:03 am 
A 20 year project for that purpose? Why doesn’t he seek publicity more often? Did you try the conspiracy theory link at the end?








 Deiseach says: 

			April 29, 2014 at 11:22 am 
Except you have to wait until you get to the moon to drop your bowling ball and feather and have the theory proved right about “they both fall at the same rate because they are both acted upon by the same force”.
Or the counter-factual “Okay, you see those two lights in the sky?  The big one in the day that goes from east to west and looks like it’s moving around us while we’re staying still?  And the small one in the night that moves from east to west and looks like it’s moving around us while we’re staying still?  Yeah, well, you can believe the evidence of your senses about the small one but the big one is actually staying still and we’re moving.  What do you mean, ‘evidence’?  This is SCIENCE!!!”
Scott’s little slap at the Scholastics is all well and good, but even science has to rely on *spit* philosophy when its debating things for which it has not yet got the physical evidence, especially when it won’t be able to back it up with physical evidence for a couple of centuries.








 suntzuanime says: 

			April 29, 2014 at 12:50 pm 
Except you have to wait until you get to the moon to drop your bowling ball and feather and have the theory proved right about “they both fall at the same rate because they are both acted upon by the same force”.
I was just doing  the thing where I subtly undercut my own points as I make them out of sheer uncontrollable perversity. 😀










 Gates VP says: 

			April 29, 2014 at 2:20 pm 
Personally, I think the whole global warming thing is an even bigger mess than you’ve described 🙂
Underpinning “global warming” is “man-made climate change”, which seems like a silly smoke-screen debate. In fact, even the premise of “global warming” is a really silly debate.
There’s a 100% that humans having a dramatic influence on the earth’s climate. And I really mean that 100%. We are clearly performing dramatic modifications to our climate and we’re clearly doing so at a faster rate than previous species.
So back to “global warming”… what if it’s really “global cooling”? Does it actually matter? I mean, what if it was “global cooling”, would we start burning down forests en masse just to “fix” the problem?
All of this debate back and forth is just ignoring the real problem: the planet is changing, how do we want to adapt, how do we want to mold it?
I mean, we want to slow CO2 emissions because they also contain lots of things that unhealthy for us to breathe. Does it really matter if that cools or warms the planet?








 anon says: 

			April 30, 2014 at 9:39 am 
Your comment is one of the worst I’ve ever seen on this website. It matters whether or not CO2 warms or cools the planet.
First, we would need to pursue different policies in response to each possibility because they involve different scenarios for danger. Global warming might be stoppable by throwing giant ice cubes into the ocean, but if there’s global cooling that’s the last thing we should do. Second, they would have different consequences. Maybe warming would kill one billion people while cooling would only kill one million.
If CO2 is causing cooling and cooling is bad, we should want more trees and not fewer. Cooling is the opposite of warming but that doesn’t imply that the opposite of one problem’s solution will solve the other.
Policies aren’t determined in a vacuum. If the only harmful consequence of CO2 is its effects on our lungs, it might not be a good to switch to other types of energy because the downsides might outweigh the upsides.








 Gates VP says: 

			April 30, 2014 at 4:07 pm 
If CO2 is causing cooling and cooling is bad…
This is the core issue. We have no way coherent strategy for calling the change “good” or “bad”.
Global warming might be stoppable by throwing giant ice cubes into the ocean,…
There’s also the assumption here that we could come up with a plan for to fix something that we’re not really sure is broken.
Is the goal here really to keep the earth at a static temperature for the remainder of human existence?










 MugaSofer says: 

			April 30, 2014 at 1:23 pm 
“There’s a 100% that humans having a dramatic influence on the earth’s climate. And I really mean that 100%.”
No, you really don’t.
http://www.lesswrong.com/lw/mp/0_and_1_are_not_probabilities/‎








 Gates VP says: 

			April 30, 2014 at 3:27 pm 
Fine, this is a heavy statistics blog.
The only way we can argue this would require a lot of semantics about the definition of the words “climate change”.
We dropped a pair a nuclear bombs in Japan several decades ago. If those effects of those bombs do not count as “climate change” for that region, then we need to be very specific about how we’re going to define “climate change” and attempt to measure the “man-made” effects thereof.










 Ellie Kesselman says: 

			May 4, 2014 at 12:29 am 
Gates, you said,
We are clearly performing dramatic modifications to our climate and…doing so at a faster rate than previous species. So back to “global warming”…what if it’s really “global cooling”? Does it actually matter?
No, it doesn’t matter. Climate is a complex dynamic system. At the very least, it makes sense to conserve non-renewable resources because we’ve nearly depleted them. Agreement and action on some basics like that would help. So would some honesty about carbon credit schemes (neoliberal economics is too easy to game) and boondoggle solar tax credits/government funds that corrupt Green types have used for personal enrichment, repeatedly.
Your comment didn’t deserve to be called “one of the worst ever seen on this website”! This is NOT a heavy statistics website. Arguing over your rhetorical use of Prob(x) = 1.0  is petty. Ignore your detractors here. You are correct; they are more wrong.












 jaimeastorga2000 says: 

			April 28, 2014 at 9:37 pm 
The “Experimenter Effects And The Remote Detection Of Staring” link is broken.








 Vertebrat says: 

			April 29, 2014 at 10:54 pm 
Too many people were staring at it.










 Ken Arromdee says: 

			April 28, 2014 at 9:52 pm 
How do you distinguish
1) Psychic researchers get as many good results as normal researchers because both sets of researchers are equally sloppy, and
2) Psychic researchers get as many good results as normal researchers because psychic researchers are worse at research than normal researchers (raising the level of positive results) but this is compensated for by the fact that psychic powers are not real (reducing the number of positive results)?
In other words, you’re basing this on the premise that psychic researchers are exactly the same as regular researchers except that they’re researching something that’s not going to pan out.  I see little reason to believe this premise.  For instance,  I would not be very surprised if gullibility or carelessness is correlated with belief in psychic powers, and willingness to do psychic experiments is also correlated with belief in psychic powers.








 Scott Alexander says: 

			April 28, 2014 at 9:57 pm 
I doubt psychic researchers are just as good as normal researchers (though a few are) and I agree that if I meant “control group” literally, in terms of trying to find the quantitative effect size of science, this methodology wouldn’t be good enough.
I’m using control group more as a metaphor, where the mistakes of the best parapsychologists can be used as a pointer to figure out what other scientists have to improve.








 Ken Arromdee says: 

			April 30, 2014 at 4:38 pm 
But once you concede that psychic researchers aren’t really like ordinary researchers , you have little reason to believe that psychic researchers will make the same sorts of mistakes that ordinary researchers do.  Even if you find single examples of both types making the same mistake, you have no reason to believe that the distribution is the same among both groups.  It could be that a mistake that is common among psychic researchers is rare among normal ones, and focusing on it is misplaced.
I’m also generally skeptical about using X as a metaphor when X is not actually true.








 Johann says: 

			May 5, 2014 at 4:59 am 
With respect, I provided in my atrociously long reply a series of arguments, with evidence, that parapsychologists are at least as good as mainstream researchers in most respects, and significantly better in others. Skeptics like Chris French concur with me; see his recent talk (https://www.youtube.com/watch?v=ObXWLF6acuw) for evidence of this.
Mosseau (2003), for example, took an empirical approach to this, and compared research in top parapsychology journals like the Journal of Scientific Exploration and the Journal of Parapsychology with mainstream journals such as the Journal of Molecular and Optical Physics, the British Journal of Psychology, etc, finding that, most of the time, fringe research displayed a higher level of conformance to several basic criteria of good science. This includes the reporting of negative results, usage of statistics, rejection of anecdotal evidence, self-correction, overlap with other fields of research, and abstinence from jargon. While I’m aware most of these don’t directly impact quality of experimentation, they do provide respectable evidence that parapsychologists are actually about as careful, in their scientific thinking, as most anyone else.
Moreover, research trying to establish a link between  belief in psi phenomena and measures like IQ and credulity has been for the most part unsuccessful, finding that belief in psi does not vary according to level of education (although belief in superstitions like the power of 13 does).
Finally, there is the fact that skeptics have directly involved themselves in the critique and even design of parapsychological studies. Ganzfeld studies after 1986, for example, owe much of their sophistication and rigor to Ray Hyman, who coauthored a report called the Joint Communique with Honorton, a parapsychologist, where a series of recommendations were specified whose implementation would be convincing, and have been widely adopted today.
I discuss many more examples of sophistication in parapsychological research; see, again, my absurdly large post for these details.














 Sniffnoy says: 

			April 28, 2014 at 10:11 pm 
Some nitpicking:
When you talk about probabilities of 10^7 and such, obviously, this should be odds ratios.  I mean, these are pretty similar when you’re talking about an odds ratio of 10^-11, not so much when it’s 10^7.
Also, some writing nitpicking:
By my count, Bem follows all of the commandments except [2] and [8].
You seem to mean [6] and [10]?  (Rest of paragraph similarly.)
Other fields don’t have this excuse. In psychotherapy, for example, practically the only consistent finding is that whatever kind of psychotherapy the person running the study likes.
You seem to have left out the verb in this sentence?








 Val says: 

			February 4, 2016 at 6:30 am 
“In psychotherapy, for example, practically the only consistent finding is that whatever kind of psychotherapy the person running the study likes.”
What they found was which kind of psychotherapy the experimenter likes.  Not the best-worded the sentence could be, but the point is in there.










 Sniffnoy says: 

			April 28, 2014 at 10:18 pm 
Also: Male Scent May Compromise Biomedical Research.  Not actually related other than being another instance of “science is hard”, but I thought that you’d find it amusing and that it was worth pointing out.








 Eliezer Yudkowsky says: 

			April 28, 2014 at 10:21 pm 
It’s possible that you and I and some of the most experienced scientists and statisticians on the planet could get together and design a procedure for “meta-analysis” which would require actual malice to get wrong.  I’ll be happy to start the discussion by suggesting that step 1 is to convert all the studies into likelihood functions on the same hypothesis space, and step 2 is to realize that the combined likelihood functions rule out all of the hypothesis space, and step 3 is to suggest which clusters of the hypothesis space are well-supported by many studies and to mark which other studies must then have been completely wrong.
Until that time, meta-analyses will go on being bullshit.  They are not the highest level of the scientific pyramid.  They can deliver whatever answer the analyst likes.  When I read about a new meta-analysis I mostly roll my eyes.  Maybe it was honest, sure, but how would I know?  And why would it give the right answer even if the researchers were in fact honest?  You can’t multiply a bunch of likelihood functions and get what a real Bayesian would consider zero everywhere, and from this extract a verdict by the dark magic of frequentist statistics.
I can envision what a real, epistemologically lawful, real-world respecting, probability-theory-obeying meta-analysis would look like.  I mean I couldn’t tell you how to actually set down a method that everyone could follow, I don’t have enough experience with how motivated reasoning plays out in these things and what pragmatic safeguards would be needed to stop it.  But I have some idea what the purely statistical part would look like.  I’ve never seen it done.








 Andrew Hickey says: 

			April 29, 2014 at 8:44 am 
This is the first time I’ve ever wished for an upvote button on a WordPress blog. Everything Eliezer says here.








 Josh H says: 

			April 29, 2014 at 10:46 pm 
Rather than try to come up with an infallible procedure for doing valid science, it might be simpler and more productive to tweak the incentives.  In other words, separate the people who perform the experiments from the people who generate the hypotheses.  I just wrote up some quick thoughts on what that might look like.








 suntzuanime says: 

			April 29, 2014 at 11:13 pm 
The problem with is that in non-pathological science there is a lot of interplay between experimentation and hypothesis-generation. It used to be that science was “do an experiment to figure out how the world works” rather than “decide everything in the world is fire then do an experiment to see if that’s true”. The latter is still better than deciding everything in the world is fire and not bothering with experiment, but it injects a lot of friction, especially into exploratory work.
A slightly modified version of your proposal might separate reaching conclusions from proving them.  You wouldn’t outsource your experimentation, you’d still do your own experiments. But their results would be considered preliminary, and you’d need to have your results replicated by a replication lab in order to be stamped as official science and published in the serious portions of serious journals.








 Josh H says: 

			April 30, 2014 at 12:46 pm 
Yeah, I think that’s a much better way of putting it.  Discovery could still be experimental, but things like “putting it in a peer reviewed journal” could be outsourced.










 Nancy Lebovitz says: 

			April 30, 2014 at 12:43 pm 
Incentives can only select from what people can figure out how to do.
http://www.youtube.com/watch?v=O4f4rX0XEBA
If you don’t want to watch the whole thing, you could start at about 3:18.








 Josh H says: 

			April 30, 2014 at 12:53 pm 
I agree that changing incentives can’t make people start doing something they don’t know how to do.    
People do know how to do experiments to disprove a hypothesis, though.  What they don’t know how to do is systemically prevent experimenter bias from systemically warping the design of and statistical interpretation of such experiments, leading to continual production of false positives.  
If we can set up incentives such that the experimenter’s bias is orthogonal to the hypothesis being true or false, we would still expect some false negatives and false positives, since science is hard, but we’d expect them to statistically average out over time instead of accumulating as entire disciplines worth of non-results.












 Kevin C says: 

			April 30, 2014 at 12:45 am 
step 1 is to convert all the studies into likelihood functions on the same hypothesis space
While I agree that if the procedure you propose were possible it would be helpful, I’m skeptical that step 1 is possible* outside the hardest of sciences. Sure, in physics, math, computer science, and maybe chemistry you can define the hypothesis space clearly. However, once you go even as far as molecular biology ex vivo, the hypothesis space becomes too difficult to measure, much less convert the original English & jargon description of the hypothesis into a proper representation of the hypothesis space. (Some in vitro biology may still be measurable, but as soon as you’re dealing with even the simplest living cells, you’ve got hundreds of proteins that have to be part of the hypothesis space, even if the hypothesis on that dimension is merely “protein X” may be present but has neutral effect on the measured outcomes.)
* That is, not possible for modern day humans. I’m agnostic on whether even a super-human AI could correctly represent the hypothesis space of a molecular biology paper. I think you get into encoding issues before you get to hypotheses that complex.










 Douglas Knight says: 

			April 28, 2014 at 10:43 pm 
There is another way to do placebo science – subtly sabotage the researcher’s experiment. This is routinely done to all real science students (ie, physics majors at maybe 20 schools).
Three find large positive effects, two find approximate zero effects, and two find large negative effects. Without doing any calculatin’, this seems pretty darned close to chance for me.
The effects of chance assuming the null hypothesis are much more specific than the average effect being zero. If your sample sizes are adequate, there should be no large effects, by definition of “adequate.”
━━━━━━━━━
An analogy occurred to me, comparing the pyramid of evidence to Lewis Thomas’s take on medicine. His “Technology of Medicine” said that real medicine requires understanding and allows cheap immediate cures. In contrast, most real-world medicine is expensive use of techniques that barely work and do so for no apparent reason. The pyramid of evidence is purely a product of medicine, attempting to evaluate treatments that have tiny effect sizes. With no understanding, the only evaluation method is large samples. But it is not merely a tool for fake medicine, it is an example of fake science.








 Douglas Knight says: 

			April 28, 2014 at 11:14 pm 
I corrected my citation from Lewis Thomas’s Youngest Science to an essay in his Lives of a Cell. But the specific work is unimportant because you should read everything he wrote. Not just Scott, but also you. Sadly, that is only a few hundred pages.








 gwern says: 

			May 6, 2014 at 5:57 pm 
There is another way to do placebo science – subtly sabotage the researcher’s experiment. This is routinely done to all real science students (ie, physics majors at maybe 20 schools).
I don’t follow. How are physics majors’ experiments being sabotaged?








 Douglas Knight says: 

			May 6, 2014 at 7:58 pm 
The TA comes in at night and miscalibrates equipment. I don’t know the details. It is probably hard to cause qualitative changes, such as to move them into full placebo condition. Instead they get the wrong numeric answer or unexpectedly large error bars.








 gwern says: 

			May 6, 2014 at 8:28 pm 
I see. So this is a systematic practice endorsed by the school and not just some TAs screwing with the little undergrads for lulz?








 Douglas Knight says: 

			May 6, 2014 at 8:48 pm 
systematically endorsed.














 Kevin says: 

			April 28, 2014 at 10:54 pm 
Bem definitely picked up a signal. The only question is whether it’s a signal of psi, or a signal of poor experimental technique.
But as Bem points out, demanding high effect size limits our ability to detect real but low-effect phenomena. Just to give an example, many physics experiments – like the ones that detected the Higgs boson or neutrinos – rely on detecting extremely small perturbations in the natural order, over millions of different trials.
The point I’m about to mention, suggested by these two excerpts, is mostly covered in the Experimenter Effect section, but in a way that seems somewhat indirect to me. That point is systematic uncertainty. Particle physics experiments can confidently capture small effects because – in addition to commandments 1, 2, 4, 5, among others – we spend a great deal of time measuring biases in our detectors. Time spent assessing systematic uncertainty can easily make up the majority of a data analysis project. The failure to find (and correct or mitigate, if possible) systematic biases can give us results like faster-than-light neutrinos.
Of course, it is much easier to give this advice than to take it and apply it to messy things like medicine or psychology. I freely admit that I would barely know where to start when it comes to such fields. Systematic uncertainty is an important topic in this type of discussion, though.








 Chris Hallquist says: 

			April 28, 2014 at 11:50 pm 
Can anyone think of a remotely sensible explanation for the Wiseman and Schlitz result? Right now, “skeptics emit negative auras, or the enthusiasm of a proponent is a necessary kindling for psychic powers” is looking pretty good.
If someone were raising money to fund a replication of this experiment, I would totally consider donating.








 nydwracu says: 

			April 29, 2014 at 2:09 am 
Or psi ability is distributed unequally, and people with more of it observe/notice it firsthand and so are more likely to believe in it. Or psi doesn’t exist and the RNG has a sense of humor.
Or:
Most participants were run by whichever experimenters was free to carry out the session, however, on a few ocassions (e.g., when a participant was a friend or collegue of one of the experimeters) the experimenter would be designated in advance of the trial. Thus most participants were assigned to experimenters in an opportunistic, rather than properly ‘randomised’ (e.g., via random number tables or the output of an RNG), way.
Something really weird could have happened there, but I have no idea what bias could have been added by that that would produce those results.
It’s probably the RNG’s sense of humor. But it would be interesting to see someone steelman psi.








 Anonymous says: 

			April 29, 2014 at 2:20 am 
I think the hypothesis was that some of the watched had psi powers and could detect the watchers.  But if the experiment accidentally picked up on Schlitz’s psi ability to make people uncomfortable over a video link, that would be amusing.








 Avantika says: 

			April 29, 2014 at 3:45 am 
This seems like a simple and actually reasonable-sounding explanation.










 ckp says: 

			April 29, 2014 at 7:38 am 
But if psi is real, then their psi powers might be affecting the RNG that assigns them to experimenters.
~spooky~








 Oligopsony says: 

			April 29, 2014 at 1:40 pm 
Or psi doesn’t exist and the RNG has a sense of humor.
That psi is semi-agentic and mostly seeks to fuck with people is something that parapsychologists have seriously considered.










 Scott Alexander says: 

			April 29, 2014 at 6:25 am 
Yeah, I don’t know. This is one place where, contrary to the spirit of this post, I’m pretty willing to accept “they got a significant result by coincidence”. I’d also donate to a replication.








 Shmi Nux says: 

			April 29, 2014 at 2:13 pm 
Replicating this experiment is a wrong way to go. It is designed to detect psi, but instead uncovers a more interesting effect of participant-dependence of psi-detection, which is worth studying, by constructing a separate experiment explicitly for that purpose. Once the dependence part is figured out it makes sense to review the original protocol.








 nydwracu says: 

			April 29, 2014 at 11:17 pm 
If they got a significant result by coincidence, what of their earlier results? It’s possible to explain it by saying that Schlitz had consistently bad methods and they got a significant result by coincidence…
It could also be a really weird chemical thing somehow? Like, if intending to creep someone out results in subconscious emission of chemicals that can produce the effect of feeling creeped out in someone sitting in a room #{distance} away? (I think of that because of that rat study.)










 Deiseach says: 

			April 29, 2014 at 11:30 am 
Well, if the placebo effect works positively, in that you can think yourself better if given a sugar pill and told it’s a powerful new medicine, maybe there’s a negative effect as well?
Perhaps “skeptics interfere with the vibrations” isn’t just an excuse by fraudulent mediums as to why they can’t produce effects (translation: they don’t dare try their conjuring tricks) in the presence of investigators.
If a skeptic is running an experiment with the conscious attitude “I am doing impartial science here” but all the time in the back of his mind he’s thinking “This is hooey, I know there’s no such thing as telepathy/precognition/what have you, this is not going to work”, maybe that really does trigger some kind of observer effect?
(I’m not even going to try and untangle Schrodinger’s cat where if you go in with a strong expectation that the cat is dead, would this skew the likelihood of the cat being dead when you open the box beyond what you’d expect from chance?)
To be fair, I’m sceptical myself about measuring galvanic skin changes; I wouldn’t hang a rabid dog on the evidence of a “lie detector”, and I’m as unconvinced as Chesterton’s Fr. Brown in the 1914 story “The Mistake of the Machine”:
“I’ve been reading,” said Flambeau, “of this new psychometric method they talk about so much, especially in America.  You know what I mean; they put a pulsometer on a man’s wrist and judge by how his heart goes at the pronunciation of certain words.  What do you think of it?”
“I think it very interesting,” replied Father Brown; “it reminds me of that interesting idea in the Dark Ages that blood would flow from a corpse if the murderer touched it.”
“Do you really mean,” demanded his friend, “that you think the two methods equally valuable?”
“I think them equally valueless,” replied Brown.  “Blood flows, fast or slow, in dead folk or living, for so many more million reasons than we can ever know.  Blood will have to flow very funnily; blood will have to flow up the Matterhorn, before I will take it as a sign that I am to shed it.”
“The method,” remarked the other, “has been guaranteed by some of the greatest American men of science.”
“What sentimentalists men of science are!” exclaimed Father Brown, “and how much more sentimental must American men of science be!  Who but a Yankee would think of proving anything from heart-throbs?  Why, they must be as sentimental as a man who thinks a woman is in love with him if she blushes.  That’s a test from the circulation of the blood, discovered by the immortal Harvey; and a jolly rotten test, too.” 








 nydwracu says: 

			April 29, 2014 at 11:19 pm 
Doesn’t matter if galvanic skin changes aren’t related to anything — if they aren’t a sign of any change in mental state or similar, but can still be affected by psi, then there’s still something that is affected by psi.










 moridinamael says: 

			April 29, 2014 at 1:33 pm 
Let’s say for the sake of argument that Schlitz gave off weird, nervousness-inducing vibes.  Interacting with him made their galvanic response fluctuate more *in general,* as they sit in the testing room thinking about what a creep he is.  So when Schlitz was staring at them, the meters are recording samples which are going to be sampling from a different distribution of agitation valence. Maybe they picked a bad statistical lumping function on top of this.
This was the first thing I thought of.








 Anonymous says: 

			May 22, 2014 at 2:08 am 
It’s possible the 20m distance and however many walls wasn’t enough for sensory isolation, and one of the starers made detectable sound when moving to look at/away from the screen.  
Blinding the sender to the experimental condition would avoid both accidental and malicious back channels like this.  One possible design would be to have the video be sometimes delayed by 30 seconds, which would let you separate the effect of “the receiver is being watched” and “the sender thinks they’re watching”.








 Jonas says: 

			February 6, 2016 at 4:47 pm 
One possible design would be to have the video be sometimes delayed by 30 seconds
If the average time of travel from greeting the researcher to entering the view of the camera is small enough that a 30 second difference is noticeable, there is still a signal (“how long until I see the person on the screen?”) which the “sender” could pick up on. Idea for getting around this: have a stooge delay people 30 seconds in the hall if their video isn’t delayed, or have them sit in one of two rooms with unequal distance, the video delay = the average travel time differential.  Or just instruct the sender to delay turning on his screen for X seconds, where X >= the slowest plausible travel time plus the video delay.












 John says: 

			April 29, 2014 at 12:30 am 
“In psychotherapy, for example, practically the only consistent finding is that whatever kind of psychotherapy the person running the study likes. ” – I think you accidentally a word.








 Sarah says: 

			April 29, 2014 at 1:04 am 
I think this is an argument for including cruder, common-sense heuristics for thinking about scientific studies.
*Effect size. If the effect is not *very* large and the results *very* unequivocal, you probably either have an illusory result or an artifact of misunderstood structure (A appears to *sometimes* cause B if A is really two things, A1 and A2, and only A2 causes B.)
*Physical plausibility. You don’t believe in psi because it violates physics. You also shouldn’t believe that drugs that don’t cross the blood-brain barrier can have effects on the brain.  
*Analogy.  If people have been finding MS “cures” for decades, the next MS cure isn’t so credible.
*Multiple independent lines of reasoning. Evolutionary, biochemical, and physiological arguments pointing in the same direction.  Especially simple arguments that are hard to screw up.
*Motivation. Cui Bono. Yes, we care who funded it.
I think what we’re finding is that *blind* science, automatable science, “studies say X and this quickly-checkable rubric confirms the studies are good”, isn’t a good filter. 
To be fair, rubrics could stand a lot of improvement.  (Cochrane, btw, *does* pay a lot of attention to experimental design.)  I do think an ideal meta-analysis could do a lot better than the average meta-analysis.
But the purpose of methodology is to abstract away personal opinion. We don’t do this *just* to better approach truth. We also do it to avoid getting in fights.  We want to be able to claim to be impersonal, to be following a documented set of rules.  In an era where the volume of science is such that all metrics will be gamed and cargo-culted, methodology may just not be enough.  Old-fashioned, pre-modern heuristics like “is this an honest man?” and “does this make sense?” are unreliable, to be sure, but they’re unreliable in a different way than statistics and procedures, and it may be time to consider their value.








 Michael Edward Vassar says: 

			April 29, 2014 at 1:50 pm 
The hardcore take on funding bias is to just consider any study unworthy of consideration if it was funded *at all*.  That’s actually what I’d like to enable, and what everything I’m working on is an attempt to build towards.








 Jonas says: 

			February 6, 2016 at 5:04 pm 
consider any study unworthy of consideration if it was funded at all
Reading it literally, that is only possible if the study doesn’t incur any expenses. Here are some potential expenses: researcher salary, subject compensation, equipment.
I can see how you do science with no researcher salary, if the researchers themselves are independently wealthy* or have the necessary free time, and I can see uncompensated subjects participating for the fun or to promote knowledge. But building all your equipment (e.g. particle accelerators or microscopes) from scratch, no buying of any components, not harnessing specialization and trade? ‘You nuts?
* That would make science a hobby of the aristocracy, just like in the Good Old Reactionary Days (I’m not a (neo-)reactionary).
Something I would expect from a Normal Person—you know, the kind who mostly doesn’t comment on blogs like this one—would be to at least allow self-funding, i.e. allowing the aristocrats and hobbyists to buy their own equipment. Maybe what you meant was to taboo funding other than through The Right Channel, which is what? Government subsidies to basic science? But why is government agenda less distorting than other agendas? Okay, suppose it isn’t and gives the individual researchers free reign; why is their agendas less distorting?
If your goal is for agenda influences to all wash out and simply fund people who (in aggregate) do Pure Truth-Seeking Science, it’s not clear that self-funding or un-funding or government-funding or government-plus-private-funding is the right (or wrong) way to achieve that effect.
Comments? Have I horribly distorted what you said?










 Anonymous says: 

			April 29, 2014 at 3:09 pm 
The problem with physics argument is that there could be yet unknown mechanism causing the effect. Like drugs that don’t cross blood-brain barrier but would happen to be radioactive. If we wouldn’t know about radioactivity and couldn’t separate radioactive drugs from others, it would seem like violation of physics and could happen in just some labs..










 Jacob Steinhardt says: 

			April 29, 2014 at 1:11 am 
In regards to the Wiseman & Schlitz paper, the sample size is quite small and p-value is only 0.04. Shouldn’t one major possible explanation be: this happened by chance?








 Daniel says: 

			May 2, 2014 at 5:09 pm 
(This is simplifying issues and ignoring fundamental problems with nullhypotheses testing.)
Let’s imagine two studies. Study A has a sample size of 100 and the p-value to reject H_0 is 0.04. Study B has a sample size of 1000 and the p-value to reject H_0 ist 0.04.
Question: What’s the difference in the probability to falsely reject the H_0 between the two studies?








 Johann says: 

			May 4, 2014 at 9:46 pm 
There is no difference. P(i ≤ α| H0), where “α” is any threshold (e.g. 0.05) and i is the p-value of a study, does not depend upon sample size, n. There’s nothing particularly surprising about this, though, IMO; people often think there is because they mix it up with P(i ≤ α| Ha), or power, which increases as n increases.
The second study had only a larger sample size; given the same p-value, then, this logically implies it had a much smaller effect size. The converse is true: the first study had a much larger effect size, which made up for its small sample size. The result was the same p-value.
IF there’s a real effect, the p-value of a study should asymptotically converge to zero, but if there’s no effect, the p-value will hover around zero regardless of sample size, just as the measured ES does. This is pretty intuitive to me.
But the commenter you replied to is correct with respect to *practical* considerations such as: large studies being of generally better quality, less likely to be non-representative of the true ES, etc, etc.








 Daniel says: 

			May 9, 2014 at 5:41 am 
Indeed, larger samples are better (see e.g. recently discussed in Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., and
Munafo, M. R. (2013). Power failure: why small sample size undermines the reliability of
neuroscience. Nature Reviews Neuroscience 14, 365ff. for more details, representativity may not be achieved by just more data, though, see the discussion of failures of “big data”-analysts to actually consider these statistical issues).
Anyway, I wanted to reject the idea that the *combination* of “high” p-values and small sample size is the problem. Indeed it’s quite reasonable to use less strict significance thresholds for smaller sample sizes and on the other hand a high p-value does not become somehow better or more reliable if the sample size increases, indeed, as you imply, p-values of 0.04 will be quite meaningless for samples of more than let’s say 10k respondents. Why? Because the p-value indicates only how likely the result would have been if the H_0 that there is no effect at all would be true, a statistical fiction which is usually not a reasonable possibility at all.








 Anonymous says: 

			May 9, 2014 at 11:54 am 
Actually, a p-value of .04 is meaningless for small sample sizes, but becomes increasingly meaningful as the sample size increases—but not in the direction one might think.  For generally reasonable prior distributions, as N increases, a p-value of .04 indicates increasing evidence for the null hypothesis.  
For example, using a common default prior, and assuming a “medium” effect size under H1, a 2-tailed p-value of .04 implies Bayes factors (for H1 vs H0) of 1.7, .64, ,.21, and .07) for N=20, 200, 2000, and 20000, respectively.  Only for the largest two sample sizes does the Bayes factor show much support for either hypothesis over the other, and then the favored hypothesis (since BF<1) is the null.  
You can verify the above trend by using the Bayes factor calculator here, although your specific numeric results will depend on the prior you choose.














 James A. Donald says: 

			April 29, 2014 at 2:30 am 
All of these are extremely bad solutions, since they worsen the bureaucratization of science.
Back in the eighteenth and early nineteenth century, science was high status.  Smart, wealthy, important people, would compete each to be more scientific than the other.  The result was that the scientific method itself was high status, and was, therefore, actually followed, rather than people going through bureaucratic rituals that supposedly correspond to following the scientific method.  By and large, this successfully produced truth and exposed nonsense.








 ozymandias says: 

			April 29, 2014 at 2:57 am 
I am unclear about why the high-status scientists wouldn’t preregister their trials, use heterogeneity analyses, look for high effect sizes, try to avoid experimenter effects, make their meta-analyses stronger, and the rest of it. It seems like all that advice could as easily be implemented by smart, wealthy, important people each competing to be more scientific than the other. Indeed, it seems like that’s exactly what they *would* be competing on. “What should we do?” and “how should we do it?” are importantly different questions.








 Anonymous says: 

			April 29, 2014 at 3:08 am 
For countersignalling reasons! Preregistering your trials etc. signals that you think your scientificity is in doubt, which means you aren’t really a top class scientist.  Remember when Einstein said he’d defy evidence that conflicted with his theory because he was so sure it was right? There was no way Einstein was going to be confused for a crank.








 ozymandias says: 

			April 29, 2014 at 3:29 am 
“Everyone adopts extremely rigorous standards except Einstein” seems like a great improvement on the current situation.








 Anonymous2 says: 

			April 30, 2014 at 5:46 pm 
You’ve missed what Anonymous was saying.












 ckp says: 

			April 29, 2014 at 7:48 am 
We can’t turn back the clock on the bureaucratization of science now, because science is just so BIG nowadays. The amount of science is increasing exponentially (or even superexponentially) with time, and we’ve exhausted all of the “easier” low-hanging fruit results where status might have been enough to make sure you did it right.








 Michael Edward Vassar says: 

			April 29, 2014 at 1:52 pm 
That’s one hypothesis.  I don’t consider it to be a credible hypothesis though.










 Leonard says: 

			April 29, 2014 at 10:17 am 
The problem is not that science is not high status.  Perhaps it used to be; I am not sure.  But now, it most certainly is.  Indeed, that is part of the problem.  Science has such high status that we allow its crazier emanations to override common sense.
The problem is the intersection of leaderless bureaucracy and “funding”.  Bureaucratically “funded” science gradually loses its connection to reality, and sinks bank into the intellectual morass from whence it came.








 Anthony says: 

			April 29, 2014 at 12:24 pm 
James, you’re wrong. Back in the 18th and early (to mid) 19th century, scientists were working mostly on non-living systems, where it’s a *lot* easier to be repeatable and to eliminate measurement biases. Until Darwin, almost any study of living, non-human systems was either stamp-collecting or wrong. There were plenty of people in those days who considered themselves “scientists”, studying psychology, sociology, and the like, but outside their respective schools, we mostly consider them cranks, unlike the pioneers of chemistry and physics.








 Piano says: 

			April 29, 2014 at 10:11 pm 
The problem is that Science is high status, but science is not. We’re trying to slowly make the two similar enough that science gets some high status by proxy and by virtue of Scientists accepting scientists into the fold. But, as long as Science is funded by democracies, politics will trump science. 
To an extent, that’s okay. Most people cannot afford for certain things to be destroyed by the truth, so democracy is an inadvertant and effective defence mechanism. Given the existence of people below IQ 125 (the “stupids”) who are members of different groups, we need to A) stick with democracy-controlled Science, B) obfuscate science the right amount so that it’s still allowed yet the smarter scientists can still keep their jobs, or C) Mechanize the whole thing so that the only thing that can still be accused of heresy is systems of mathematics that are necessary for the rest of the economy to function. 
Until someone shows that mathematics, and not the mathematicians, have been accused of heresy, I’m going to be partial towards C.










 Doug S. says: 

			April 29, 2014 at 3:15 am 
Bem and other parapsychologists should be required to attempt to publish their papers in physics journals. (Let’s see them massage a p-value all the way down to 0.0000003!)








 Erik says: 

			April 29, 2014 at 8:43 am 
He already massaged the p-value down to 0.00000000012 in the meta-analysis.










 Doug S. says: 

			April 29, 2014 at 3:18 am 
Incidentally, high school and undergraduate college students disprove basic physics and chemistry in their lab courses all the time…








 Zach says: 

			April 29, 2014 at 8:40 pm 
Sure, but those students are generally shown to be wrong when their experimental methods are critiqued, things that are supposed to be random are randomized, math errors are corrected, or others replicate their experiments. The issue here is that the p-values actually decrease and we become even more certain of the “wrong” results after replication and meta-analysis.










 Mai La Dreapta says: 

			April 29, 2014 at 9:39 am 
My prior belief in psi was awfully low (though maybe not as low as 10^-20), but a major effect of reading this article and the linked studies has been to greatly increase my belief in its possible existence. This is particularly the case since all of the various arguments and hypotheses about experimenter effects causing these results to appear bear the stench of motivated cognition. And noticing the amount of motivated cognition required to explain away the result makes me place the estimated probability even higher.
Thanks, Scott. /sarcasm








 Mai La Dreapta says: 

			April 29, 2014 at 10:37 am 
I should add that the other effect of reading this article was to make me much more skeptical about published science, especially in non-STEM sciences, and I think this was the intended result. But of course my default setting was to pretty much disbelieve all published psychology, sociology, and economics anyway.








 Anon256 says: 

			April 29, 2014 at 7:47 pm 
Non-Science/Technology/Engineering/Maths sciences?








 nydwracu says: 

			April 29, 2014 at 11:20 pm 
Medicine isn’t generally included in STEM. Maybe he meant medical science?
edit: Or social science, given the context.








 Mai La Dreapta says: 

			April 30, 2014 at 9:18 am 
This is what I get for not thinking about acronyms. So let me be explicit: I have high confidence in physics, chemistry, mathematics, computer science, engineering, and biology. I have low confidence in psychology, sociology, anthropology, and many forms of medicine.
(My actual degree is in linguistics, and I have a middling view of that field. Most theoretical linguistics is trash, but most linguistics is not theoretical linguistics.)








 Creutzer says: 

			April 30, 2014 at 12:52 pm 
What linguistics are you thinking about that is not theoretical linguistics, but also not stamp collecting? Or are you thinking of the stamp collecting?








 Mai La Dreapta says: 

			April 30, 2014 at 1:18 pm 
@Creutzer, I’m not familiar with your use of the term “stamp collecting” here. Is this a disparaging term for basic research, i.e. going somewhere, learning an undocumented language, and writing up a grammar for it? If so, then I admit that a lot of the non-garbage linguistics is “stamp collecting”, but I strongly reject the implicit value judgement in that term.
In any case, I suggest historical linguistics as a branch of linguistics which is neither stamp collecting nor unempirical gas-bagging. Phonetics likewise. Phonology has some very good points, but the theoretical spats over generative/OT models are pretty much useless. Syntax is a wasteland.
I adopt the maxim “Chomsky is wrong about everything” as a good rule of thumb for both linguistics and politics.








 Creutzer says: 

			April 30, 2014 at 2:55 pm 
Well, stamp collection, whether in biology or linguistics, doesn’t develop explanations. That makes me kind of go “meh”.
I agree about all your other points. I was just puzzled by the “majority” statement.
There is one thing to be said for the Chomskyans, though: Ironically, they are the better stamp collectors. Just think of all the details about familiar language like English, German, Dutch and Italian that have been discovered by Chomskyans. And those Chomskyans who do turn do the description of new languages often look at things systematically in a way others wouldn’t, and they less often use the word “topic” in a way that makes me want to strangle them.












 name says: 

			April 29, 2014 at 11:32 pm 
I once had a college professor who believed in all sorts of parapsychology. One day, in the middle of a lecture, she announced that the class was going to do a… I forget the term she used and it certainly wasn’t “mind-reading exercise”, but a mind-reading exercise is what it was.
This went as follows: Everyone paired off. One person would picture a location important to them for a few minutes, focusing solely on that, and then the other person would close their eyes, notice whatever thoughts came into their head, and try to pick up what the first person was thinking of. Then the two people would switch roles. Class discussion began after the thing was complete.
Varying degrees of success were reported. 
I noticed myself interpreting the somewhat ambiguous statements of the other person in my pair as in the right general area of it. So it could be that success was only due to suggestibility after the fact.
But I gave no ambiguous statements; I said the other guy was thinking of a beach in Hawaii. He said that’s exactly what he had been thinking of. But I found a year later that he didn’t like the professor very much: he was a Catholic and she did not like Christianity at all, and so on, and she’d given him a low grade on one class for disagreeing with her. I can’t remember the chronology here, but the class he got a low grade in was the one least unrelated to that particular subject, and I know it was taught in that same classroom. He didn’t say anything in the class discussion either. Nevertheless, it could be that he was bullshitting for fear of getting a bad grade, and thought she’d overhear him.
It could be that similar explanations apply to the whole class. It could also be that the results this professor had reported herself seeing were the result of poor planning, or bullshitting for effect, or one of a thousand other possibilities. It could also be that every other report of psi, or of any other strange effect that doesn’t quite fit into current theories of physics, had a similar explanation: myth, prescientific explanations pushed forward through cultural inertia, charlatanry, bad experimental design…
It could be.








 nydwracu says: 

			April 30, 2014 at 6:55 am 
(Dammit. Gravatar. The response that was supposed to obviously bring about was that sometimes things that can be easily explained by postulating an otherwise surprising entity really are results of pure coincidence. Now I have to figure out another method to elicit the critique of neoreaction I was trying to lead to.)








 Anonymous says: 

			April 30, 2014 at 9:47 am 
Here’s Derren Brown doing that one: https://www.youtube.com/watch?v=k_oUDev1rME








 nydwracu says: 

			April 30, 2014 at 3:52 pm 
Can’t watch videos on this thing, but it’s interesting that you’d link a pickup artist. The thing in the anecdote actually happened, but what I left out was the professor’s habit of pacing around the classroom. I’m almost certain that the force at work was just the students knowing to avoid contradicting the professor. A clever status-building exercise, but I think she actually believed all of it.














 James Babcock says: 

			April 29, 2014 at 10:23 am 
I once met someone who believed she had psychic powers. She described having done a personal experiment, with a mutual friend as experimenter/witness, and gotten a surprisingly large effect size. The experiment involved predicting whether the top of a deck of Dominion cards was gold or copper.
As it happened, I had played Dominion at this friend’s apartment before, and so I had an unusually good answer to this experiment: I had seen that particular deck of cards before and it was marked. Not deliberately of course, but the rules of Dominion lead to some cards getting used and shuffled much more than others, so if cards start getting worn, they get easy to distinguish.
That sort of observation would never, ever appear up in a study writeup.








 Vanzetti says: 

			April 29, 2014 at 11:03 am 
Hmmm…
Science gave us a giant pile of utility.
Parapsychology gave us nothing.
I feel like this argument is good enough for me to ignore even a million papers in Nature.








 Randy M says: 

			April 29, 2014 at 11:29 am 
Can you draw a dividing line between what science-like things are is and what parapsychology-like things are is that cleaves the useful from the useless without begging the question?








 Vanzetti says: 

			April 29, 2014 at 11:47 am 
No. But that’s parapsychology we are talking about. It’s pretty far away from the line I can’t draw, safely on the side of the crazy bullshit.
Now, psychology on the other hand… 🙂








 ozymandias says: 

			April 29, 2014 at 12:31 pm 
Can you explain your system of ranking things from “more sciencey” to “less sciencey”? Uncharitably I would assume the answer is how high-status it is among skeptics.














 Robin Hanson says: 

			April 29, 2014 at 11:41 am 
As a theorist at heart, I’m tempted to adopt an attitude of just not believing in effects where the empirically estimated effect size is weak, no matter what the p-values. Yeah I won’t believe that aspirin reduces heart problems, but that seems a mild price to pay. I could of course believe in a theory that predicts large observed effects in some cases, and weaker harder to see effects in other cases. But then I’d be believing in the law, not believing in the weak effect mainly because of empirical data showing the weak effect.








 Desertopa says: 

			April 29, 2014 at 8:29 pm 
It may *seem* like a mild price to pay, but in practice it leads to, what, more than a thousand avoidable deaths per year? In medicine, failure to acknowledge small effect sizes, when applied over large populations, can result in some pretty major utility losses.








 Scott Alexander says: 

			April 29, 2014 at 9:20 pm 
This was the argument I was going to make too (although I bet it’s way more than a thousand).
Also, I’m pretty sure you’d have to disbelieve in the Higgs boson and a lot of other physics.








 Sniffnoy says: 

			April 29, 2014 at 9:22 pm 
Also, I’m pretty sure you’d have to disbelieve in the Higgs boson and a lot of other physics.
No, he already addressed this; he’s talking about purely empirically detected effects with small effect size, not effects with small effect size backed up by theory.








 Scott Alexander says: 

			April 29, 2014 at 9:41 pm 
So would or would not Robin believe in the Higgs boson more once it was detected than he did before when it was merely predicted?
If the latter, does he think it was a colossal waste of money and time to (successfully) try to detect it?












 Julio Siqueira says: 

			May 4, 2014 at 8:34 pm 
“Yeah I won’t believe that aspirin reduces heart problems, but that seems a mild price to pay.”
Actually, there are theories about the way aspirin works:
“Antiplatelet agents, including aspirin, clopidogrel, dipyridamole and ticlopidine, work by inhibiting the production of thromboxane.”
http://www.strokeassociation.org/STROKEORG/LifeAfterStroke/HealthyLivingAfterStroke/ManagingMedicines/Anti-Clotting-Agents-Explained_UCM_310452_Article.jsp
The “inhibition” of headaches by aspirin is based pretty much on the very same mechanism, with some minor variations in the biochemical pathways, and differences in the body’s target-areas. But because its effect size is much much bigger than its effect as an antiplatelet, theorists at heart rarely dismissed it (especially when in need…). Aspirin was used to fight headaches for about a century before its anti-headache mechanism was discovered.
Julio Siqueira
http://www.criticandokardec.com.br/criticizingskepticism.htm










 Jonathan Graehl says: 

			April 29, 2014 at 12:12 pm 
http://news.sciencemag.org/brain-behavior/2014/04/male-scent-may-compromise-biomedical-research says that lab mice display pain less in the presence of the scent of a human male or his smelly t-shirt. The mice showed 2/3 the pain when near the human male scent (in person or through shirt).








 Eric Rall says: 

			April 29, 2014 at 4:01 pm 
As I understand it, the actual story of Galileo vs the Scholasticists involved a key role played by the emergence of gunpowder artillery as a major battlefield weapon. Specifically, artillery officers who aimed their weapons using Aristotelian mechanics (cannonball follows a straight line from the muzzle until it runs out of impetus, then falls straight down at a rate dependent on its mass) missed their targets, while those who used the theories developed by Galileo et al (curved path whose curve depends only on the angle and muzzle velocity of the cannonball, not its mass) hit their targets. And because effective use of artillery was becoming a life-and-death issue for various high-status people, those people paid serious attention to what made their artillery officers better at hitting their targets.
In that light, I’d suggest considering putting applied engineering at the top of the pyramid of science. The ultimate confirmation of a theory as substantially correct (*) has to be the ability to use the assumption of that theory’s correctness in actually making and doing things and to have those things actually work in ways that they wouldn’t if the theory were fundamentally flawed. Of course, as I write this, I’m realizing that while this works great for things like using physics theories to build airplanes and rockets, the “applied science” standard can have really shitty results in fields where the appearance of success is easier to come by accidentally. I’ll leave telling the difference between the two cases as a massive unsolved problem.
(*) “Substantially correct” has to be qualified because of issues like Newtonian Mechanics being demonstrably wrong in very subtle ways, but it still having practical usefulness because it’s correct enough to correctly predict all sorts of well-understood cases.








 suntzuanime says: 

			April 29, 2014 at 4:43 pm 
If your psychological theory is so accurate, why aren’t you a cult leader?








 Anthony says: 

			April 30, 2014 at 4:41 pm 
I commented below that science with bad epistemology is called “engineering”, but it’s worse than that. Engineering results don’t need a theory, and/or can live quite happily with two mutually-incompatible theories about what’s happening. 
I’m a soil engineer, and some of what we do is downright embarrassing when you look into its theoretical basis. And on how much we extrapolate based on seriously limited data. (Extrapolate, not just interpolate.)








 Nancy Lebovitz says: 

			May 2, 2014 at 12:35 pm 
It sounds like engineering that’s based on incoherent theory would be fertile ground for finding hypothesis to test to develop better theories. Are people working on that?








 Anthony says: 

			May 4, 2014 at 5:27 pm 
In  my field, somewhat. What we’ve got is theory with mathematically intractable application, and/or real-life circumstances with inadequate characterization of the properties so that the theory is impossible to apply. Imagine finding the actual amount of friction in every pair of surfaces in a car.
Software engineering seems to get along quite well with very little contact with the formal theory of software, and from the outside, it seems that their terrible results stem from overmuch complexity rather than poor theoretical foundations.














 somnicule says: 

			April 29, 2014 at 9:55 pm 
It seems that naive experiments on sufficiently complicated systems may as well be correlational studies. Along the lines of Hanson’s comment, without a powerful causal model behind the results of an experiment, it’s very hard to draw any meaningful or useful conclusions, particularly in cases of small effect size. Things like throwing medications at the wall and seeing what sticks, just seeing what happens in specific circumstances for psychology experiments, etc. all seems a bit cargo culty. A stop-gap measure until we get solid casual models.








 The Nabataean says: 

			April 30, 2014 at 9:14 am 
Suppose this isn’t just lousy protocol, and psi really does exist. All of these purported psychic phenomena are events that seem just a little too farfetched to be coincidence, and happen on the scale of human observers.
If this is the case (and just so you don’t get the wrong idea, I think that’s a very big ‘if’), that could be evidence in favor of the Simulation Hypothesis. Maybe the beings running the simulation occasionally bias their random number generators for the benefit (or confusion) of the simulated humans. Maybe they want to see how much we can deduce about our world when faced with seemingly inviolable laws of physics that nevertheless seem to be violated. Why? Perhaps they want to find out if they could have missed something in their own model of physics. They think they understand their world fully by now, but then again, there are always some anomalies and fishy results. They might want to run a simulation to see whether, if there really were some monkeywrenches being thrown into otherwise tidy patterns of cause and effect, intelligent beings would be able to infer their existence.
Is parapsychology the control group, or are we the experimental group?








 Mai La Dreapta says: 

			April 30, 2014 at 9:26 am 
“Psi exists” strikes me as more likely than “We are in a simulation”, and is favored by my internal implementation of Occam’s Razor.








 Slippery Jim says: 

			April 30, 2014 at 10:15 am 
Simulation Hypothesis and psi?
Enter Johnstone’s Paradox
1. The universe is finite.
2. All phenomena in the universe are subject to, and can be explained in terms of, a finite set of knowable laws which operate entirely within the universe.
1)If reality is ultimately materialistic and rational, then it could be described in a finite set of instructions and modelled as information.
2)If it could be modelled in this way, then it will be — at the very least because, given limitless time, all possible permutations of a finite universe should occur.
3)For every one original reality there will be many such sub-models, and they too will generate many sub-sub-models.
4)The nature of complex systems means that it is almost impossible for any reality to reproduce itself exactly, indeed there is greater likelihood that the submodels will be mutations of the original, subject to different structures and laws.
5)Because the models severely outnumber the original reality, or realities, it is therefore more likely that we are living in a universe modelled as information, and it is most likely that it is not identical to the original reality.
6)Thus Johnstone’s Paradox: if reality is ultimately materialistic and rational, then it is highly unlikely we are living in a materialistic, rational universe.
[This was advanced in 1987 by Lionel Snell and is roughly isomorphic to Bostrom’s argument.]








 anon says: 

			April 30, 2014 at 5:44 pm 
The conclusion seems flawed. Maybe we’re living in a universe that’s an inaccurate simulation of a different universe. But that has no bearing on whether or not the universe is materialistic or rational. We might not match the original reality, but that doesn’t imply causality doesn’t exist.










 Doug S. says: 

			May 1, 2014 at 5:13 pm 
So, in other words, “Not only does God play dice, sometimes he ignores the result and just says it worked.”








Pingback: The Ouroboros of Science | CURATIO Magazine




 Alexander Stanislaw says: 

			April 30, 2014 at 11:30 am 
Everyone seems to be assuming that bad epistemology makes for bad science. But does it? One advantage to bad epistemology (ie. normal science in which scientists have an incentive to prove their hypotheses rather than objectively test them) is that correct results are recognized more quickly. You get incorrect results too, but that is the price you pay. I anticipate that a super rigorous approach to science would slow down progress in most fields.








 Douglas Knight says: 

			April 30, 2014 at 11:38 am 
If you bias towards false positives, then maybe true positives get published quicker, but there’s a difference between “published” and “recognized.” In psychology, everyone is in their own bubble, completely ignoring everyone else’s work. Good work is never recognized.








 Anthony says: 

			April 30, 2014 at 4:19 pm 
Science with bad epistemology is called “engineering”.








 Alexander Stanislaw says: 

			April 30, 2014 at 4:49 pm 
And I’d say that engineering is a massive success from an instrumental rationality perspective.












 Chris says: 

			April 30, 2014 at 12:19 pm 
Just wanted to leave a note saying that this is outstanding — the best blog post I’ve read this year, I think — and I expect I’ll be referring people to it for a long time.  Thanks for writing it!  Have you thought about setting up a tip jar of some sort?  (Paypal, Gittip, Patreon, etc)








 Scott Alexander says: 

			May 1, 2014 at 1:09 am 
Thank you! I don’t need the money that much right now, but if you want to donate to a charity I like, you can go for http://intelligence.org/ or anything on http://www.givewell.org/charities/top-charities








Pingback: A modest proposal to fix science « Josh Haas's Web Log




 MugaSofer says: 

			April 30, 2014 at 2:30 pm 
I feel suddenly less critical of Mage: The Ascension, a game where reality was entirely subjective/shaped by expectations and the “laws of physics”, aka scientific consensus, were standardized by an ancient conspiracy in order to give humanity a stable footing for civilization.








 Johann says: 

			April 30, 2014 at 5:22 pm 
Let’s see if we can agree on one thing here, Alexander; I think you’ve written a very intellectually engaging piece, with a great deal of thought behind it—certainly one of the more interesting I have read—but I still have some basic concerns I would like to flesh out. I’ll start off with the caveat that I’m favorably disposed towards psi and parapsychology, and that I’m fairly well invested in researching the field, but I hope you’ll agree with me that we can have a productive exchange despite this most unsupportable conviction :-). If I am correct, all participants including myself will leave with an enhanced understanding, and perhaps respect, for the positions of both sides of this debate.
I’ll start by noting my most significant argument in relation to your piece: that if all these experiments, as you graciously concede, are conducted to a standard of science that is generally considered rigorous, are we not well-justified in concluding at least this: “The possibility that psi phenomena exist must now be seriously considered”? If not; what, I ask, can we say in defense of scientific practice? For if we allow it of ourselves to conduct numerous experiments of high-quality, designed by definition to eliminate (or at least strongly mitigate) explanations alternate to the one we have decided to test for, and then do not even bequeath to our conclusions—upon finding a positive result—the concession that the original explanation is a viable one, how do we justify our first impetus to scientifically investigate that explanation in the first place?
To illustrate my difference to your position, consider the following quote from your essay:
“After all, a good Bayesian should be able to say “Well, I got some impressive results, but my prior for psi is very low, so this raises my belief in psi slightly, but raises my belief that the experiments were confounded a lot.”
I’m led to question whether you really did not mean to say something slightly different. After all, if we take these words at face value, can we not—satirically—call them a decent formula for confirmation bias? A prior belief is examined with a strenuous test; that test lends evidence against the belief; we therefore conclude the test is more likely to have been flawed (i.e. evidence against our position causes us to reaffirm our belief). How do you counter this? IMO, statistical inference, whether bayesian or frequentist, only allows us to rule out the hypothesis of chance—it says nothing about the methodology behind an experiment. Thus, people only ever accept the p-value or Bayes factor of a study literally if they already believe the experiments were well-done.
Now let me address some of your specific points, to see if I cannot make the psi hypothesis a slightly more plausible one to you:
You mention Wiseman & Schlitz (1997), an oft-cited study in parapsychology circles, as strong evidence that the experimenter effect is operating here. I certainly agree. At the end of their collaboration, both had conducted four separate experiments, where three of Schlitz’s were positive and significant, and zero of Wiseman’s were. Their results can only be explained in two ways: (1) psi does not exist, and the positive results are due to experimenter bias, and (2) psi does exist, and the negative-positive schism is still related to experimenter effects. Let’s ignore issues of power, fraud, and data selectivity for now (if you find them convincing, we can discuss them in another post).
The rub, for me, is that this is an example of a paper that is designed to offer evidence against hypothesis (1)—Wiseman certainly wasn’t happy about it. The reason is that both experimenters ran protocols that were precisely the same but for their prior level of interaction with subjects (and their role as starers), ostensibly eliminating methodology as a confounding problem. Smell or other sensory cues, for example (as was mentioned in the above comments), could not have been the issue; staring periods were conducted over closed circuit television channels, and the randomization of the stare/no-stare periods was undertaken by a pseudo-random algorithm, where no feedback was given during the session that would allow subjects to detect, consciously or subconsciously, any of the impossibly subtle micro-regularities that might have occasioned in the protocol.
Now, you—understandably, from your position—criticize hypothesis (2), but consider the following remarks from Schlitz and Wiseman, after their experiment had been completed:
“In the October 2002 issue of The Paranormal Review, Caroline Watt asked each of them [Wiseman and Schlitz] what kind of preparations they make before starting an experiment. Their answers were: Schlitz: […] “I tell people that there is background research that’s been done already that suggests this works […] I give them a very positive expectation of outcome.” Wiseman: “In terms of preparing myself for the session, absolutely nothing”
The social affect of both experimenters seems to have been qualitatively different; we can say this almost with complete certainty (and it’s not unexpected). If we acknowledge, then, such confounding factors as “pygmalion effects” (Rosenthal, 1969), it would be only rational to conclude that—should psi exist—attempts to exhibit it would be influenced by them. Even more clearly, IMO (and why parapsychologists tend to see this experiment as consistent with their ideas), it was Wiseman who did the staring in the null experiments, and Schlitz who did the staring in the positive ones. Would it not make sense that a believer in psi would be more “psychic” than a skeptic, if psi exists? (or that a person with confidence in their abilities could make a better theatrical performance, or more likely deduce the solution to a complex mathematical problem, if they are not insecure about their skill level?)
Parapsychologists are only following the data, to the best of their ability. You’ll find that, under the psi hypothesis, the discrepancy in success is relatively simple to explain, whereas under the skeptical explanation we must conclude such a thing as that the most miniscule variation in experimental conditions—so miniscule that it must be postulated apart from the description of the protocol and will likely never be directly identified—can cause a study to be either significant or a failure. We must, in other words, logically determine that our science is still utterly incapable of dealing with simple experimenter bias; not just on the level of producing spurious conclusions more often than not (as Ioannidis et al show), but to the degree of failing to reliably assess literally any moderately small effect. This is itself a powerful claim.
But I will return to the nature of the psi hypothesis later. For now, I will cover parapsychological experimenter effects more broadly. Consider the following: as we probably both agree, Robert Rosenthal is one of those scientists who has done a great deal of work to bring expectancy influences to our attention; his landmark (1986) book, “Experimenter Effects in Behavioral Research”, for example, has not inconsiderably advanced our understanding of self-fulfilling prophecies in science. Would it then surprise you to learn that Rosenthal has spoken favorably on the resistance of a category of psi studies (called ganzfeld experiments) to just the sort of idea expounded by hypothesis (2)? See the following quote from Carter (2010):
“Ganzfeld research would do very well in head-to-head comparisons with mainstream research. The experimenter-derived artifacts described in my 1966 (enlarged edition 1976) book Experimenter Effects in Behavioral Research were better dealt with by ganzfeld researchers than by many researchers in more traditional domains.”
What if I told you that Rosenthal & Harris (1998) co-wrote a paper evaluating psi, in response to a government request, with overall favorable conclusions towards its existence; would you be inclined to read a little more of the literature on parapsychology? (The reference here is “Enhancing Human Performance: Background Papers, Issues of Theory and Methodology”) 
Whatever you believe about psi, I agree with you that examination of parapsychological results can do much to bolster our understanding of setbacks in experimentation; however, I also believe that thinking and examining our many attempts (and there are quite literally thousands of experiments, and dozens of categories, with their own literature) to grapple with potentially psychic effects, have the merit of helping to engender a truly reflective spirit of inquiry, for the reason that they represent precisely that ideal of science that we dream of meeting—using data and argument to resolve deeply controversial, and potentially game-changing, issues. 
On a superficial level, we already have evidence that parapsychology employs much more rigorous safeguards against experimenter effects than most any other scientific discipline. Watt & Naategal (2004), for example, conducted a survey and found that parapsychology had run 79.1% of its research using a double-blind methodology, compared to 0.5% in the physical sciences, 2.4% in the biological sciences, 36.8% in the medical sciences, and 14.5% in the psychological sciences. These findings are consistent with those of an earlier survey on experimenter effects by Sheldrake (1998), which found an even greater disparity favorable to parapsychology.
Originating out of vigorous debates between proponents and skeptics, however, I find it intuitively plausible that this should be the case (the same amount of vehement scrutiny used to contest telepathy is not used to criticize studies of the effect of alcohol on memory, for example), so these findings—while a bit surprising to me—don’t seem, on reflection, to be very much out of place.
I think, however—and you will probably agree with me—that I could ramble on about
safeguards and variables all day, without any effect on your opinion, if I do not discuss the most crucial, foundational issues pertaining to psi. It would be like trying to convince you that studies of astrology have rigorously eliminated alternate explanations; after all, if the hypothesis we would have to entertain is that the stars themselves, billions of miles away, determine our likelihood to get laid on any given day, it doesn’t matter how strong the data is—we will always suspect a flaw.
I therefore suggest we take a wide-angle view, for a moment, on the psi question. We cannot hope to be properly disposed towards its investigation if we do not—certainly it would be unacceptable to simply absorb  the popular bias against it, without critical thought, since that’s exactly the religious mindset we  eschew; neither would it be acceptable to enjoin its possibility because we want it to be true, or because it’s widespread in the media.
Let me first address the physical argument. I’m well-versed in the literature of physics and psi myself, but my friend, Max, is a theoretical physics graduate studying condensed matter physics, with a long-standing interest in parapsychology. He and I both agree that you are overestimating the degree to which psi and physics clash. Before I state why, consider that our opinion is not so unusual, for those who have thought about the question at length; Henry Margenau, David Bohm, Brian Josephson, Evan Harris Walker, Henry Stapp, Olivier Costa de Beauregard, York Dobyns, and others are examples of physicists who either believe that psi is already compatible with modern physics, or else think (more plausibly, IMO) that the current physical framework is suggestive of psi. De Beauregard actually thinks psi is demanded by modern physics, and has written so.
In light of these positions, you will see that our perspective is not an unreasonable one to maintain. Basically, we agree that if we take physical theory in its most conventional form (hoping thereby to reflect the “current consensus”), psi and physics are just barely incompatible. I say “just barely” because we have such suggestive phenomena as Bell’s EPR correlations, which Einstein himself derided as telepathy, (but which we now have incontrovertibly proved through experiment) that show how two particles may remain instantaneously connected at indefinite distances from each other, if once they interacted. It is true that this phenomenon of entanglement is exceptionally fragile; however, experimental evidence in physics and biophysics journals these days purports to show its presence in everything from the photosynthesis of algae to the magnetic compass of migrating birds, and more such claims accrue all the time. Entanglement is entering warm biological systems.
The incompatibility arises if we conceive of psi as an information signal; if we think something is “transmitted”; because the no-signaling theorem in quantum mechanics says EPR phenomena are just spooky correlations, not classical communication. You cannot use an entangled particle, as Alice, to get a message to Bob, for example, in physics parlance. However some parapsychologists and physicists don’t think of psi as a transfer, and lend to it the same spooky status as EPR—unexplained non-local influence. If this is correct, and you are willing to accept that non-local principles can scale up to large biological organisms (as the trend of the evidence is indicating), but to a larger degree than has ever been experimentally verified (outside parapsychology, of course), then certain forms of psi are already compatible with physics (e.g. telepathy).
It may also surprise you to know that the AAAS convened a panel discussion on the compatibility of physics and psi, with numerous physicists in attendance, where the general consensus was that physics cannot yet rule out even phenomena like precognition. The main reason given was that the equations of physics are time-symmetric; they work forwards and backwards equally well. There are, in fact, interpretations of quantum mechanics like TSQM that play explicitly on this principle, with optics experiments unrelated to parapsychology designed to provide evidence for retro-causality (e.g. Tollaksen, Aharanov, Zeilinger). Some of them exhibit results that are rather intuitively and elegantly explained under a retro-causal model, and have more convoluted mathematical interpretations in other frameworks (all QM experiments at this time can be explained by all the interpretations, to various degrees).
I would talk more about other ways that psi and physics can be reconciled, such as by introducing slight non-linearities in QM, but I sense that this may bury my point rather than clarify it. Where psi and physics are concerned, therefore, I say just this: that if physicists can unblinkingly confront the possibility of inflating space, multiple universes, retro-causality, observer-dependent time, universal constants, black holes, worm holes, extra dimensions, and vibrating cosmic strings—much for which there exists fleetingly little experimental evidence, a good deal of theoretical modeling, and a lot of funding—we cannot, with a straight face, dismiss a considerable body of experimental evidence for something as mundane as telepathy—or of slightly more significance: precognition (the future doesn’t have to be “fixed” to explain these experiments, BTW).
Now, if you’re looking for evidence that Bem’s experiments themselves, and their replications, were well-conducted, and well-guarded against spurious expectancy effects, thus providing parapsychological evidence for retro-causality, I can only say that I personally think they were, having read the relevant papers and thought about their methodologies. However, my area of expertise relates more to ganzfeld experiments (telepathy), which in my opinion convincingly show that critics have been unable to account for the results. I have been led to this conclusion by personal examination of the data, as well as debate, and in this capacity I have seen every methodological and statistical criticism in the book, as well as every parapsychological rejoinder to them. IMO, no one has yet been able to identify a single flaw in either the ganzfeld experiments or the treatment of those experiments that can successfully account for their results—and none of the major skeptics purport to. I am happy to debate anyone on this issue. A paper authored by myself and Max, in fact, will be coming out in the Journal of Parapsychology in June, on that very subject, if you care to read it; it tackles a number of general criticisms of psi research, with a focus on the ganzfeld, using empirical and theoretical approaches. Look for “Beyond the Coin Toss: Examining Wiseman’s Criticisms of Parapsychology”. At the parapsychology convention in San Francisco this year, as well, we may likely do a presentation on it. 
The rub on the ganzfeld, by the way, is this: where the baseline expected hit rate is 25%, the observed hit rate is 32%, across thousands of trials and more than hundred experiments; and if we partition trials into those where subjects have been specially selected in accord with characteristics predicted to significantly enhance performance, we instead find hit rates of upwards of 40% (with 27% as the average proportion of hits across unselected subjects). A main focus of our paper is the proposal that we can better the ganzfeld experiments by focusing on these special subjects.
As I wrap up, I will admit to finding this extremely fascinating. Confronted with the kind of findings I discussed, I find myself saying—as we are often impelled to do, in science, by strange data—that it didn’t have to be this way. And it didn’t. For every example of a spuriously successful scientific hypothesis, I would wager that there are a dozen that simply didn’t make it. We could have obtained a packet of null studies in parapsychology, but instead we wound up with a collection of successively improved, robust, and (many) methodologically formidable experiments (many are also poorly conducted) that collectively—in almost every paradigm examined to date—exhibit astronomically significant departures from chance. Intelligent people have performed and assailed these experiments, but no satisfactory explanation exists today for them.
Is it possible that this seeming signal in the noise is psi? I think so. Physics doesn’t rule it out; some aspects are even suggestive. People, also, have anecdotally claimed its existence for millennia, so psi is not without an observational precedent. Nor is it without an experimental precedent, as many experiments conducted from the turn of the 19th century to today have sought to evidence it, and found results consistent with the hypothesis. 
Should we be surprised at psi? Of course—the phenomenon defies our basic intuitions; I’m not claiming we shouldn’t be skeptical. But we should also be open-minded, and not hostile. Psi touches our scientific imaginations; it has accompanied our scientific journey from its inception (one of the first proposals for the scientific method, made by Francis Bacon was to study psi). It is directly in connection with investigating it, in fact, that several of our favorite procedures came into being, such as blinding and meta-analysis.
I conclude this commentary with the following point: if, after having obtained results like those I just described and alluded to, as well as those you eloquently summarized for us, all that we can bring ourselves is to say is “there must have been an error someplace”, I respectfully contend that what has failed is not our science, but our imagination. It is not time for scientists to throw out their method; it is not time for us to conclude that an evidence-based worldview cannot survive in the face of human bias; rather, it is time for scientists to become genuinely excited about the possibility of psi. We need more mainstream researchers, more funding, and more support to decide the question. Surely you will concede it is an interesting one. In pursuit of its answer, much is to be gained in understanding either Nature or our own practices in connection with investigating her mysteries.
* If any of what I have said interests you, I highly recommend reading the following exchange between my colleague, Max, and Massimo Pigliucci (a skeptic), on the topic of parapsychology—especially the comments. The debate is illuminating. 
http://rationallyspeaking.blogspot.com/2011/12/alternative-take-on-esp.html
http://rationallyspeaking.blogspot.com/2012/01/on-parapsychology.html
Thank you for an interesting read.
Best, – Johann








 anon says: 

			April 30, 2014 at 5:59 pm 
TL;DR Version
– When Scott used Bayes’ Theorem to update towards the result that the study was flawed, that was more or less confirmation bias.
– Statistics can only tell us about probability not other things (this argument made no sense to me, I might be misunderstanding it).
– Skeptics block psi effects which is why only believers discover their effects.
– People are more skeptical of psi than other phenomena, unfairly.
– Lots of smart people believe in psi and have published studies on it.
– Physics doesn’t rule out psi. Quantum entanglement provides a mechanism psi might be operating through.
– Commenter is publishing a study on psi soon with a physicist friend.








 Johann says: 

			April 30, 2014 at 7:02 pm 
Here’s to hoping the above is, more-or-less, facetious?
An aversion to long comments is understandable, but since it is more liable to occur in connection with positions we disapprove of, the danger of missing interesting challenges to our ideas is there.








 Randy M says: 

			April 30, 2014 at 7:05 pm 
well, it was a very long comment, and a not unfair summary. This could use more elaboration:
“If this is correct, and you are willing to accept that non-local principles can scale up to large biological organisms (as the trend of the evidence is indicating)”
What evidence is that referring to?








 Kibber says: 

			May 8, 2014 at 3:04 pm 
With regards to the above comment, and in my personal experience, it’s more about credibility and writing style than disagreement. Scott can write long posts that I would read because I know from experience that his posts are good, plus his writing style is usually engaging all the way through to the end. In the above comment, you lost me around “certainly one of the more interesting” – i.e. before actually revealing any positions. Not necessarily a huge loss, of course, but disagreement had nothing to do with it.








 Johann says: 

			May 9, 2014 at 1:23 am 
@ Kibber
No matter how I write, some will perceive me as either genuine, evasive, stupid, or intelligent. And the smaller their sample is of my writing, the less accurate will be their judgements.
My aim was simply to put forth a counter-perspective to Scott’s in the most genial, open manner possible; and to mitigate misunderstanding it had to be thorough. It is of course my prejudice that I have made arguments which should be considered on a level similar to his; this includes pointing out two technical mistakes on Scott’s part that I hope will eventually be fixed, and a number of observations in parapsychology that seem to me compelling of further study.
My perspective is that of someone who had directly analyzed portions of parapsychology data—specifically ganzfeld data—and has two modest co-written papers accepted for publication on the subject, claiming that the findings are interesting in specific ways that warrant replication. It is your choice whether to include this perspective in your assessment.
I am also more than happy to answer any questions you may have about my arguments. A good, well-intentioned debate is hard to pass up.
Best, – J










 he who hates deceitful error messages, like that i'm posting ""too quickly"" says: 

			April 30, 2014 at 8:02 pm 
thanks for the precis










 Scott Alexander says: 

			May 1, 2014 at 1:07 am 
I think on a Bayesian framework, the probability that psi exists after an experiment like this one would depend on your prior for psi existing and your prior for an experiment being flawed.
This experiment produced results that could only possibly happen if either psi existed or the experiment was flawed, so we should increase both probabilities. However, how *much* we increase them depends on the ratio of our priors.
Suppose that before hearing this, we thought there was a 1/10 chance of any given meta-analysis being flawed (even one as rigorous as this one), and a 1/1000 chance of psi existing.
Now we get a meta-analysis saying psi exists. For the sake of simplicity let’s ignore its p-value for now and just say it 100% proves its point.
In 1000 worlds where someone does a meta-analysis on psi, 100 will have the meta-analysis be flawed and 1 will have psi exist. 
The results of this study show we’re in either the 100 or the 1. So our probabilities should now be:
1/101 = ~0.99% chance psi exists
100/101 = ~99.1% chance the meta-analysis is flawed
I think it’s a little more complicated than this, because we know there are other parapsychological experiments whose success or failure is correlated with this one. It’s probably not true that if a similar meta-analysis came out, I’d have to update to 90/10. And the fact that there have been a lot of studies looking for psi that found none also has to figure in somewhere.
But this is the basic dynamic at work making me thing of this as “mostly casts doubt on analysis” rather than “mostly proves psi”
Regarding my low prior, you make a physics argument, and I’m not really qualified to judge. But I don’t find psi physically too implausible, in the sense that I wouldn’t be surprised if, a hundred years from now, scientists can create a machine that does a lot of the things psi is supposed to be able to do (manipulate objects remotely, receive messages from the future, etc).
My worries are mostly biological. Psi would require that we somehow evolved the ability to make use of very exotic physics, integrated it with the rest of our brain, and did so in a way that leaves no physical trace in the sense of a visible dedicated organ for doing so. The amount of clearly visible wiring and genes and brain areas necessary to produce and process visual input is obvious to everyone – from anatomists dissecting the head to molecular biologists looking at gene function to just anybody who looks at a person and sees they have eyes. And it’s not just the eyes, it’s the occipital cortex and all of the machinery involved in processing visual input into a mechanism the rest of the brain can understand. That’s taken a few hundred million years to evolve and it’s easy to trace every single step in the evolutionary chain. If there were psi, I would expect it to have similarly obvious correlates.
There’s also the question of how we could spend so much evolutionary effort exploiting weird physics to evolve a faculty that doesn’t even really work. I don’t think any parapsychologist has found that psi increases our ability to guess things over chance more than five to ten percent. And even that’s only in very very unnatural conditions like the ganzfeld. The average person doesn’t seem to derive any advantage from psi in their ordinary lives. The only possible counterexample might be if some fighting reflexes were able to respond to enemy moves tiny fractions of a second before they were made – but it would be very strange for a psi that evolved for split second decisions to also be able to predict far future (also, this would run into paradoxes). Another possibility might be that psi is broken in modern humans for some reason (increased brain size? lifestyle?). But I don’t know of any animal psi results that are well-replicated.
These difficulties magnify when you remember that psi seems to be a whole collection of abilities that would each require different exotic physics. As weird as it would be to have invisible organ-less non-adaptive-fitness-providing mechanisms for one, that multiplies when you need precognition AND telepathy AND telekinesis.








 Johann says: 

			May 1, 2014 at 3:41 pm 
Well, Scott, you bring up some good points. Thank you for the swift and gracious reply. I will address your argument about Bayesian priors first.
The way I see it, there is something strange about your approach, and that is that if you use a two prior system, the experimental evidence becomes essentially superfluous—and however logical the grounding for it may be, if this is the case we must be led to seriously question it. Consider: no matter how many meta-analyses we conduct, or how many experiments, if your priors are at 1/10 for flaws and 1/1000 for psi (a reasonable psi prior, BTW), you will always accept the flaw hypothesis. Not to put too fine a point on it, but if we were randomly selecting from infinite possible worlds where we conduct a meta-analysis, we would always have a 100 times greater chance of selecting a world with flaws than with psi, and thus also a 99.1% chance of being in the world with flaws (given a positive MA) and a .99% chance of being in the world with psi. What’s remarkable to me is that it wouldn’t matter how many positive MAs we got, sequentially; these probabilities would hold steady!
My concern is that the two-prior system is a self-fulfilling prophecy, and not how Bayesian ideas should actually play out. But that’s not to say it’s not unreasonable, in certain contexts, to think this way. We all know human beings have only a limited time to pursue what interests them; from the perspective of a person trying to make rational decisions about what to pursue and what to avoid—for whom parapsychology is just one of a mass of unreliable claims—it makes sense to default on the flaw hypothesis. However, for someone who has decided that the field is worth a more intense form of scrutiny, this analysis cannot be acceptable. I have the burden of proof in this discussion; it is my job to persuade you that parapsychology is worth the latter, and not the former, treatment.
Even a flaws prior based on empirical estimates of problematic meta-analyses is still only the crudest approximation to the “true” error prior of any MA; what it does is it uses the roughshod general quality of a scientific discipline (in this case all meta-analyses!) to predetermine the amount of evidence in one example—again, a perfectly reasonable approach for estimating the likelihood of success, generally, or of any particular meta-analysis one doesn’t want to invest time in, but rather superficial for a question one wants to have an accurate answer to. I don’t know about you, but I want to have a superbly accurate answer to whether psi exists—it’s of great importance to me, and I will be satisfied with nothing less.
To the genuinely moved investigator, I think, the first step is to undertake a considerable analysis of the experimental and statistical methodology. The flaws prior must disappear, to be replaced with the condition that if any flaw is found which the investigator deems influential, the resulting p-value of the evidence (or Bayes factor) must be left in serious question until such time as it can be shown that (1) the flaw did not exist or (2) it could not have significantly impacted the data.








 Scott Alexander says: 

			May 1, 2014 at 8:27 pm 
I don’t think it’s as pessimistic as you make it sound.
In theory, if this meta-analysis dectupled my probability to 1%, the next one that comes out positive should dectuple my probability to (almost) 10%, and so on.
In practice this doesn’t work because I assume that, if this meta-analysis is systematically flawed, the next one shares those same flaws. But you could raise my probability by showing it doesn’t. For example, if the next meta-analysis fixes the problem I mention above about lack of conclusive evidence in peer-reviewed exact replications, that should raise my probability further, and so on.
You ask below what evidence I would find conclusive. I don’t think I would find any single study conclusive. But if some protocol was invented that consistently showed a large positive effect size for psi, and it was replicated dozens of times with few or no negative replication attempts, and it was even replicated when investigated by skeptics like Wiseman, I think that would be enough to make me believe.








 Johann says: 

			May 3, 2014 at 7:19 pm 
The problem with the two-prior system is that even if the flaws prior varies between meta-analyses—which I agree is better than keeping it fixed—it still has the effect of turning the Bayes factor of any particular meta-analysis into little more than a large constant by which to multiply your predetermined priors, leaving the same relative odds for your hypotheses at the end of the analysis as at the beginning. 
Essentially, the approach seems to me to be slapping numbers onto what people already do: disregard the Bayes Factor or the p-value if they think flaws are more likely than the result of the MA.
All I’m saying is that determining that last part—the likelihood that flaws are really a more reasonable explanation—requires an exhaustive, self-driven inspection, if one genuinely wants to avoid a Type II error as well as Type I error.
I also think you may have misread the results of published exact replications because you left out
Bem (2012)
N=42
ES=0.290
Savva et al. (2004)
N=25
ES=0.34
Savva et al. Study 1 (2005)
N=50
ES=0.288
Savva et al. Study 2 (2005)
N=92
Es=-0.058
Savva’s studies can be listed as exact replications of Bem, without precognition, because they stated explicitly in their papers that they were replicating Bem (2003), the first series of habituation experiments in Bem (2011) which Bem presented at a PA convention 11 years ago. Bem (2012), also, counts as a legitimate prospective replication of Bem (2011), IMO.
If you factor in these missing studies, you now have 6/11 positively directional results, five of which display effect sizes from .27 to .34, and only two of which display negative effect sizes of comparable magnitude. Just eyeballing it, I’m willing to bet that a combination of these ES values would result in an aggregate ES of around 0.09, the reported value for exact replications of Bem in general.








 Johann says: 

			May 3, 2014 at 10:30 pm 
I’m also interested in hearing your perspective on the fact that not only was there a general overall result in Bem et al. (2014), but several observations, as well, that run directly counter to what we might predict in the absence of an effect. I’d like to see what you think about the strength of such observations as evidence against the experimenter error/manipulation hypothesis.
 For example:
(1) Consider that the much talked about “experimenter degrees of freedom” would be expected to more strongly impact conceptual rather than exact replications—hardly a controversial point—yet the ES of the conceptual experiments is lower than that of the exact. Indeed, exact replications have the highest ES of the batch.
Also, if you read Table 1 carefully, as well as the paragraph before it, you will see that all the experiments listed under “Independent Replications of Bem’s experiments” exclude Bem’s original findings, including the “exact replications”, so the point you make above in your essay that he counted those in the analysis is, I believe, mistaken.
(2) There were very noticeable differences between  fast-thinking and slow-thinking experiments, as well as no obvious reason why this should be according to the experimenter manipulation hypothesis. In particular, every single fast-thinking category yielded a p-value of less than .003, but both types of slow-thinking experiments yielded p-values above .10. Bem gives a good explanation of this in terms of the psi hypothesis, pointing out that online experiments seemed to have very strongly hampered the ES of slow-thinking protocols. It seems to me the experimenter error/manipulation hypothesis would be at a loss to account for this; why should it be that slow-thinking protocols or online administration of these protocols lower incidences of bias? 
I think the safeguards of these experiments pretty much rule out sensory cues; therefore the skeptical explanation must lie in p-hacking, multiple analysis, experimenter degrees of freedom, selective outcome reporting, etc. However (1) seems inconsistent with most of this, the check for p-hacking failed to find a result, and (2) decisively refutes the prediction that such biases evenly affected fast and slow-thinking protocols—the most straightforward prediction we would have made prior to seeing the results.
I’m not claiming any of this is conclusive, but I am saying that when you think carefully about some of the results in this MA, they take you by surprise. This is the kind of ancillary data that contributes to the veracity of an effect as much or more than the basic overall effect measure, IMO; if these types of suggestive trends weren’t present throughout parapsychology databases, I would find them less convincing.








 Johann says: 

			May 4, 2014 at 12:53 am 
BTW, Max just told me that he did the sample-size weighted mean calculation for the ES values I report plus the ones you report: the result is 0.0802. This exactly confirms what I said in my above post, and refutes the contention that published exact replications of Bem’s studies fail to replicate the results of the 31 reported exact replications, published or not, in Bem et al. (2014).
Max also bets there is heterogeneity across these ES values, given their extreme variance; an I^2 test on them might be appropriate. If moderate to great heterogeneity was found, it would count as further evidence against the null.










 Johann says: 

			May 1, 2014 at 7:15 pm 
I don’t have time to address the physics/biology argument in detail now, but consider just these few points: 
(1) psi is taken by many parapsychologists to be nothing more than an artifact of retrocausality; under the DATS model of psi, which makes testable predictions, psi is just the collateral of a conscious process that slips backwards and forwards in time (we don’t need to factor in long-term precognition right now, since the evidence for that is mostly anecdotal). See: http://www.lfr.org/lfr/csl/library/DATjp.pdf 
EDIT: Retrocausality can explain, BTW, the results of probably most, if not all, psi experiments. It is also one of the ways entanglement has been theorized to work, and  it is entailed by the TSQM model of quantum mechanics. Bell’s theorem, if I recall, establishes a non-local, anti-realist, OR retrocausal universe. Most physicists opt for non-locality, but I think the experimental evidence from the physicists I mentioned, as well as from parapsychology, should prompt us to examine the retrocausality option more carefully.
For a fascinating series of physiological psi experiments which complement Bem’s and offer evidence for presentiment, where another comprehensive meta-analysis has also been published, see: http://journal.frontiersin.org/Journal/10.3389/fpsyg.2012.00390/pdf
The effect size of the above experiments is considerably larger than Bem’s, on the order of the ganzfeld findings.
(2) It is true that we haven’t found an obvious organ associated with the “psi sense”, but it is also true that the human body has a number of senses beyond the five—precisely how many is constantly in debate—that don’t have organs as clear as eyes, for example, 
(3) The brain has been rightly termed the most complex object in the known universe; so complex it contains the mystery of conscious experience—the “Hard Problem”—which has bewildered neuroscientists and philosophers for centuries.
When it comes to spooky physics, consider that we have predicted entanglement to occur in the eyes of birds, with empirical and theoretical evidence (still in debate), as well as in the photosynthesis of algae, and also quantum tunneling in proteins across the biological spectrum; if this is the case, imagine the level of spooky physics that might take place in a human brain.








 suntzuanime says: 

			May 1, 2014 at 7:29 pm 
To be fair, it doesn’t take much to bewilder philosophers.










 Johann says: 

			May 1, 2014 at 7:18 pm 
A final question for now: what level of evidence would convince you that some form of psi exists? What sort of experiment, under what conditions?








 Troy says: 

			May 2, 2014 at 12:15 pm 
There’s also the question of how we could spend so much evolutionary effort exploiting weird physics to evolve a faculty that doesn’t even really work. I don’t think any parapsychologist has found that psi increases our ability to guess things over chance more than five to ten percent. And even that’s only in very very unnatural conditions like the ganzfeld. The average person doesn’t seem to derive any advantage from psi in their ordinary lives.
One of the most common “psychic” anecdotes that I hear is some variation of the following story: I had a sense that something was wrong with my Aunt Bea, so I picked up the phone and called, and my brother answered and said that she just had a heart attack. I think such anecdotes are especially common among twins and other close relatives.
Let’s assume that there’s actually some kind of telepathy going on here and that it’s either explained by genetic similarities or close emotional connections. Either way, it seems plausible that being able to (even highly fallibly) tell when your close genetic relative or emotional companion is in trouble could be of significant evolutionary advantage.








 gwern says: 

			May 6, 2014 at 6:52 pm 
Let’s assume that there’s actually some kind of telepathy going on here and that it’s either explained by genetic similarities or close emotional connections. Either way, it seems plausible that being able to (even highly fallibly) tell when your close genetic relative or emotional companion is in trouble could be of significant evolutionary advantage.
Nice thing about the evolutionary theory is that it suggests quite a few testable predictions: since it seems clear that psi, if it exists, varies between people, if it’s genetically based we should see the usual factor results from sibling/fraternal-twin/identical-twin studies; we should see dramatic increases in psi strength when we pair related or unrelated people in ganzfeld or staring studies (presumably identical-twin and then parent-child bonds are strongest, but we might expect subtler effects like stronger dad-son & mom-daughter communication, weaker communication among people related by adoption etc), we should be able to show decreases in communication between couples who were linked in 1 session but had broken up by the next sesssion, and so on. You can probably think of more implications. (Hm… would we expect people who grew up in dangerous places or countries, and so whose relatives/close-ones would be more likely to be at risk, to have greater receptiveness?)
On the other hand, when we try to put it in evolutionary terms as kin selection, it casts a lot of doubt on the hypothesis, since the benefit doesn’t seem to be big and so selection would be weak. I’m not a population genetics expert, but I’ll try to do some estimating…
The benefit: remember the quip – you would sacrifice yourself for 2 siblings, 4 nephews, 8 cousins… How often does one feel worried about Aunt Bea? And how often is Aunt Bea actually in trouble? (It’s no good for the psi sense if it spits out false negatives or positives, it only helps when it generate a true positive and alerts you when the relative is in danger.) Speaking for myself, I’ve never had that experience. Even the people who generate such anecdotes don’t seem to experience such events more than a few times in a lifetime. Imagine that you’re alerted, say, 3 or 4 times in a lifetime, a quarter are correct, and you have even odds of saving their lives single-handedly, and also that they’re still young enough to reproduce, and you costlessly wind up saving Aunt Bea’s life. You’re related by a quarter to Aunt Bea, I think (half comes from mother, mother will be sibling-related to Aunt Bea, so 0.5 * 0.5 = 0.25, right?), then your inclusive fitness gain here is 0.25 * (1/4) * (1/2) = +0.03125. I think these are generous figures and the true s is a lot lower, but let’s go with it.
So let’s say that if psi were a single mutation rather than a whole bunch, it has a selective advantage of 0.03125. An advantage doesn’t guarantee that the mutation will become widespread through the population, since the original bearers can just get unlucky before reproducing much. One good approximation to estimate fitness is apparently simply doubling the selective advantage (http://rsif.royalsocietypublishing.org/content/5/28/1279.long π≈2s), so then the probability of fixation is 6%.
So if such a mutation were to ever happen, it’s highly unlikely that it would then spread.








 endoself says: 

			May 6, 2014 at 10:15 pm 
If it appears once it is unlikely to spread. Many mutations that increase fitness by 3% have reached fixation, since the same mutation, or different mutations with the same effects, can occur many times given a large enough population and enough time.








 gwern says: 

			May 6, 2014 at 10:48 pm 
Yes, but the question of how often a psi single mutation would arise leads us to the question of whether that’s remotely plausible (no), hence it must be a whole assemblage of related pieces, and that leads us to the question of how psi could possibly work, much less evolve incrementally; and I’d rather not get into that just to make my point that the fitness & probability fixation can’t be very big.












 Anonymous says: 

			May 4, 2014 at 3:29 pm 
This experiment produced results that could only possibly happen if either psi existed or the experiment was flawed, so we should increase both probabilities. However, how *much* we increase them depends on the ratio of our priors.
That’s patently wrong.  It is fundamental to Bayesian inference that the amount by which our prior odds change in response to new evidence is independent of those prior odds:
Posterior odds = Bayes factor × Prior odds , 
where the Bayes factor, the ratio of the marginal likelihoods of the two hypotheses we are considering, quantifies the relative weight of the evidence for those two hypotheses. 
Suppose that before hearing this, we thought there was a 1/10 chance of any given meta-analysis being flawed (even one as rigorous as this one), and a 1/1000 chance of psi existing.
Now we get a meta-analysis saying psi exists. For the sake of simplicity let’s ignore its p-value for now and just say it 100% proves its point.
In 1000 worlds where someone does a meta-analysis on psi, 100 will have the meta-analysis be flawed and 1 will have psi exist.
The results of this study show we’re in either the 100 or the 1. So our probabilities should now be:
1/101 = ~0.99% chance psi exists 100/101 = ~99.1% chance the meta-analysis is flawed.
The new evidence increases your odds of the psi vs the non-psi hypothesis; thus it is evidence in favor of the psi hypothesis relative to the non-psi hypothesis.  It is fundamental to Bayesian inference that if enough such evidence accumulates, the probability of the psi hypothesis must approach 1 in the limit.  However, if you continue to update your odds in the manner you describe, your probability of psi can never exceed 1/2.  Thus, no amount of such evidence could ever convince you of the psi hypothesis, in contradiction of a fundamental Bayesian law.










 Jesper Östman says: 

			May 4, 2014 at 3:09 am 
Interesting points. I looked for 4 studies by Wiseman and Schlitz but could only find 3. What is the study after Wiseman and Schlitz 1997?
(She only mentions 3 as of February 2013 http://marilynschlitz.com/experimenter-effects-and-replication-in-psi-research/ , and I have failed to find other Wiseman and schlitz papers when googling a bit )








 Johann says: 

			May 4, 2014 at 2:37 pm 
Prior to their collaboration, Wiseman and Schlitz had both carried out staring studies with null results, which prompted their joint experiment. These were Wiseman & Smith (1994), Wiseman et al. (1995), and Schlitz & LaBerge (1994). I count these as semi-evidential; kind of like preliminary studies; which offered results suggestive of an experimenter effect, but which were then reproduced under more rigorous conditions.
I missed one of those studies though, so the revised count should be Shlitz: 3/4 and Wiseman: 0/5, three of which for Schlitz (2 successes/3) were part of the collaboration, and three of which for Wiseman (3 failures/3) were as well. 
Altogether, given that every success mentioned is an
independently significant experiment, the statistical fluke hypothesis is unlikely.








 Johann says: 

			May 5, 2014 at 5:18 am 
* I say “carried out studies with null results” above, but what I really meant was Wiseman got null results and Schlitz got positive results.














 Johann says: 

			April 30, 2014 at 7:22 pm 
The following popular article in Nature mentions a few examples:
http://www.nature.com/news/2011/110615/pdf/474272a.pdf
There is a decent talk on the subject by physicist Jim Al-Khalili, at the Royal Academy, unconnected with parapsychology (he’s got a great bow-tie, though):
https://www.youtube.com/watch?v=wwgQVZju1ZM
^ If the above link doesn’t show, type “Jim Al-Khalili – Quantum Life: How Physics Can Revolutionise Biology” into Youtube instead.
There are also a number of references:
Sarovar, Mohan; Ishizaki, Akihito; Fleming, Graham R.; Whaley, K. Birgitta (2010). “Quantum entanglement in photosynthetic light-harvesting complexes”. Nature Physics
Engel GS, Calhoun TR, Read EL, Ahn TK, Mancal T, Cheng YC et al. (2007). “Evidence for wavelike energy transfer through quantum coherence in photosynthetic systems.”. Nature 446 (7137): 782–6.
 “Discovery of quantum vibrations in ‘microtubules’ inside brain neurons supports controversial theory of consciousness”. ScienceDaily. Retrieved 2014-02-22.
Erik M. Gauger, Elisabeth Rieper, John J. L. Morton, Simon C. Benjamin, Vlatko Vedral: Sustained quantum coherence and entanglement in the avian compass, Physics Review Letters, vol. 106, no. 4, 040503 (2011) 
Iannis Kominis: “Quantum Zeno effect explains magnetic-sensitive radical-ion-pair reactions”, Physical Review E 80, 056115 (2009)
You can check Wikipedia, if you like, as well; it has those references and a little bit of information.








 Christian says: 

			April 30, 2014 at 7:54 pm 
This is why scientists should be humble and embrace constructivism and second-order cybernetics when they write papers.








 Troy says: 

			April 30, 2014 at 8:33 pm 
Are there any surveys of what percentage of professional psychologists (or other relevant scientists) believe in psi (or think the evidence for it is strong enough to take it seriously)? Presumably said survey would have to be anonymous to get reliable results, since believers might be embarrassed to say so publicly.








 Johann says: 

			April 30, 2014 at 9:01 pm 
That’s an excellent question, actually, and the answer is yes.
Wagener & Monnet (1979) and Evans (1973) both privately polled populations of scientists, technologists, and academics, and found that between 60-70% of them agreed with the statement that psi is either a “proven fact or a likely possibility” (response bias and other confounding variables exist, though). Consistently low results have been found for belief among psychologists; in Wagener & Monet (1979), psychologists who thought psi was either “an established fact or a likely possibility” were just 36% of the total, compared to 55% natural scientists, and 65% social scientists.
When it comes to the scientific elite, however, it is another matter. Here, evidence from McClenon (1982) seems to point to unambiguous skeptical dominance, with less than 30% of AAAS leaders holding to the likelihood of psi. Still, 30% is a lot of scientists—especially given these are the board members of the AAAS, the largest scientific academy in the world. Add to that the fact that the Parapsychological Association is an affiliate member of the AAAS—despite a vigorous campaign to remove them in 1979—and you have an interesting situation.
We must keep in mind that while both of these pieces of information are interesting, they shouldn’t do much to sway our judgement. In both surveys, it is difficult to gauge to what extent the opinions of those polled were formed in response to the empirical evidence.
You may find the Wagener & Monet (1979) results here: http://www.tricksterbook.com/truzzi/…ScholarNo5.pdf
A larger body of results is reviewed here: http://en.wikademia.org/Surveys_of_academic_opinion_regarding_parapsychology








Pingback: Links For May 2014 | Slate Star Codex


Pingback: The motto of th… | On stuff.




 Ilya Shpitser says: 

			May 1, 2014 at 6:18 am 
People like to mock Less Wrong, saying we’re amateurs getting all starry-eyed about Bayesian statistics even while real hard-headed researchers who have been experts in them for years understand both their uses and their limitations.  Well, maybe that’s true of some researchers. But the particular ones I see talking about Bayes here could do with reading the Sequences.

So what would be your recommendation to an endless list of people from LW from EY on down who say things about B/F that are (a) wrong, or (b) not even wrong.  Could they do with reading a textbook?
If I had to choose between the LW cohort and the stats (or even data analyst) cohort as to who had generally better calibrated beliefs about stats issues, I know who I would go with.








 Ellie Kesselman says: 

			May 27, 2014 at 10:49 am 
Yes, Ilya Shpitser! I am a mere statistician and data analyst, doubter of Jonah Lehrer’s veracity, ignorantly idolatrous in my continued use of Neyman, Pearson and Fisher. I love validation.
I recognize your name. You had a lively, cordial conversation with jsteinhardt on LW, following his Fervent Defense of Frequentist Statistics. I smiled with delight as I read of your commitment there.








Pingback: Utopian Science | Slate Star Codex




 Allan Crossman says: 

			May 2, 2014 at 7:27 am 
Just for the record, I didn’t invent the term “control group for science”, I think that was probably Michael Vassar.








 Ellie Kesselman says: 

			May 4, 2014 at 9:12 pm 
Alan Crossman,
True or not, the term “control group for science” is attributed to you, near and far, all over the internet. The origin seems to be consistent with your (commendably modest, honest) denial, per Douglas Knight comments on She Blinded Me With Science, 4 Aug 2009:
“I think I’ve heard the line about parapsychology as a joke in a number of places, but I heard it seriously from Vassar.”
EY replies, thread ends with yet others, e.g. “Parapsychology: The control group for science. Excellent quote. May I steal it?” and “It’s too good to ask permission for. I’ll wait to get forgiveness ;).”
On 05 December 2009, you wrote, Parapsychology the control group for science. I could find no other, better sources online, attributing it to you or Vassar.  Actually, none directly to him, only you.
Eek! I need to put my time to better use. This is embarrassing!










 Ellie Kesselman says: 

			May 4, 2014 at 2:55 am 
For the author, Mr. Steve Alexander,
Placing meta-analyses at the pinnacle of your Pyramid of Scientific Evidence is incorrect. As a practicing frequentist statistician, I am certain.  Also, this is one of the few times that I actually agree with Eliezer Yudkowsky!  He commented on your post. Substitute “frequentist” for Bayes, and vice-versa, in his comment. The conclusion is the same, in my informed opinion: meta-analyses are less, rather than more, ah, robust, compared to some of the other pyramid levels.
I mention this with good intent (it isn’t like a tiny missing word). You said,
There is broad agreement among the most intelligent voices I read (1, 2, 3, 4, 5) about a couple of promising directions we could go.
No, noooo! Number 3 is notorious science fraud, Jonah Lehrer. Lehrer acknowledged that he fabricated or plagiarized everything. He even gave a lecture about it at a prominent journalism school, maybe Columbia or Knight or NYU, last year, after being found out. You should probably re-think whether you want to cite him as one of the most intelligent voices you read.
Unlike most critiques of statistical analysis, yours does contain a core of truth!
People are terrible. If you let people debate things, they will do it forever, come up with horrible ideas, get them entrenched, play politics with them, and finally reach the point where they’re coming up with theories why people who disagree with them are probably secretly in the pay of the Devil.
I enjoyed that, very much.








 Scott Alexander says: 

			May 4, 2014 at 9:04 pm 
I think you’re misunderstanding. I am posting the standard, internationally accepted “pyramid of scientific evidence”, and then criticizing it. I didn’t invent that pyramid and I don’t endorse it.
Jonah Lehrer is indeed a plagiarist. He’s also smart and right about a lot of things. Or maybe the people whom he plagiarizes are smart and right about a lot of things. I don’t know. In either case, the source doesn’t spoil the insight, nor does that article say much different from any of the others.








 Ellie Kesselman says: 

			May 4, 2014 at 9:48 pm 
I regret being unclear. I meant that I agreed with this, and only this, in EYudkowsky’s comment earlier:
…meta-analyses will go on being [bullshXt]. They are not the highest level of the scientific pyramid…When I read about a new meta-analysis I mostly roll my eyes.
Me too!
I don’t know what this is about,
“You can’t multiply a bunch of likelihood functions and get what a real Bayesian would consider zero everywhere, and from this extract a verdict by the dark magic of frequentist statistics.”
When I make my magical midnight invocations to the dark deities of frequentist statistics, open my heart and mind to the spirits of Neyman, Pearson and Fisher, I work with maximum likelihood estimates (MLE’s), not “likelihood functions”. There are  naive Bayes models and MLE for expectation maximization algos  [PDF!], but I don’t know if EY had that in mind.
You’ll lose credibility if you continue to claim that Jonah Lehrer is among the most intelligent voices you read. That is, of course, entirely your perogative. I only wanted to be friendly, helpful.










 gwern says: 

			May 6, 2014 at 6:20 pm 
No, noooo! Number 3 is notorious science fraud, Jonah Lehrer. Lehrer acknowledged that he fabricated or plagiarized everything. He even gave a lecture about it at a prominent journalism school, maybe Columbia or Knight or NYU, last year, after being found out. You should probably re-think whether you want to cite him as one of the most intelligent voices you read.
He acknowledged plagiarizing some things (mostly things I’d regard as fairly trivial and common journalistic sins of simplifying & overstating), but if he plagiarized ‘everything’ I will be extremely impressed. I don’t recall anyone raising doubts about his ‘Decline’ article, involved people commented favorably on the factual aspects of it when it came out, the NYer still has it up, and my own reading on the topics has not lead me to the conclusion that Lehrer packed his decline article with lies, to say the least. If you want to criticize use of that article, you’ll need to do better.








 Ellie Kesselman says: 

			May 27, 2014 at 4:11 pm 
Another online acquaintance: Gwern of Disqus comments, who has found (sometimes-amusing) fault with my comments on inane The Atlantic posts.
So.  You like writing about Haskell, the Volokh Conspiracy, bitcoin and the effectiveness of terrorism. Goldman Sachs has not been extant for 300 years. I was saddened by your blithe dismissal of Cantor-Fitzgerald, post-9/11. I worked, briefly, for Yamaichi Securities, on floor 98 of Tower 2, but several years after the  1993 WTC explosion.
Please consider dropping by for a visit on any of my Wikipedia talk pages. You have 7 years’ seniority to me there. David Gerard is a decent person. He wrote your theme song, the mp3, so you must have some redemptive character traits :o) I am FeralOink, a commoner.












 Anonymous says: 

			May 4, 2014 at 7:06 pm 
@johann:
it still has the effect of turning the Bayes factor of any particular meta-analysis into little more than a large constant by which to multiply your predetermined priors, leaving the same relative odds for your hypotheses at the end of the analysis as at the beginning.
Last time i checked, multiplying a positive number by a large constant resulted in a larger number.  You need a review of Bayes factors, multiplication, or both.








 Johann says: 

			May 5, 2014 at 4:31 am 
I don’t think you understand me clearly: the main utility of Bayesian statistics is that we can update a prior to a posterior, by multiplication with a Bayes factor. On a calculational level, this is all that happens, and indeed Scott’s approach doesn’t break from this. However, when it comes to the actual inference, what should happen is more than this; ideally, we should allow our beliefs to be guided by what those numbers actually represent. Because Scott uses two priors, however, the relative odds of his two competing hypotheses (i.e. there is a true effect and there is not) remains the same before and after any particular statistical test of the evidence. Something about this is just not right, IMO.
In practice, I know that it is nonsense to believe what the numbers literally say, without skepticism. People should only take statistics at face-value, for areas that really intrigue them, after having satisfied themselves thoroughly that the experiments under analysis are not explainable on the basis of flaws. But after this has occurred, those numbers have real meaning!








 Anonymous says: 

			May 5, 2014 at 2:40 pm 
@Johann:
I don’t think you understand me clearly
Actually your new post confirms that I did understand you, and that you don’t understand how Bayesian updating works.
[W]e can update a prior to a posterior, by multiplication with a Bayes factor. On a calculational level, this is all that happens, and indeed Scott’s approach doesn’t break from this.
In fact the approach Scott proposed for updating his probabilities was dead wrong, because he made the contribution from the new evidence depend on the prior, which violates one tenet of Bayesian inference; and using his method of updating, his posterior probability for psi can never exceed 1/2, which violates another tenet of Bayesian inference.
Because Scott uses two priors, however, the relative odds of his two competing hypotheses (i.e. there is a true effect and there is not) remains the same before and after any particular statistical test of the evidence. Something about this is just not right, IMO.
Bayesian inference always considers (at least) two hypotheses.  Often, the second hypothesis is the complement of the first, but this need not be the case.   It is perfectly fine to consider the prior odds of an observed effect being due to psi (H1) vs  being due to experimental bias (H2).  The prior odds is a ratio of two probabilities, P(H1)/P(H2), and is hence a single non-negative number.  This number is multiplied by the Bayes factor, which is the ratio of the probability of the data under the psi hypothesis to the probability of the data under the bias hypothesis, and is hence also a non-negative number.  Unless this number is 1, or the prior odds 0 or infinity, then multiplying the prior odds by the Bayes factor will result in posterior odds that are different than the prior odds.  Clearly, the odds will not remain the same, as you claim.








 Johann says: 

			May 6, 2014 at 12:50 am 
It may well be that I am technically mistaken in my analysis—I have not deeply studied Bayesian hypothesis testing—but my impression is still that we’re not actually disagreeing on much, although now I have a concern or two about your approach as well. I would be glad to be corrected on any mistake, BTW.
Firstly, I will be as clear as possible about what I mean. I see Scott’s approach as one that conducts two separate hypothesis tests, both correctly performed. My contention, though, is that it is fundamentally wrong to do *both*. It is clear to me that Scott is juggling four subtle hypotheses, when really he’s only interested in two, to start with: psi exists vs. it does not, and invalidating flaws exist vs. they do not. He sets his prior for flaws at 1/10 (implying a prior of 9/10 for no invalidating flaws) and his prior for psi at 1/1000 (implying a prior of 999/1000 for no psi), multiplies both of them by the Bayes factor of a study, let’s say 300, and obtains two posterior distributions, let’s say 30 to 1 for flaws vs. no flaws and 3 to 10 for psi vs. no psi.
Now, since the existence of invalidating flaws effectively begets the same conclusion as the non-existence of psi, we can make the following comparison: At the start of the test, the ratio of Scott’s two priors was (1/10)/(1/1000) = 100/1, implying that he favored the flaws hypothesis a hundred times more than the psi hypothesis. Now, the ratio for his posteriors is (30/1)/(3/10) = 100/1, so it is clear that nothing has changed and the very performance of the test was meaningless. If you believe there are likely to be flaws in a study, why update the numbers?
If you saw something different in Scott’s methodology, feel free to explain.








 Anonymous says: 

			May 6, 2014 at 1:20 pm 
@Johann:
It is clear to me that Scott is juggling four subtle hypotheses, when really he’s only interested in two, to start with: psi exists vs. it does not, and invalidating flaws exist vs. they do not.
No. There are only two hypotheses under consideration: H1: Results of experiments purporting to show psi are actually due to psi; H2: Such results are due to bias in the experiments.
He sets his prior for flaws at 1/10 (implying a prior of 9/10 for no invalidating flaws) and his prior for psi at 1/1000 (implying a prior of 999/1000 for no psi)…
The two hypotheses quoted above that are complementary to H1 and H2 (ie, the material you have parenthesized) do not enter into the analysis.
[He] multiplies both of them by the Bayes factor of a study, let’s say 300, and obtains two posterior distributions, let’s say 30 to 1 for flaws vs. no flaws and 3 to 10 for psi vs. no psi.
No.  It works like this:  We start with the prior odds of H1 vs H2: 
Prior odds = P(H1)/P(H2) = .001/.1 = .0001 .
We multiply the prior odds by the Bayes factor for H1 vs H2. If D stands for our observed data (in this case, the results of the experiments in the meta-analysis), then
Bayes’ factor = P(D|H1)/P(D|H2) = 300.
And, by the odds form of Bayes’ theorem, we multiply the prior odds by the Bayes’ factor to obtain the posterior odds of H1 vs H2:
Posterior odds = P(H1|D)/P(H2|D) = 300 × .0001 = .03 .
So, prior to observing the data, we believed that results purporting to show psi were 10,000 times as likely to be due to bias than to psi.  After observing the data, we believe that results purporting to show psi are only 33 (=1/.03) times as likely to be due to bias than to psi.  Our new observations have increased our belief that the results are due to psi relative to our belief that they are due to bias by a factor of 300.
Hopefully that makes sense to you.














 Julio Siqueira says: 

			May 4, 2014 at 9:17 pm 
Hi Scott,
I must say that I am sort of a “believer in psi.” Also, I have read pretty much about it over the last ten years, and I have been following to a certain extent the psi-believers vs psi-skeptics debate (on the web, in papers, in books, like “Psi Wars: Getting to Grips with the Paranormal,” etc). Further, I have, on some occasions, taken sides rather fiercely on this issue. Yet, I must acknowledge that many critiques of psi works are of high value. And I did find your evaluation (article) above very interesting and worthy of respect. I would like to comment on a few points:
“None of these five techniques even touch poor experimental technique – or confounding, or whatever you want to call it. If an experiment is confounded, if it produces a strong signal even when its experimental hypothesis is true, then using a larger sample size will just make that signal even stronger.” … … “Replicating it will just reproduce the confounded results again.”
I believe that, if only confounding were involved in the issue, there actually would be a drifting in the results, and not confirmation plus confirmation plus confirmation. It would take more than poor standards to veer the results in one direction: it would take some sort of fraud, at least either conscious or unconscious.
And, the results in the Wiseman and Schlitz’s work was interesting. It is curious that it was not heavily replicated (It might be interesting to find out if it was mostly because of the believers or because of the skeptics…).
I just would like to add, as gentlemanly as possible (and thus, honour the high level of the debate on this page), that I do not share your view regarding Wiseman. But, that is not the issue, anyway.
I also think that Johann made very good contributions to the debate on this page. It is nice that he provided a fair amount of information about quantum mechanics’ based biological phenomena, which is an area of knowledge that has been increasing considerably (and robustly) over the last ten years.
Julio Siqueira
http://www.criticandokardec.com.br/criticizingskepticism.htm






Pingback: The Bem Precognition Meta-Analysis Vs. Those Wacky Skeptics | The Weiler Psi




 Adam Safron says: 

			May 5, 2014 at 8:20 pm 
For the Wiseman & Schlitz staring study (or “the stink-eye ‘effect’” as I like to call it), although I haven’t looked closely, I think I might have an explanation for how different results could be obtained with “identical” methods. It wasn’t a double-blind study. Although the person receiving the stink-eye was unaware of when they were being stared at, the person generating the stink was able to monitor the micro-expressions of the participants, and so be influenced in when they ‘chose’ to commence with stink-generation. Under this account, there is a causal relationship, but it goes in the reverse direction, and isn’t mediated by anything spooky, except for the spookiness of the exquisite pattern-detecting abilities of brains.








 Kibber says: 

			May 8, 2014 at 3:21 pm 
At least in the 1997 paper that I looked at, the experimenters used randomly generated sequences of stare and non-stare periods – i.e. the decision to stare or not was truly random and not at-will.










 hughw says: 

			May 5, 2014 at 8:24 pm 
The analogy of the meta experiment to a control using a placebo is slightly wrong. In giving a subject the placebo, you are causing him to believe it might work. He did not enter the experiment believing it would work. Whereas, the parapsychologists all enter the experiment believing parapsychology is real.








 he who posts slowly says: 

			May 5, 2014 at 8:34 pm 
Parapsychologists are not distinguished by the property of believing their hypothesis is correct.








 hughw says: 

			May 6, 2014 at 9:53 am 
It’s a premise of this essay. “…the study of psychic phenomena – which most reasonable people don’t believe exists but which a community of practicing scientists does and publishes papers on all the time…. I predict people who believe in parapsychology are more likely to conduct parapsychology experiments than skeptics”








 Julio Siqueira says: 

			May 6, 2014 at 10:05 am 
Hi Hughw,
I think you are oversimplifying the issue. And, as to the (one of the) “premise” of this essay, bear in mind that Scott said *most* reasonable people do not believe in psi. He did not say that *all* reasonable people do not believe in psi. Further, it is said above (in your quote) that the *community* believes in psi. But is is not said that *all* the parapsychologists believe in psi.
Even though we all do not believe in God, Angels, and Demons, the Devil is still in the details…
Best,
Julio Siqueira
http://www.criticandokardec.com.br/criticizingskepticism.htm








 Anonymous says: 

			May 6, 2014 at 10:50 am 
Psychologists believe in their hypothesis, just like parapsychologists believe in theirs.














 nyan sandwich says: 

			May 6, 2014 at 4:53 pm 
>Imagine the global warming debate, but you couldn’t appeal to scientific consensus or statistics because you didn’t really understand the science or the statistics, and you just had to take some people who claimed to know what was going on at their verba.
You say this immediately after spending 3 sections proving that even in our world, statistics and consensus don’t actually work, but then don’t mention it in this context even to lampshade it.
There is no way this is accidental, because I know you read Jim’s blog, and his influence on this post is quite apparent, and he makes that argument all the time.
I’ve noticed this habit you have before where you bust out some extremely interesting argument and then fail to even lampshade the obvious implication. It’s not plausible that it’s an accident, but it’s also too weird for it to be deliberate. I’m confused.








 Douglas Knight says: 

			May 6, 2014 at 6:54 pm 
I very much doubt Scott reads Jim’s blog, outside of Jim’s responses to Scott.
What influence do you see of Jim on this post?










 gwern says: 

			May 6, 2014 at 6:22 pm 
Some random comments:
using statistics like “fail-safe N” to investigate the possibility of suppressed research.
Nitpick: I think ‘fail-safe N’ should be avoided whenever possible. It assumes that publication bias does not exist, and so simply doesn’t do what one wants it to do. (See http://arxiv.org/abs/1010.2326 “A brief history of the Fail Safe Number in Applied Research”.)
This scientist – let’s give his name, Robert Rosenthal – then investigated three hundred forty five different studies for evidence of the same phenomenon. He found effect sizes of anywhere from 0.15 to 1.7, depending on the type of experiment involved. Note that this could also be phrased as “between twice as strong and twenty times as strong as Bem’s psi effect”. Mysteriously, animal learning experiments displayed the highest effect size, supporting the folk belief that animals are hypersensitive to subtle emotional cues.
I agree the Rosenthal results are interesting, but I think the Pygmalion effect is more likely to be an example of violating the commandments & statistical malpractice (Rosental also gave us the ‘fail-safe N’…) than subtle experimenter effects influencing the actual results; see Jussim & Harber 2005, “Teacher Expectations and Self-Fulfilling Prophecies: Knowns and Unknowns, Resolved and Unresolved Controversies” http://www.rci.rutgers.edu/~jussim/Teacher%20Expectations%20PSPR%202005.pdf
But first of all, I’m pretty sure no one does double-blind studies with rats.
Not really. They barely do randomized studies. That’s part of why animal studies suck so hard; see the list of studies in http://www.gwern.net/DNB%20FAQ#fn97








 Anonymous says: 

			May 6, 2014 at 7:28 pm 
 I think ‘fail-safe N’ should be avoided whenever possible. It assumes that publication bias does not exist, and so simply doesn’t do what one wants it to do.
Rosenthal’s fail-safe N should never be used, but not because it assumes that publication bias does not exist, but because it is based on the unrealistic assumption that the mean effect size in the unpublished studies is 0.  On the contrary, if the true effect size is 0, then the mean effect size in the unpublished studies would be expected to be negative.
In the Bem et al meta-analysis, the authors calculated, in addition to Rosenthal’s fail-safe N, Orwin’s fail-safe N, which in principle can provide a more realistic estimate of the number of unpublished studies because it allows the investigator to set the assumed mean unpublished effect size to a more realistic, negative, value.  But, bizarrely, Bem et al, set the value to .001, actually assuming that the unpublished studies support the psi hypothesis!








 gwern says: 

			May 6, 2014 at 8:26 pm 
Rosenthal’s fail-safe N should never be used, but not because it assumes that publication bias does not exist, but because it is based on the unrealistic assumption that the mean effect size in the unpublished studies is 0. On the contrary, if the true effect size is 0, then the mean effect size in the unpublished studies would be expected to be negative.
Yes, that’s what I mean: publication bias is a concern because it’s a bias, studies which are published are systematically different from the ones which are not, and the fail-safe N ignores this and instead is sort of like sampling-error.












 Julio Siqueira says: 

			May 6, 2014 at 7:42 pm 
Hi Scott,
Regarding your biological concerns (from someone who is highly concerned with biology…):
First, you say “exotic physics.” We, naturally, have to be careful when using the word “exotic” in this context. For example, the “physics” that almost everybody would point out as being “exotic”, namely Quantum Mechanics, is actually far more ubiquitous even than the Almighty Omnipresent Holy Lord Himself ! (if He exists). 
Then you add that “the amount of clearly visible wiring and genes and brain areas”…  …“is obvious to *everyone* ”… …“from anatomists (…) to molecular biologists (…) to JUST ANYBODY WHO LOOKS AT (…) eyes.” (emphasis added). And as a consequence, you expect similar *obvious* correlates. I think we have to remember that even almighty stuff isn’t always (surprisingly enough) obviously “perceptible.” Especially when some sort of canceling out is at play. For example, we all know that electromagnetic force is pretty much almighty (far mightier than gravity; and gravity is not exactly a light weight… – pun intended). Yet, were it not for lightening bolts, even fairly advanced human societies might have passed it by completely, without an inkling of perception of it. See, we have *obvious* physical apparatus for dealing with air (lungs; the inhaling process; exhaling; etc), with water or somewhat solid matter (mouth, teeth, stomach, etc), with visible light (eyes), with sound (ears), etc. But unlike electric fish, we do not have *obvious* biological machinery to deal with “lightening stuff.” Yet, not only is “electricity” itself immensely present in our biological machinery (i.e. even we, humans, take huge advantage of it), electromagnetism actually knits reality tight; and without it, matter would wander astray (even atoms would fall apart). What recent biology is telling us about biological uses of quantum phenomena is that it seems that we don’t have any *obvious* correlate to it in terms of biological machinery. Yet, the correlates that we do have are not only ubiquitous but essential for life. Enzymes work taking advantage of quantum tunneling. Photosynthesis, if my mind serves me well, takes advantage of quantum entanglement. And when I say “take advantage”, what I mean is: cannot do without! So, yes, it is speculative and even unlikely, but… it might just be that the correlates are there. It is just us that cannot see them yet.
And next you add: “There’s also the question of how we could spend so much evolutionary effort exploiting weird physics to evolve a faculty that doesn’t even really work.”, and you remind us that psi won’t give us a head start greater than five percent over chance (as it seems; Ganzfeld), and then you mention its (apparent) absence in our ordinary lives, and the possible paradoxes of precognition (short and/or long term). Well, what might be happening (if psi exists…) is that we are not really looking at *how* psi works and at *what for* psi works. For example, electromagnetism does not exist for making lightening… Lightening bolts are just minor manifestations of the greater phenomenon of electromagnetism. They are almost “spin offs.” The basic and “true” function of electromagnetism in Nature is to knit reality together (Especially protons and electrons “directly”, as happens in atoms. And, oddly enough, as a consequence of this knitting, electromagnetism becomes pretty much… hidden!). So maybe the function of psi (i.e. the *main* function of it in Nature) is not to get humans synched in telepathy or forewarned in precognition or “physically” unencumbered in telekinesis. These might actually be lesser deities, when instead we should rather look for the Big Guy.  🙂 
Finally you say something like: “Weird Physics + Invisible Organ-less Non-adaptive-fitness-providing Mechanisms x Precognition x Telepathy x Telekinesis.” Parapsychology does have problems… Admittedly, even according to psi researchers who believe in psi, two chief problems are, 1: possible theories for the paranormal and, 2: the role of the paranormal in Nature. We have been very clumsy in tackling these issues, IMHO. (Note that I am not a psi researcher. I am just considering these issues to be a concern of us all, no matter where we stand regarding it). So we are pretty much in the dark trying to make sense out of something that almost everybody agrees that… may not even exist! But since we (i.e. some of us) *are* trying to make sense out of it, one possibility (aside from the alternatives offered by Scott: poor studies, biased researchers, weird combinations of the two previous alternatives, etc) that comes to my mind when I look at the apparent anomaly in the Ganzfeld database (or when I look at Bem’s results now) is that this anomaly is a very “intentional” phenomenon. What I mean is that: even if you can control electronic devices with the electricity from your neurons (something that we now can routinely accomplish), it takes a lot of practice and “informative feedback” for you to learn how to master it. Yet, when it comes to shifting the odds in the desired direction in Ganzfeld sessions, we seem to be naturals in that… I think the only biological conclusion that can be drawn from it is that, if psi exists, we all use it routinely. But… maybe we do not use it for the things most people (including almost all parapsychology researchers) believe it is used for.
Anyway, just thoughts…
Best Wishes,
Julio Siqueira








 Nancy Lebovitz says: 

			May 6, 2014 at 10:59 pm 
So far as the Invisible Organ is concerned, if we don’t know how psi works, how likely would we be to recognize an organ for it?








 Julio Siqueira says: 

			May 7, 2014 at 9:37 am 
Hi Nancy,
That is really an impediment. And just to give an example that makes things far far worse: we knew pretty well how enzymes work. We knew immensely well (some might say: astronomically well) how quantum mechanics works. Yet, no one, for decades, was able to unveil the fact that enzymes make use of quantum mechanics. Now, imagine this scenario under the conditions that you reminded us of (we not knowing how psi work). The expected result might as well be: decades or even centuries of searching in the dark.










Pingback: Lightning Round – 2014/05/07 | Free Northerner




 MoodyDoc says: 

			May 8, 2014 at 5:54 pm 
The first thing that came to my mind when I read about the weird controversy of the results of Wiseman & Schlitz’s is that one scientist has psi-powers and the other not, while the subjects are all in average the same susceptible. Or it is really a kind of placebo that works telepathic. Like, when the one scientist stares at the subject while thinking “you can feel that I’m staring at you now!” this is subconsciously sensed. But if the other looks at the screen he acts more like an observer rather than an influencer. Still, observing the experiment from the outside by scientific means one would not see the difference in the “input” but only the output. However, if so, then a life brain scan of the experimenters would surely be interesting to look at. And again another dataset to analyse more or less objective…






Pingback: Nothing About Potatoes | Things I found on the internet. Cannot guarantee 100% potato-free.


Pingback: What we’re reading: Dealing with missing sequence data, SNP2GO, and the challenge of replication in bad results | The Molecular Ecologist




 Norm DeLisle says: 

			May 9, 2014 at 1:18 pm 
Very Nice! A couple of other observations. My father was a process development engineer at Dow Chemical. His design work in this area was to take a research result and test it’s commercial viability in a sizable plant process, a kind of enhanced replication. Researchers thought this was grunt work (the persistent attitude toward replication), and that all that was required was to make what they did in the laboratory bigger. But when you increase the size of a process by 5-7 orders of magnitude, a great many things become different. The lesson is that it isn’t always clear which changes in experimental conditions are important to the outcome, and the opinion of the researcher is a poor guide.
The second observation is from an article I read from the late 70-80s. It was a test of the hypothesis that niacin in large doses reduced the symptoms of schizophrenia. The design was double blind and the people who evaluated the improvement didn’t know who was receiving the niacin. However, the waiting room for the people being evaluated would hold 4-5 people at a time. Naturally they talked, and because of the niacin flush, they quickly figured out who was on placebo. It was, incidentally easy to figure out what the drug was, too. The conclusion was that one-third of the people on placebo broke the blind and went out and bought niacin. No one told the researchers because there were incentives for not telling the blind had been broken. Experimental conditions includes everything, not just what the researcher thinks is critical to successful publication.
Also, there is some evidence that placebos work even when people know they are placebos.








 Solo Atkinson says: 

			January 13, 2016 at 10:57 am 
A well-known example of this phenomenon is the development of the Haber-Bosch nitrogen fixation process. Haber established the concept on a tabletop and Bosch overcame obstacles to do it large scale. They rightly share the credit, but it’s easy to slip focus back to the “original genius” perspective. Fascinating stories.








Pingback: Das Versagen der Religionen - Seite 7




 Put Down Artist says: 

			May 22, 2014 at 5:32 am 
“That doesn’t tell you much until you take some other researchers who are studying a phenomenon you know doesn’t exist – but which they themselves believe in – and see how many of them get positive findings.”
This statement made my jaw drop. The illogic is stunning. You have made an assumption, assumed this assumption to be true, and are then deriding the people who are researching the question with an open mind.
I’m pretty sure this doesn’t need to be explained to you, but you don’t, in fact, know that these phenomenon don’t exist until you have studied them scientifically. No ifs, ands or buts. Possible bias by researchers has to be taken into account when considering their results, and when they themselves formulate their experiments – in any scientific research. 
However, it is ridiculous to assert that because you personally don’t believe in something anyone trying to determine whether there is a way to scientifically measure and validate alleged phenomenon is automatically wrong.
This is a very worrying thought pattern I see from ‘skeptics’ all the time. It is not skepticism at all, it is a kneejerk response to things that threaten their personal world view and belief system. By this definition 99% of the people on the planet are ‘skeptics’, and the only genuine skeptics are those prepared to challenge their own world views.
I’m concerned that someone could actually take that statement seriously, so, allow me to assert that to actually prove that a group of researchers science is wrong, you actually have to go into their methodology and conclusions and find errors. To take issue with their conclusions and then assume that because you don’t like them their methodology must be flawed is the the least scientific approach imaginable.
Sheesh.








 Caroline Watt says: 

			May 23, 2014 at 4:34 pm 
Love your post, and v pleased that you approve of our KPU trial registry 🙂
Just FYI, the other parapsych pre-registry that you refer to (the one Richard Wiseman and I set up for Bem replications) dates back to November 2010 and is no longer active.






Pingback: Science smorgasbord 2 | Deadline island




 Phil Goetz says: 

			July 3, 2014 at 12:14 pm 
The results? Schlitz’s trials found strong evidence of psychic powers, Wiseman’s trials found no evidence whatsoever.
Take a second to reflect on how this makes no sense.
It makes perfect sense. Schlitz has psychic powers. Wiseman doesn’t. They need to redo the experiment, keeping Schlitz as the starer in both groups.






Pingback: other mind meditation | Meditation Stuff (@meditationstuff)




 Stephanie says: 

			November 8, 2015 at 3:42 pm 
Wow, what an amazing post.  I love your blog, it’s awesome.  I just had my heart broken a little though.  This is why I’m a physicist.  Still hard as hell and not as clear as many people think, but easier than fields involving biological organisms to get some level of confidence in your results.  As long as you actually care about reality.  Some physicists are just mathematicians, ie string theorists.
Did you ever read the golem?  We read it in a philo of science course i took in the education department. (http://www.amazon.com/The-Golem-Should-Science-Classics/dp/1107604656). It focusses a bit too much on the uncertainty side, but, i think too many people have gone from faith in an invisible sky god to faith in “Science”.  I have a post on my blog with an essay i wrote for fun in grad school comparing incentives in science vs incentives in free market capitalism.  I always found faith in the invisible hand of free market capitalism to cure all human ills to be a bit too much like faith in the invisible sky god, and faith in “science” is right up there.  I have faith that, in the long run, science will probably get closer to reflecting how the universe actually works, but not in any particular current paradigm.  Skepticism is a virtue in science.  Except in climate change, which i consider more like a religion, which i also have post on.








 Kevin Keough says: 

			January 2, 2016 at 10:48 am 
Check out Rupert Sheldrake re much of this








 Solo Atkinson says: 

			January 13, 2016 at 10:45 am 
“…both authors suggest maybe their co-author hacked into the computer and altered the results.”
Actually, it was more collegial than that. Together, they suggest that one of them may have hacked the results.












Meta

Register Log in
Entries feed
Comments feed
WordPress.org


B4X is a free and open source developer tool that allows users to write apps for Android, iOS, and more.
The Effective Altruism newsletter provides monthly updates on the highest-impact ways to do good and help others.
80,000 Hours researches different problems and professions to help you figure out how to do as much good as possible. Their free career guide show you how to choose a career that's fulfilling and maximises your contribution to solving the world's most pressing problems.
Jane Street is a quantitative trading firm with a focus on technology and collaborative problem solving. We're always hiring talented programmers, traders, and researchers and have internships and fulltime positions in New York, London, and Hong Kong. No background in finance required.
The COVID-19 Forecasting Project at the University of Oxford is making advanced pandemic simulations of 150+ countries available to the public, and also offer pro-bono forecasting services to decision-makers.

Substack is a blogging site that helps writers earn money and readers discover articles they'll like.
Seattle Anxiety Specialists are a therapy practice helping people overcome anxiety and related mental health issues (eg GAD, OCD, PTSD) through evidence based interventions and self-exploration. Check out their free anti-anxiety guide here.
Giving What We Can is a charitable movement promoting giving some of your money to the developing world or other worthy causes. If you're interested in this, consider taking their Pledge as a formal and public declaration of intent.
Dr. Laura Baur is a psychiatrist with interests in literature review, reproductive psychiatry, and relational psychotherapy; see her website for more.  Note that due to conflict of interest she doesn't treat people in the NYC rationalist social scene.
Beeminder's an evidence-based willpower augmention tool that collects quantifiable data about your life, then helps you organize it into commitment mechanisms so you can keep resolutions. They've also got a blog about what they're doing here
MealSquares is a ""nutritionally complete"" food that contains a balanced diet worth of nutrients in a few tasty easily measurable units. Think Soylent, except zero preparation, made with natural ingredients, and looks/tastes a lot like an ordinary scone. 
Support Slate Star Codex on Patreon. I have a day job and SSC gets free hosting, so don't feel pressured to contribute. But extra cash helps pay for contest prizes, meetup expenses, and me spending extra time blogging instead of working.
Metaculus is a platform for generating crowd-sourced predictions about the future, especially science and technology. If you're interested in testing yourself and contributing to their project, check out their questions page

Norwegian founders with an international team on a mission to offer the equivalent of a Norwegian social safety net globally available as a membership. Currently offering travel medical insurance for nomads, and global health insurance for remote teams.
AISafety.com hosts a Skype reading group Wednesdays at 19:45 UTC, reading new and old articles on different aspects of AI Safety. We start with a presentation of a summary of the article, and then discuss in a friendly atmosphere.
Altruisto is a browser extension so that when you shop online, a portion of the money you pay goes to effective charities (no extra cost to you). Just install an extension and when you buy something, people in poverty will get medicines, bed nets, or financial aid. 




 















"
6,"



MAT337. Introduction to Real Analysis









MAT337. Introduction to Real Analysis
Fall 2018

Web page: http://www.math.toronto.edu/ilia/MAT337.2018/. 
Class Location & Time: Tue, 1:00PM - 2:00 PM; Thu, 11:00 AM - 1:00 PM; NE2190
Instructor: Ilia Binder (ilia@math.toronto.edu),  DH3026.
  
Office Hours: Tue 2:00 PM - 3:00 PM and Thu 10:00 AM-11:00 AM
Teaching Assistant:  Belal Abuelnasr, (belal.abuelnasr@mail.utoronto.ca ).
Office Hours:   Fri, 10-11 AM, DH3050.

Textbooks: Understanding Analysis, Second Edition, by Stephen Abbott.
  This book is provided as a free electronic resource to all UofT students through the library website.
  Click on the following link to access the textbook (you may be required to enter your UTORid and password): http://myaccess.library.utoronto.ca/login?url=http://books.scholarsportal.info/viewdoc.html?id=/ebooks/ebooks3/springer/2015-07-09/1/9781493927128 

Prerequisites:  MAT102H5, MAT224H5/MAT240H5, MAT212H5/MAT244H5, MAT232H5/MAT233H5/MAT257Y5
Exclusions:  MAT337H1, MAT357H1,MATB43H3, MATC37H3
  
Prerequisites will be checked, and students not meeting them will be removed from the course by the end of the second week of classes. If a student believes that s/he does have the necessary background material, and is able to prove it (e.g., has a transfer credit from a different university), then s/he should submit a 'Prerequisite/Corequisite Waiver Request Form'.

Topics. 
The course is the rigorous introduction to Real  Analysis. We start with the careful discussion of The Axiom of Completeness and proceed to the study of the basic  concepts of limits, continuity, Riemann integrability, and differentiability.  

Topics covered in class.
September 6: An introduction. Real numbers and the Axiom of Completeness. Section 1.3.
September 11: The Axiom of Completeness. Nested Interval property. Sections 1.3, 1.4.
September 13: Nested Interval property. Archimedean property. Definitions of the limit of a sequence (including an alternative definition). Limits and algebraic operations. Sections 1.4, 2.2, 2.3.
September 18: Limits and algebraic operations. Limits and order. Squeezed sequence lemma.Section 2.3.
September 20: The Monotone Convergence Theorem. Iterated sequences. Positive series. Liminf and limsup. Section 2.4.
September 25:  Liminf and limsup. Subsequences and their limits. Bolzano-Weierstrass Theorem.  Section 2.5.
September 27: Bolzano-Weierstrass Theorem.  Cauchy Criterion. Series. Sections 2.5, 2.6, 2.7.
October 2: Open and closed sets. Interrior, exterior, and border points. Section 3.2.
October 4: Interrior, exterior, and border points. Compact sets. Heine-Borel Theorem. Sections 3.2, 3.3.
October 16:  Heine-Borel Theorem. Baire's Theorem. Sections 3.3, 3.5.
October 18:  Functional limits. Sequential criterion. Continuity. Sections 4.2, 4.3.
October 23:  Continuity and compact sets. Uniform continuity. Section 4.4.
October 25:  Uniform continuity and compact sets. The Intermediate value Theorem. Differentiability (including an alternative definition). Darboux's Theorem. Sections 4.4, 4.5, 5.2.
October 30:  Rolle's theorem. The Mean Value Theorem. L'Hospital rule. Pointwise and Uniform convergence. Sections 5.3, 6.2.
November 1:   Uniform convergence. Continuity of uniform limit. Uniform convergence and differentiation. Sections 6.2, 6.3.
November 6:   Midterm review.
November 8:   Midterm.
November 13:   Uniform convergence and differentiation. Uniform convergence of series. Sections 6.3, 6.4.
November 15:   Power series. Section 6.5.
November 20:   Riemann Integration. Section 7.2.
November 22:   Riemann Integration: criterion of integrability, non-integrable functions integrability of continuous functions, additivity and algebraic properties of Riemann integral. Sections 7.2, 7.3, 7.4.
November 27:   Algebraic properties of Riemann Integral. Integrability of Uniform limit. Section 7.4.
November 29:   The Fundamental Theorem  of Calculus. Integration by parts. Riemann integrability criterion. Sections 7.5, 8.1.
December 4:   Final review. 


Homework. The assignments should be submitted through Quercus. To submit, you can scan or take a photo of your work (or write your work electronically). Please make sure that the images are clear and easy to read before you submit them.

Assignment #1, due September 13: The assignment is based on the material you have learned in MAT102.

Please do the following exercises from the textbook:  1.2.3, 1.2.4, 1.2.5, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.2.11, 1.2.12, 1.2.13.


Assignment #2, due September 20.


Assignment #3, due October 4.


Assignment #4, due October 18.


Assignment #5, due October 25.


Assignment #6, due November 1.


Assignment #7, due November 8.


Assignment #8, due November 15.


Assignment #9, due November 22.


Assignment #10, due November 29.

Tutorials and presentations. Each student must be registered in one of the tutorials (on ROSI). The attendance of tutorials is mandatory. Based on the homework assignments, the students will be selected to present some of the homework problems at the tutorials. An unexcused absence at the tutorial on the day you are selected for the presentation will result in zero credit for the presentation. 
Tutorials will begin on Friday of the second week of classes. 
Quiz. There  will be a one-hour in-tutorial quiz on Friday, September 28, or Monday, October 1, depending on your tutorial section. No aides are allowed for this quiz. The quiz will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4.
Recommended preparation (do not hand in): problems 1.3.2, 1.3.3, 1.3.6, 1.3.8, 1.4.8, 2.2.2, 2.2.4, 2.3.2, 2.3.7, 2.4.1, 2.4.6, 2.4.8.

Midterm Test. There  will be a two-hour in-class midterm test on Thursday, November 8. No aides are allowed for this test. The test will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 3.2, 3.3, 3.5, 4.2, 4.3, .4.4, 4.5, 5.2, 5.3.
  Recommended preparation: assignment #7, and (do not hand in):  all the quiz review problems, 2.5.9, 2.6.4, 2.7.7, 3.2.8, 3.3.8, 3.5.9, 4.2.4, 4.3.6, 4.4.11, 4.5.6, 5.2.10, 5.3.4. 
Final exam.   The final exam will be held on Wednesday, December 12, 5-8pm, at KN137. No aides are allowed for this test. 
The exam will cover the material of the sections 1.3, 1.4, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 3.2, 3.3, 3.5, 4.2, 4.3, .4.4, 4.5, 5.2, 5.3, 6.2, 6.3, 6.4, 6.5, 6.6 (up to Theorem 6.6.2), 7.2, 7.3, 7.4, 7.5, 8.1 (up to Theorem 8.1.2).
You will be required to state and prove in detail one of the following Theorems from the textbook: 2.4.2, 2.5.5, 3.3.4, 4.2.3, 4.3.9, 4.4.1, 4.4.2, 4.4.7, 5.2.7, 5.3.2, 6.2.6, 6.4.4, 7.2.8, 7.5.1.
Recommended preparation (do not hand in): all the quiz and midterm review problems, 6.2.3, 6.2.13, 6.2.14, 6.2.15, 6.3.1, 6.3.6, 6.4.2, 6.4.4, 6.4.10, 6.5.2, 6.5.8, 7.2.3, 7.3.2, 7.3.5, 7.4.3, 7.4.10, 7.5.2, 7.5.4.
Additional office hours: Tuesday, December 11, 12 - 1. Location: DH3000 .

 Grading. Grades  will be based on the best eight out of ten homework assignements (10%), an in-tutorial quiz (10%), an in-lecture midterm test (25%), tutorial presentations (15%), attendance of tutorials and active participation in the discussions (5%), and Final exam (35%). I will also  occasionally assign bonus problems.
Late  work. No late work will be accepted. Special consideration for late assignments or missed exams must be submitted via e-mail within a week of the original due date. There will be no make-up quiz, midterm test, or final. Justifiable absences must be declared on ROSI, undocumented absences will result in zero credit.
E-mail policy.
E-mails must originate from a utoronto.ca address and contain the course code MAT337 in the subject line. Please include your full name and student number in your e-mail.

Academic Integrity.
    Honesty and fairness are fundamental to the University of Toronto’s mission. Plagiarism is a form of academic fraud and is treated
    very seriously. The work that you submit must be your own and cannot contain anyone elses work or ideas without proper
    attribution. You are expected to read the handout How not to plagiarize (http://www.writing.utoronto.ca/advice/using-sources/how-not-to-plagiarize) and to be familiar with the Code of behaviour on academic matters, which is linked from the UTM calendar under
  the link Codes and policies.

Maintained by:   Ilia Binder (ilia@math.toronto.edu)


","





02-251: Great Ideas in Computational Biology







02-251 Great Ideas in Computational Biology
Spring 2019






Home
Schedule
Piazza
Gradescope









Date
Lecture Topic
Instructor


1/15
Introduction
Genome assembly (Part 1)
KingsfordCompeau

1/17
Genome assembly (Complete)
Compeau


1/18
Recitation on Genome Assembly
TAs


1/22
Sequence alignment (Part 1)
Alignment Demo
Kingsford

1/24
Sequence alignment (Part 2)
Kingsford


1/25
Recitation on Dynamic Programming
TAs


1/29
Read mapping (Part 1)Suffix Tree Math
Compeau

1/31
No Class (cancelled due to cold)
Polar Vortex


2/1
Recitation on Read Mapping (Part 1)
TAs


2/5
Read mapping (Part 2)
Compeau


2/7
Multiple  sequence alignment
Kingsford


2/8
Recitation on Read Mapping (Part 2)
TAs


2/12
Mini-lecture day 1

2/14
Midterm 1



2/19
Hidden Markov models
Kingsford


2/21
Metagenomics
Compeau


2/22
Recitation on HMMs
TAs


2/26
Phylogenetics (in progress)
Compeau


2/28
Phylogenetics (complete) and some necessary mathematics
Compeau


3/1
MCMC for Phylogenetics
TAs


3/5
Motif Finding, Gibbs Sampling, EM
Kingsford


3/7
RNA sequencing, gene expression
Kingsford


S P R I N G   B R E A K


3/19
Network biology: intro & function prediction
Kingsford

3/21
Network biology: evolution of modularity
Kingsford


3/26
Population Genetics
Compeau


3/28
Mini-lecture day 2



3/29
Recitation on Soft Clustering
TAs


4/2
Midterm 2



Note: All topics after Midterm 2 are tentative and subject to change.


4/4
Evolutionary Game TheoryAnimated GIF
Kingsford


4/9
Neural Modeling
Kingsford


S P R I N G   C A R N I V A L


4/16
Universality of Neural Networks and Deep Learning (incomplete)
Compeau


4/18
Protein Structure Prediction
Kingsford


4/19
Recitation on Neural Nets
TAs


4/23
Finishing up Neural Nets
Compeau

4/25
Three Mini-""Great Ideas"": Turing Patterns, Fragile Genomes, and DNA Computing
Compeau


4/26
Drop-in Project Office Hours During Recitation
TAs


4/30
Project Presentations
Students

5/2
Project Presentations
Students









"
7,"



Stellar : Message of the Day








Search: 










MIT Course Management System





Home
Course Guide
@Stellar
Updates





Message of the day
Continue








Stellar CMS
Information Services & Technology


W92 . 304 Vassar Street
Cambridge . MA . 02139



Get Help


FAQ


User Guide


Contact the Help Desk
Request a Stellar site



Resources

Supported Browsers

Certificates

Library E-Reserves

WebSIS



Updates


What's new?

Subscribe






","




15-251 Fall 2018






15-251 Great Ideas in Theoretical Computer Science

Home
Diderot
Course Info
Schedule
Calendar
Weekly Planner
Notes
Staff



 Course Information

Prerequisites

      The formal prerequisites for the course are (15-122 or 15-150) and (21-127 or 21-128 or 15-151). In particular, we expect the students to have taken an introductory computer science course that goes beyond basic computer programming and covers algorithmic thinking. On the mathematics side, we expect the students to have experience reasoning abstractly and know how to write formal proofs.
      
Learning Objectives

      Broadly speaking, the course has several goals. First, it provides a rigorous/formal introduction to the foundations of computer science, which is the science that studies computation in all its generality. An important component of this is improving your analytic and abstract thinking skills since nature's language is mathematics. Second, the course intends to prepare you to be innovators in computer science by presenting some of the great ideas that people in the past have contributed to science and humanity. We hope that you will learn from their examples. Third, the course gives you opportunities to improve your social skills by emphasizing cooperation, clarity of thought, and clarity in the expression of thought.  
      

      More specifically, some of the main learning objectives are the following.
      
 Define mathematically the notions of computation, computational problem, and algorithm.
         Express, analyze and compare the computability and computational complexity of problems.
         Use mathematical tools from set theory, combinatorics, graph theory, probability theory, and number theory in the study of computability, computational complexity, and some of the real-world applications of computational concepts.
         State and explain the important and well-known open problems in the theory of computation.
         Write clearly presented proofs that meet rigorous standards of correctness and conventional guidelines on style.
         Identify and critique proofs that are logically flawed and/or do not meet the expected standards of clarity.
         Cooperate with other people in order to solve challenging and rigorous problems related to the study of computer science.
      


      Note that even though all of the topics we discuss in the course have real-world applications, often we will not be explicitly discussing the applications. This is because initially it is better to separate concerns regarding real-world applications from the exploration of fundamental truths and knowledge that shape how we view and understand the world. The quest for truth and understanding, wherever it takes us, eventually do produce applications, some that we hoped to achieve, and some that were beyond our wildest dreams. The focus of the course is on that quest for truth and understanding, which is arguably more important than specific applications.  
      
External Resources

      There is no required textbook for the course. The material is fairly diverse, and no standard text contains it. Lecture notes will be provided. Furthermore, the lectures will be recorded and the links to the video recordings as well as the slide handouts will be provided on the course website.
      

      If you want to look at books which contain parts of the course material, we recommend the following:
      


 Introduction to the Theory of Computation by Michael Sipser,
         The Nature of Computation by Cristopher Moore and Stephan Mertens,
         Introduction to Theoretical Computer Science by Boaz Barak,
         Quantum Computing Since Democritus  by Scott Aaronson.
      

Mentoring System

      You will all be assigned a mentor TA at the beginning of the semester. Your mentor will keep track of your progress, grade your homeworks, and help you do well in the course. Don't hesitate to contact your mentor TA about anything related to the course. For instance, you can set up meetings with you mentor TA to review course material, go over homeworks or exams, or chat about studying strategies.
      

      Throughout the semester, feel free to reach out to anyone on the course staff about anything. We are here to help you anyway we can!
      
Grading

      Your grade will depend on the following factors.
      

Homeworks. There are 10 homework assignments.
      

Midterm Exams. There are 3 Midterm exams (Sep 18 from 9:00am to 10:20am, Oct 10 from 6:30pm to 9:30pm, and Nov 7 from 6:30pm to 9:30pm). Please mark your calendars. The first midterm is an early midterm and has half the weight of other exams. The purpose of this exam is to give you early feedback and give you a chance to adjust your approach to the course if needed.
      

Final Exam. There is a Final exam at the end of the semester during the finals week.
      

Class Participation. This is based on attendance in lectures and recitations, as well as completion of weekly online quizzes.
      

      Your numerical grade will be calculated according to the following table.
      



Course Component
Weight


Homework
25%


Midterm 1
10%


Midterm 2
20%


Midterm 3
20%


Final
20%


Participation
5%




      At mid-semester, letter grade cut-offs will be announced.
      

Homework System

      Homework is an extremely important component of the course and is the main tool we use to teach you valuable skills, reinforce key concepts, and help you learn the material.
      

      There are some general rules that apply to all the questions in the homework:
      
 You cannot share written material with anyone.
         You cannot discuss solutions to the problems on any discussion forum.
         You cannot solicit answers to the homework questions, i.e., you cannot ask anyone to provide you the solution to a problem, before the homework writing session.
         Searching the internet for general concepts is allowed. Googling for specific keywords that happen to appear in one of the homework questions is prohibited.
         You must always cite your sources including the people you have worked with.
         For the collaborative portions of the homework, you must think about a problem for 15 minutes before you start discussing it with someone else.
         If you work on a publicly visible whiteboard/blackboard, you must erase all contents when you are done.
      
      If you have any doubts about whether something is within the rules or not, do not hesitate to contact the course staff.
      

Types of questions: There will be 4 types of questions in the homework and each question will be clearly labeled with its type.
      

      SOLO - You must work on these questions by yourself. In addition to the rules mentioned above, you are not allowed to discuss these questions with anyone except for the course staff.
      

      GROUP - These questions must be solved in groups of 3 or 4. Working on these questions just by yourself is not allowed! You must clearly indicate your group members. You can change your group from week to week, but you can have at most one group per week. Other than your group members, you may discuss these questions with the course staff.
      

      OPEN COLLABORATION - You can discuss these questions with anyone you like from class (i.e., other students currently taking the course and the course staff). Other than the general rules stated above, there are no additional rules for this type of question.
      

      PROGRAMMING - Not all homework assignments will contain a programming question, but some might. The SOLO rules apply to these types of questions. You must submit your programs to Autolab by 6:30pm the day the homework is due.
      

      A homework assignment for a particular week will contain SOLO and/or PROGRAMMING type questions covering the current week's material, plus, GROUP and/or OPEN type questions covering the previous week's material. This has a couple of benefits. First, you'll solve problems on a topic for two weeks rather than one, which helps with retention. Second, after solving the easier solo questions, you will be much better prepared to solve the harder collaborative questions.
      

Homework writing sessions: You will not hand in written up solutions to every question of the homework. Every Wednesday from 6:30pm to 7:50pm at DH 2315, we will have a homework writing session. We will randomly pick a subset of the homework questions (usually 3 questions are picked), and you will be required to write the solutions to those problems individually during this proctored setting. We expect that you will have already practiced writing down the solution to every question in the homework prior to Wednesday night. Therefore these homework writing sessions should be relatively straightforward and stress-free.
      

Homework grading: After the homework writing session, you will get back your graded homework the following recitation. The rubrics that we used to grade each question will be posted on Diderot. You will know who graded which question. Whenever there is a point deduction on your homework, an explanation should be given, but if you do not understand why you lost points, please don't hesitate to contact us so we can clarify things for you.
      

      Grading proofs is a complicated process. We try our best to be as fair and as consistent as possible. However, mistakes will happen from time to time. Therefore we have a system in place that makes grading a two-step process. The first step is that we read your solutions and assign an initial grade based on the rubric. The second step is that you carefully review the rubric and your solution, and if you have any disagreement with the number of points you got, you email the TA who graded that question. If there was a mistake, we'll correct it. If you cannot resolve the situation with the TA who graded the question, email one of the head TAs to get a second opinion. If you are still not satisfied, email one of the instructors.
      

      Note that your grade can never go down as a result of a regrade request; it can only go up.
      

      The deadline for homework regrade request is Wednesday 6:30pm (one week after the corresponding homework writing session). Email your request to the TA who graded the question. 
      

Homework resubmission: It is very important that you learn from your mistakes and correct them. For this reason, after you get your graded homework back, you will be allowed to resubmit solutions that you have gotten wrong. You may (and should) go to the homework solution sessions (see below for details) or ask about the solution during office hours. If you turn in a completely correct and well-written solution, you will receive back 50% of the lost credit for that question. If on the other hand your solution is not near-perfect, then unfortunatly you will not receive any points back.
      

      The deadline for homework solution resubmission is Sunday 6:30pm (11 days after the corresponding homework writing session). Email your resubmission, along with your original solution, to the TA who graded that question.
      

Proof-writing guidelines: The quality of your write-up and presentation matters a lot, so you should make sure your solutions are very clearly explained. If you are not sure of something, or you think there is a gap in your argument, clearly indicate these in your write-up (you will earn more points doing so rather than writing a wrong argument!!). Do not try to sell a wrong or incomplete proof! If you leave a question completely blank, you will earn 20% of the credit for that question.
      

      To help you write correct and clear proofs, we have prepared a document with a list of guideline points. The guideline points will appear as a checklist in each homework. For each proof you write, you will have to tick the checklist items to acknowledge that you are following the guidelines.  
      

Click here to access the document.


Homework solution sessions: Unfortunately, we will not be publishing written solutions to the homework problems. The main reason is that any homework solution we post kills the question for future semesters of the course (and any other course that might be using a similar question). Most questions we ask are pedagogically very valuable, and coming up with such questions is very hard. So we don't want to kill those questions by publishing solutions. That being said, we don't keep the solutions a secret either. We hold homework solution sessions twice a week and go over the solutions (on the blackboard) to the problems that appeared in the writing session. We are also always happy to go through the solutions to any problem with you during office hours. 
      

      Note that during the solutions sessions, we will not write the full proof on the blackboard. We expect you to fill in the details yourself.
      

      The times and locations of the homework solution sessions will be announced at the beginning of the semester.
      
Recitation System

      The recitation sections that you have signed up for on SIO will only be used for the first week of the course. Starting week 2, we will transition to a different system.
      

      One of the main advantages of recitations over lectures is that the sections are much smaller in size. In order to improve the student-TA ratio and give you more flexibility, we will be asking you the times you are available on Fridays and Saturdays. Based on that information, we will assign you to a recitation slot. And a typical recitation section will have about 12 students. 
      

      In addition to the above change, we will offer 3 different spiciness levels for recitations. You will select yourself which level is appropriate for you. During the semester if you feel like you would like to switch to another level, let us know, and we'll arrange the switch.
      






Bell pepper
          Not spicy. We will go over the definitions to make sure everyone understands them fully. Then we will solve the problems together (as many as the time allows).
        






Jalapeño pepper
          Normal spicy. After a quick review of definitions, we'll solve the problems together. These sections will have a faster pace.
        






Habanero pepper
          Hot! We'll assume you are comfortable with everything covered in lecture and notes, so we'll directly dive into the problems. These sections will have the fastest pace.
        



      We will take attendance in recitation. 
      
Diderot

      We will use Diderot for several purposes, as listed below. Every student is required to signup for the course's Diderot page!
      

 Making announcements. Important announcements related to the course will be made on Diderot. You must check Diderot and/or your email on a daily basis to receive the announcements in a timely manner.
         Asking and answering questions. If you have a question about the course that can be easily answered electronically, please use Diderot.
         Finding group members. You can use Diderot's ""Social"" posts to search and find group members to collaborate for the homework.
         Conducting in-class polls. We will be asking poll questions during lectures related to the topics being discussed. We will give a link to the Google form containing the poll question through a Diderot post. We expect everyone to answer all the poll questions. We will not keep track of whether your answers are correct, however, we will use the polls to keep track of attendance. If a lecture contains multiple polls, a random one will be chosen to take attendance. If you experience a technical difficulty that prevents you from participating in a poll, then see the instructor right after lecture so your presence can be noted.
         Publishing parts of course content. We plan to publish course notes, homeworks and recitations on Diderot.
      
Asking Questions

      Even though we are always ready to help and provide support anyway we can, there is a fine balance that we have to respect. Ultimately, we would like you to develop the necessary skills to be self-sufficient problem solvers. You will have many questions throughout the semester. Reflecting on your questions to try to figure out the answers on your own is extremely valuable, and we want to make sure that you are not robbed of this experience. Here are some general guidelines for asking questions.
      


 The general rule of thumb is the following. Before you ask a question to us, ask yourself whether you can figure out the answer yourself. If the answer is ""yes"" or ""maybe"", then you should give a solid effort in trying to find the answer. This is an extremely valuable learning experience.
         Whenever you ask a question, first tell us what your own thoughts about the question are and what you have tried. If you don't, then we will usually respond to your question with another question asking you what your thoughts are. When you explain your thoughts to us, this allows us to see and fix any misunderstanding and help you more effectively.
         If a homework problem is ambiguous to you, try to figure out all the possible interpretations and evaluate them one by one. Often, you'll find that there is really one interpretation that makes sense.
         Try not to turn a conversation with a course staff member into the Twenty Questions game. This does not maximize your learning outcomes. Remember that when a question formulates in your mind, the first person who should try to answer it is you. Our role is to help you when you are stuck.
         Certain discussions are best suited for your group. For example, if you want to bounce off ideas and get some feedback on your thought process for a GROUP or OPEN problem on the homework, you should have that conversation with your group members.
         Please do not ask us to read your solution write-up and give you feedback on how many points you would get. Solutions can have subtle bugs, and we cannot always spot such bugs after a quick glance. Properly reading and evaluating a solution can take a lot of time. That being said, even though we cannot read your solution in detail before the homework writing session, we are happy to listen to your overall proof strategy and help you try to figure out if there are any logical flaws or gaps. 
         Diderot is a good resource for short-answer questions, but can be extremely inefficient for long-answer questions or questions that may require a back and forth conversation. When you want to ask a question on Diderot, consider whether the question is suitable for that platform, and if it is not, ask your question during office hours for a more useful and efficient conversation. 
      

Use of Electronic Devices

      The use of electronic devices like phones, tablets, and laptops during lectures and recitations is prohibited. These devices cause distractions both to you and the people around you. If you would like to use an electronic device to take notes and using paper and pencil is not a good option for you, please contact one of the instructors.
      

      There is an exception to the above rule. When we open up a poll during a lecture, you are allowed to use your phone to cast your vote. Once the poll is completed, you should put away your phone. If you do not have a smart phone, please contact one of the instructors.
      
How to Succeed in 251

Download the pdf.
      
Academic Integrity

      We understand that most of you would never consider cheating in any form. There is, however, a small minority of students for whom this is not the case. In the past, when we have caught students cheating they have often insisted that they did not understand the rules and penalties. As a part of the first homework, you will be required to acknowledge that you have read and understood the cheating policies. Please read Carnegie Mellon University Policy on Academic Integrity. The following are some clear examples of cheating:
      

 Copying from another student during an exam or homework writing session.
       Discussing a SOLO problem before the homework writing session with someone who is not a part of the course staff.
       Googling for specific keywords that happen to appear in one of the homework questions.
       Showing a draft of a written solution to another student.
       Getting help from someone who you do not acknowledge on your solution.
       Receiving exam related information from a student who has already taken the exam.
       Attempting to hack any part of the 15-251 infrastructure.
       Looking at someone else’s work on AFS, even if the file permissions allow it.
       Lying to the course staff.
    

Consequences: The penalty for cheating can range from a 10% deduction on your overall course average (i.e. a letter grade drop) to directly failing the course. Furthermore, in most cases, a letter to the Dean of Student Affairs is sent and further consequences are determined by them.
    
Extended-Time and Make-Up Policy

    We are happy to provide appropriate accommodations to students who have approval from the Disability Resources Center. Please contact one of the instructors if you are in this situation.
    

    No make-up quizzes, exams, or homework writing sessions will be administered, except in the case of documented medical or family emergencies, or other university approved absences. The common cold or your computer crashing, unfortunately, do not qualify as an excused absence.
    
Well-Being and Happiness

    We very much care about your well-being and happiness! Be aware that everyone on the course staff is always available to provide counsel or chat, and you should attend office hours as often as you want for academic and non-academic conversation.
    

    However, also know that the university provides services that you may want to take advantage of at some point during the semester. If you are ever unsure about them, run into a problem, or want more information, feel free to reach out to the instructors.
    

    For a comprehensive list of CMU’s resources, please click here.
    

CMU Police Department


    Do not hesitate to call CMU police when in an emergency or if you are interested in taking advantage of their services.
    

 Website: http://www.cmu.edu/police/welcome.html
 Emergency phone number: 412-268-2323
       Non-Emergency phone number: 412-268-6232
    

Counseling and Psychological Services (CAPS)


    CAPS offer therapy, crisis support, etc. and you should reach out to CAPS for counseling if you are struggling, no matter how small you may think your problems are. If CAPS can’t help you appropriately, they also do referrals and basic consultations to help you find what you need.
    

Website: http://www.cmu.edu/counseling/
Hours: Monday through Friday 8:30am-5:00pm
      Phone number: 412-268-2922
      Location: 2nd floor, Morewood Gardens, E-Tower
    

University Health Services (UHS)


    Health services can help you in the same way a doctor does but they also offer comprehensive care management and health promotion services.
    

 Website: http://www.cmu.edu/health-services/
 Hours: M, Tu, W: 8:30am-7:00pm, Th: 10:00am-7:00pm, F: 8:30am-5:00pm, Sat: 11:00am-3:00pm
       Note: When UHS is closed, call 1(844)881-7176.
       To set up an appointment on HealthConnect, click here.
       Comprehensive Care Manager: Diane Dawson, 412-268-9171
    

15-251 Wellness Help


    If you find yourself struggling in any way or simply would like to discuss how you are feeling about 251 or just chat, reach out to one of the following people or your mentor TA to set up a casual meeting.
    

 Anil Ada (Instructor): aada@cs.cmu.edu
 Bernhard Haeupler (Instructor): haeupler@cs.cmu.edu
 Corwin de Boor (Head TA): cdeboor@andrew.cmu.edu
 Patrick Lin (Head TA): patrick1@andrew.cmu.edu








"
8,"

The ground of optimization - LessWrong 2.0 viewerArchiveSequencesAboutSearchLog InQuestionsEventsShortformAlignment ForumAF CommentsHomeFeaturedAllTagsRecent CommentsThe ground of optimizationAlex Flint20 Jun 2020 0:38 UTCLW: 241 AF: 9275 commentsLW link1 reviewOptimizationAIWorld ModelingGeneral IntelligenceSelection vs ControlDynamical systemsBest of LessWrongPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsContentsIntroductionEx­am­ple: com­put­ing the square root of twoEx­am­ple: build­ing a houseDefin­ing optimizationRe­la­tion­ship to Yud­kowsky’s defi­ni­tion of optimizationRe­la­tion­ship to Drexler’s Com­pre­hen­sive AI ServicesRe­la­tion­ship to Garrabrant and Dem­ski’s Embed­ded AgencyEx­am­ple: ball in a valleyEx­am­ple: ball in valley with robotEx­am­ple: com­puter perform­ing gra­di­ent descentEx­am­ple: billiard ballsEx­am­ple: satel­lite in orbitEx­am­ple: a treeEx­am­ple: bot­tle capEx­am­ple: the hu­man liverEx­am­ple: the uni­verse as a wholePower sources and entropyCon­nec­tion to dy­nam­i­cal sys­tems theoryConclusionThis work was sup­ported by OAK, a monas­tic com­mu­nity in the Berkeley hills. This doc­u­ment could not have been writ­ten with­out the daily love of liv­ing in this beau­tiful com­mu­nity. The work in­volved in writ­ing this can­not be sep­a­rated from the sit­ting, chant­ing, cook­ing, clean­ing, cry­ing, cor­rect­ing, fundrais­ing, listen­ing, laugh­ing, and teach­ing of the whole com­mu­nity.

What is op­ti­miza­tion? What is the re­la­tion­ship be­tween a com­pu­ta­tional op­ti­miza­tion pro­cess — say, a com­puter pro­gram solv­ing an op­ti­miza­tion prob­lem — and a phys­i­cal op­ti­miza­tion pro­cess — say, a team of hu­mans build­ing a house?
We pro­pose the con­cept of an op­ti­miz­ing sys­tem as a phys­i­cally closed sys­tem con­tain­ing both that which is be­ing op­ti­mized and that which is do­ing the op­ti­miz­ing, and defined by a ten­dency to evolve from a broad basin of at­trac­tion to­wards a small set of tar­get con­figu­ra­tions de­spite per­tur­ba­tions to the sys­tem. We com­pare our defi­ni­tion to that pro­posed by Yud­kowsky, and place our work in the con­text of work by Dem­ski and Garrabrant’s Embed­ded Agency, and Drexler’s Com­pre­hen­sive AI Ser­vices. We show that our defi­ni­tion re­solves difficult cases pro­posed by Daniel Filan. We work through nu­mer­ous ex­am­ples of biolog­i­cal, com­pu­ta­tional, and sim­ple phys­i­cal sys­tems show­ing how our defi­ni­tion re­lates to each.
Introduction
In the field of com­puter sci­ence, an op­ti­miza­tion al­gorithm is a com­puter pro­gram that out­puts the solu­tion, or an ap­prox­i­ma­tion thereof, to an op­ti­miza­tion prob­lem. An op­ti­miza­tion prob­lem con­sists of an ob­jec­tive func­tion to be max­i­mized or min­i­mized, and a fea­si­ble re­gion within which to search for a solu­tion. For ex­am­ple we might take the ob­jec­tive func­tion (x2−2)2 as a min­i­miza­tion prob­lem and the whole real num­ber line as the fea­si­ble re­gion. The solu­tion then would be x=√2 and a work­ing op­ti­miza­tion al­gorithm for this prob­lem is one that out­puts a close ap­prox­i­ma­tion to this value.
In the field of op­er­a­tions re­search and en­g­ineer­ing more broadly, op­ti­miza­tion in­volves im­prov­ing some pro­cess or phys­i­cal ar­ti­fact so that it is fit for a cer­tain pur­pose or fulfills some set of re­quire­ments. For ex­am­ple, we might choose to mea­sure a nail fac­tory by the rate at which it out­puts nails, rel­a­tive to the cost of pro­duc­tion in­puts. We can view this as a kind of ob­jec­tive func­tion, with the fac­tory as the ob­ject of op­ti­miza­tion just as the vari­able x was the ob­ject of op­ti­miza­tion in the pre­vi­ous ex­am­ple.
There is clearly a con­nec­tion be­tween op­ti­miz­ing the fac­tory and op­ti­miz­ing for x, but what ex­actly is this con­nec­tion? What is it that iden­ti­fies an al­gorithm as an op­ti­miza­tion al­gorithm? What is it that iden­ti­fies a pro­cess as an op­ti­miza­tion pro­cess?
The an­swer pro­posed in this es­say is: an op­ti­miz­ing sys­tem is a phys­i­cal pro­cess in which the con­figu­ra­tion of some part of the uni­verse moves pre­dictably to­wards a small set of tar­get con­figu­ra­tions from any point in a broad basin of op­ti­miza­tion, de­spite per­tur­ba­tions dur­ing the op­ti­miza­tion pro­cess.
We do not imag­ine that there is some en­g­ine or agent or mind perform­ing op­ti­miza­tion, sep­a­rately from that which is be­ing op­ti­mized. We con­sider the whole sys­tem jointly — en­g­ine and ob­ject of op­ti­miza­tion — and ask whether it ex­hibits a ten­dency to evolve to­wards a pre­dictable tar­get con­figu­ra­tion. If so, then we call it an op­ti­miz­ing sys­tem. If the basin of at­trac­tion is deep and wide then we say that this is a ro­bust op­ti­miz­ing sys­tem.
An op­ti­miz­ing sys­tem as defined in this es­say is known in dy­nam­i­cal sys­tems the­ory as a dy­nam­i­cal sys­tem with one or more at­trac­tors. In this es­say we show how this frame­work can help to un­der­stand op­ti­miza­tion as man­i­fested in phys­i­cally closed sys­tems con­tain­ing both en­g­ine and ob­ject of op­ti­miza­tion.
In this way we find that op­ti­miz­ing sys­tems are not some­thing that are de­signed but are dis­cov­ered. The con­figu­ra­tion space of the world con­tains countless pock­ets shaped like small and large bas­ins, such that if the world should crest the rim of one of these pock­ets then it will nat­u­rally evolve to­wards the bot­tom of the basin. We care about them be­cause we can use our own agency to tip the world into such a basin and then let go, know­ing that from here on things will evolve to­wards the tar­get re­gion.
All op­ti­miza­tion bas­ins have a finite ex­tent. A ball may roll to the cen­ter of a valley if ini­tially placed any­where within the valley, but if it is placed out­side the valley then it will roll some­where else en­tirely, or per­haps will not roll at all. Similarly, even a very ro­bust op­ti­miz­ing sys­tem has an outer rim to its basin of at­trac­tion, such that if the con­figu­ra­tion of the sys­tem is per­turbed be­yond that rim then the sys­tem no longer evolves to­wards the tar­get that it once did. When an op­ti­miz­ing sys­tem de­vi­ates be­yond its own rim, we say that it dies. An ex­is­ten­tial catas­tro­phe is when the op­ti­miz­ing sys­tem of life on Earth moves be­yond its own outer rim.
Ex­am­ple: com­put­ing the square root of two
Say I ask my com­puter to com­pute the square root of two, for ex­am­ple by open­ing a python in­ter­preter and typ­ing:
>>> print(math.sqrt(2))
1.41421356237

The value printed here is ac­tu­ally calcu­lated by solv­ing an op­ti­miza­tion prob­lem. It works roughly as fol­lows. First we set up an ob­jec­tive func­tion that has as its min­i­mum value the square root of two. One func­tion we could use is y=(x2−2)2

Next we pick an ini­tial es­ti­mate for the square root of two, which can be any num­ber what­so­ever. Let’s take 1.0 as our ini­tial guess. Then we take a gra­di­ent step in the di­rec­tion in­di­cated by com­put­ing the slope of the ob­jec­tive func­tion at our ini­tial es­ti­mate:

Then we re­peat this pro­cess of com­put­ing the slope and up­dat­ing our es­ti­mate over and over, and our op­ti­miza­tion al­gorithm quickly con­verges to the square root of two:

This is gra­di­ent de­scent, and it can be im­ple­mented in a few lines of python code:
	current_estimate = 1.0
	step_size = 1e-3
	while True:
		objective = (current_estimate**2 - 2) ** 2
		gradient = 4 * current_estimate * (current_estimate**2 - 2)
		if abs(gradient) < 1e-8:
			break
		current_estimate -= gradient * step_size

But this pro­gram has the fol­low­ing un­usual prop­erty: we can mod­ify the vari­able that holds the cur­rent es­ti­mate of the square root of two at any point while the pro­gram is run­ning, and the al­gorithm will still con­verge to the square root of two. That is, while the code above is run­ning, if I drop in with a de­bug­ger and over­write the cur­rent es­ti­mate while the loop is still ex­e­cut­ing, what will hap­pen is that the next gra­di­ent step will start cor­rect­ing for this per­tur­ba­tion, push­ing the es­ti­mate back to­wards the square root of two:

If we give the al­gorithm time to con­verge to within ma­chine pre­ci­sion of the ac­tual square root of two then the fi­nal out­put will be bit-for-bit iden­ti­cal to the re­sult we would have got­ten with­out the per­tur­ba­tion.
Con­sider this for a mo­ment. For most kinds of com­puter code, over­writ­ing a vari­able while the code is run­ning will ei­ther have no effect be­cause the vari­able isn’t used, or it will have a catas­trophic effect and the code will crash, or it will sim­ply cause the code to out­put the wrong an­swer. If I use a de­bug­ger to drop in on a web­server ser­vic­ing an http re­quest and I over­write some vari­able with an ar­bi­trary value just as the code is perform­ing a loop in which this vari­able is used in a cen­tral way, bad things are likely to hap­pen! Most com­puter code is not ro­bust to ar­bi­trary in-flight data mod­ifi­ca­tions.
But this code that com­putes the square root of two is ro­bust to in-flight data mod­ifi­ca­tions, or at least the “cur­rent es­ti­mate” vari­able is. It’s not that our per­tur­ba­tion has no effect: if we change the value, the next iter­a­tion of the al­gorithm will com­pute the ob­jec­tive func­tion and its slope at a com­pletely differ­ent point, and each iter­a­tion af­ter that will be differ­ent to how it would have been if we hadn’t in­ter­vened. The per­tur­ba­tion may change the to­tal num­ber of iter­a­tions be­fore con­ver­gence is reached. But ul­ti­mately the al­gorithm will still out­put an es­ti­mate of the square root of two, and, given time to fully con­verge, it will out­put the ex­act same an­swer it would have out­put with­out the per­tur­ba­tion. This is an un­usual breed of com­puter pro­gram in­deed!
What is hap­pen­ing here is that we have con­structed a phys­i­cal sys­tem con­sist­ing of a com­puter and a python pro­gram that com­putes the square root of two, such that:

for a set of start­ing con­figu­ra­tions (in this case the set of con­figu­ra­tions in which the “cur­rent es­ti­mate” vari­able is set to each rep­re­sentable float­ing point num­ber),

the sys­tem ex­hibits a ten­dency to evolve to­wards a small set of tar­get con­figu­ra­tions (in this case just the sin­gle con­figu­ra­tion in which the “cur­rent es­ti­mate” vari­able is set to the square root of two),

and this ten­dency is ro­bust to in-flight per­tur­ba­tions to the sys­tem’s con­figu­ra­tion (in this case ro­bust­ness is limited to just the di­men­sions cor­re­spond­ing to changes in the “cur­rent es­ti­mate” vari­able).

In this es­say I ar­gue that sys­tems that con­verge to some tar­get con­figu­ra­tion, and will do so de­spite per­tur­ba­tions to the sys­tem, are the sys­tems we should rightly call “op­ti­miz­ing sys­tems”.
Ex­am­ple: build­ing a house
Con­sider a group of hu­mans build­ing a house. Let us con­sider the hu­mans to­gether with the build­ing ma­te­ri­als and con­struc­tion site as a sin­gle phys­i­cal sys­tem. Let us imag­ine that we as­sem­ble this sys­tem in­side a com­pletely closed cham­ber, in­clud­ing food and sleep­ing quar­ters for the hu­mans, light­ing, a power source, con­struc­tion ma­te­ri­als, con­struc­tion blueprint, as well as the phys­i­cal hu­mans with ap­pro­pri­ate in­struc­tions and in­cen­tives to build the house. If we just put these phys­i­cal el­e­ments to­gether we get a sys­tem that has a ten­dency to evolve un­der the nat­u­ral laws of physics to­wards a con­figu­ra­tion in which there is a house match­ing the blueprint.

We could per­turb the sys­tem while the house is be­ing built — say by drop­ping in at night and re­mov­ing some walls or mov­ing some con­struc­tion ma­te­ri­als about — and this phys­i­cal sys­tem will re­cover. The team of hu­mans will come in the next day and find the con­struc­tion ma­te­ri­als that were moved, put in new walls to re­place the ones that were re­moved, and so on.

Just like the square root of two ex­am­ple, here is a phys­i­cal sys­tem with:

A basin of at­trac­tion (all the pos­si­ble ar­range­ments of vi­able hu­mans and build­ing ma­te­ri­als)

A tar­get con­figu­ra­tion set that is small rel­a­tive to the basin of at­trac­tion (those in which the build­ing ma­te­ri­als have been ar­ranged into a house match­ing the de­sign)

A ten­dency to evolve to­wards the tar­get con­figu­ra­tions when start­ing from any point within the basin of at­trac­tion, de­spite in-flight per­tur­ba­tions to the system

Now this sys­tem is not in­finitely ro­bust. If we re­ally scram­ble the ar­range­ment of atoms within this sys­tem then we’ll quickly wind up with a con­figu­ra­tion that does not con­tain any hu­mans, or in which the build­ing ma­te­ri­als are ir­re­vo­ca­bly de­stroyed, and then we will have a sys­tem with­out the ten­dency to evolve to­wards any small set of fi­nal con­figu­ra­tions.

In the phys­i­cal world we are not sur­prised to find sys­tems that have this ten­dency to evolve to­wards a small set of tar­get con­figu­ra­tions. If I pick up my dog while he is sleep­ing and move him by a few inches, he still finds his way to his wa­ter bowl when he wakes up. If I pull a piece of bark off a tree, the tree con­tinues to grow in the same up­ward di­rec­tion. If I make a noise that sur­prises a friend work­ing on some math home­work, the math home­work still gets done. Sys­tems that con­tain liv­ing be­ings reg­u­larly ex­hibit this ten­dency to evolve to­wards tar­get con­figu­ra­tions, and tend to do so in a way that is ro­bust to in-flight per­tur­ba­tions. As a re­sult we are fa­mil­iar with phys­i­cal sys­tems that have this prop­erty, and we are not sur­prised when they arise in our lives.
But phys­i­cal sys­tems in gen­eral do not have the ten­dency to evolve to­wards tar­get con­figu­ra­tions. If I move a billiard ball a few inches to the left while a bunch of billiard balls are en­er­get­i­cally bounc­ing around a billiard table, the balls are likely to come to rest in a very differ­ent po­si­tion than if I had not moved the ball. If I change the tra­jec­tory of a satel­lite a lit­tle bit, the satel­lite does not have any ten­dency to move back into its old or­bit.
The com­puter sys­tems that we have built are still, by and large, more prim­i­tive than the liv­ing sys­tems that we in­habit, and most com­puter sys­tems do not have the ten­dency to evolve ro­bustly to­wards some set of tar­get con­figu­ra­tions, so op­ti­miza­tion al­gorithms as dis­cussed in the pre­vi­ous sec­tion, which do have this prop­erty, are some­what un­usual.
Defin­ing optimization
An op­ti­miz­ing sys­tem is a sys­tem that has a ten­dency to evolve to­wards one of a set of con­figu­ra­tions that we will call the tar­get con­figu­ra­tion set, when started from any con­figu­ra­tion within a larger set of con­figu­ra­tions, which we call the basin of at­trac­tion, and con­tinues to ex­hibit this ten­dency with re­spect to the same tar­get con­figu­ra­tion set de­spite per­tur­ba­tions.
Some sys­tems may have a sin­gle tar­get con­figu­ra­tion to­wards which they in­evitably evolve. Ex­am­ples are a ball in a steep valley with a sin­gle lo­cal min­i­mum, and a com­puter com­put­ing the square root of two. Other sys­tems may have a set of tar­get con­figu­ra­tions and per­turb­ing the sys­tem may cause it to evolve to­wards a differ­ent mem­ber of this set. Ex­am­ples are a ball in a valley with mul­ti­ple lo­cal min­ima, or a tree grow­ing up­wards (per­turb­ing the tree by, for ex­am­ple, cut­ting off some branches while it is grow­ing will prob­a­bly change its fi­nal shape, but will not change its ten­dency to grow to­wards one of the con­figu­ra­tions in which it has reached its max­i­mum size).
We can quan­tify op­ti­miz­ing sys­tems in the fol­low­ing ways.
Ro­bust­ness. Along how many di­men­sions can we per­turb the sys­tem with­out al­ter­ing its ten­dency to evolve to­wards the tar­get con­figu­ra­tion set? What mag­ni­tude per­tur­ba­tion can the sys­tem ab­sorb along these di­men­sions? A self-driv­ing car nav­i­gat­ing through a city may be ro­bust to per­tur­ba­tions that in­volve phys­i­cally mov­ing the car to a differ­ent po­si­tion on the road in the city, but not to per­tur­ba­tions that in­volve chang­ing the state of phys­i­cal mem­ory reg­isters that con­tain crit­i­cal bits of com­puter code in the car’s in­ter­nal com­puter.
Dual­ity. To what ex­tent can we iden­tify sub­sets of the sys­tem cor­re­spond­ing to “that which is be­ing op­ti­mized” and “that which is do­ing the op­ti­miza­tion”? Between en­g­ine and ob­ject of op­ti­miza­tion; be­tween agent and world. Highly du­al­is­tic sys­tems may be ro­bust to per­tur­ba­tions of the ob­ject of op­ti­miza­tion, but brit­tle with re­spect to per­tur­ba­tions of the en­g­ine of op­ti­miza­tion. For ex­am­ple, a sys­tem con­tain­ing a 2020s-era robot mov­ing a vase around is a du­al­is­tic op­ti­miz­ing sys­tem: there is a clear sub­set of the sys­tem that is the en­g­ine of op­ti­miza­tion (the robot), and ob­ject of op­ti­miza­tion (the vase). Fur­ther­more, the robot may be able to deal with a wide va­ri­ety of per­tur­ba­tions to the en­vi­ron­ment and to the vase, but there are likely to be nu­mer­ous small per­tur­ba­tions to the robot it­self that will ren­der it in­ert. In con­trast, a tree is a non-du­al­is­tic op­ti­miz­ing sys­tem: the tree does grow to­wards a set of tar­get con­figu­ra­tions, but it makes no sense to ask which part of the tree is “do­ing” the op­ti­miza­tion and which part is “be­ing” op­ti­mized. This lat­ter ex­am­ple is dis­cussed fur­ther be­low.
Re­tar­getabil­ity. Is it pos­si­ble, us­ing only a micro­scopic per­tur­ba­tion to the sys­tem, to change the sys­tem such that it is still an op­ti­miz­ing sys­tem but with a differ­ent tar­get con­figu­ra­tion set? A sys­tem con­tain­ing a robot with the goal of mov­ing a vase to a cer­tain lo­ca­tion can be mod­ified by mak­ing just a small num­ber of micro­scopic per­tur­ba­tions to key mem­ory reg­isters such that the robot holds the goal of mov­ing the vase to a differ­ent lo­ca­tion and the whole vase/​robot sys­tem now ex­hibits a ten­dency to evolve to­wards a differ­ent tar­get con­figu­ra­tion. In con­trast, a sys­tem con­tain­ing a ball rol­ling to­wards the bot­tom of a valley can­not gen­er­ally be mod­ified by any micro­scopic per­tur­ba­tion such that the ball will roll to a differ­ent tar­get lo­ca­tion. A tree is an in­ter­me­di­ate ex­am­ple: to cause the tree to evolve to­wards a differ­ent tar­get con­figu­ra­tion set — say, one in which its leaves were of a differ­ent shape — one would have to mod­ify the ge­netic code si­mul­ta­neously in all of the tree’s cells.
Re­la­tion­ship to Yud­kowsky’s defi­ni­tion of optimization
In Mea­sur­ing Op­ti­miza­tion Power, Eliezer Yud­kowsky defines op­ti­miza­tion as a pro­cess in which some part of the world ends up in a con­figu­ra­tion that is high in an agent’s prefer­ence or­der­ing, yet has low prob­a­bil­ity of aris­ing spon­ta­neously. Yud­kowsky’s defi­ni­tion asks us to look at a patch of the world that has already un­der­gone op­ti­miza­tion by an agent or mind, and draw con­clu­sions about the power or in­tel­li­gence of that mind by ask­ing how un­likely it would be for a con­figu­ra­tion of equal or greater util­ity (to the agent) to arise spon­ta­neously.
Our defi­ni­tion differs from this in the fol­low­ing ways:

We look at whole sys­tems that evolve nat­u­rally un­der phys­i­cal laws. We do not as­sume that we can de­com­pose these sys­tems into some en­g­ine and ob­ject of op­ti­miza­tion, or into mind and en­vi­ron­ment. We do not look at sys­tems that are “be­ing op­ti­mized” by some ex­ter­nal en­tity but rather at “op­ti­miz­ing sys­tems” that ex­hibit a nat­u­ral ten­dency to evolve to­wards a tar­get con­figu­ra­tion set. Th­ese op­ti­miz­ing sys­tems may con­tain sub­sys­tems that have the prop­er­ties of agents, but as we will see there are many in­stances of op­ti­miz­ing sys­tems that do not con­tain du­al­is­tic agen­tic sub­sys­tems.

When dis­cern­ing the bound­ary be­tween op­ti­miza­tion and non-op­ti­miza­tion, we look prin­ci­pally at ro­bust­ness — whether the sys­tem will con­tinue to evolve to­wards its tar­get con­figu­ra­tion set in the face of per­tur­ba­tions — whereas Yud­kowsky looks at the im­prob­a­bil­ity of the fi­nal con­figu­ra­tion.

Re­la­tion­ship to Drexler’s Com­pre­hen­sive AI Services
Eric Drexler has writ­ten about the need to con­sider AI sys­tems that are not goal-di­rected agents. He points out that the most eco­nom­i­cally im­por­tant AI sys­tems to­day are not con­structed within the agent paradigm, and that in fact agents rep­re­sent just a tiny frac­tion of the de­sign space of in­tel­li­gent sys­tems. For ex­am­ple, a sys­tem that iden­ti­fies faces in images would be an in­tel­li­gent sys­tem but not an agent ac­cord­ing to Drexler’s tax­on­omy. This per­spec­tive is highly rele­vant to our dis­cus­sion here since we seek to go be­yond the nar­row agent model in which in­tel­li­gent sys­tems are con­ceived of as uni­tary en­tities that re­ceive ob­ser­va­tions from the en­vi­ron­ment, send ac­tions back into the en­vi­ron­ment, but are oth­er­wise sep­a­rate from the en­vi­ron­ment.
Our per­spec­tive is that there is a spe­cific class of in­tel­li­gent sys­tems — which we call op­ti­miz­ing sys­tems — that are wor­thy of spe­cial at­ten­tion and study due to their po­ten­tial to re­shape the world. The set of op­ti­miz­ing sys­tems is smaller than the set of all AI ser­vices, but larger than the set of goal-di­rected agen­tic sys­tems.

Figure: re­la­tion­ship be­tween our op­ti­miz­ing sys­tem con­cept and Drexler’s tax­on­omy of AI systems
Ex­am­ples of sys­tems that lie in each of these three tiers are as fol­lows:

A sys­tem that iden­ti­fies faces in images by eval­u­at­ing a feed-for­ward neu­ral net­work is an AI sys­tem but not an op­ti­miz­ing sys­tem.

A tree is an op­ti­miz­ing sys­tem but not a goal-di­rected agent sys­tem (see sec­tion be­low an­a­lyz­ing a tree as an op­ti­miz­ing sys­tem).

A robot with the goal of mov­ing a ball to a spe­cific des­ti­na­tion is a goal-di­rected agent sys­tem.

Re­la­tion­ship to Garrabrant and Dem­ski’s Embed­ded Agency
Scott Garrabrant and Abram Dem­ski have writ­ten about the many ways that a du­al­is­tic view of agency in which one con­ceives of a hard sep­a­ra­tion be­tween agent and en­vi­ron­ment fails to cap­ture the re­al­ity of agents that are re­ducible to the same ba­sic build­ing-blocks as the en­vi­ron­ments in which they are em­bed­ded. They show that if one starts from a du­al­is­tic view of agency then it is difficult to de­sign agents ca­pa­ble of re­flect­ing on and mak­ing im­prove­ments to their own cog­ni­tive pro­cesses, since the du­al­is­tic view of agency rests on a uni­tary agent whose cog­ni­tion does not af­fect the world ex­cept via ex­plicit ac­tions. They also show that rea­son­ing about coun­ter­fac­tu­als be­comes non­sen­si­cal if start­ing from a du­al­is­tic view of agency, since the agent’s cog­ni­tive pro­cesses are gov­erned by the same phys­i­cal laws as those that gov­ern the en­vi­ron­ment, and the agent can come to no­tice this fact, lead­ing to con­fu­sion when con­sid­er­ing the con­se­quences ac­tions that are differ­ent from the ac­tions that the agent will, in fact, out­put.
One could view the Embed­ded Agency work as enu­mer­at­ing the many log­i­cal pit­falls one falls into if one takes the “op­ti­mizer” con­cept as the start­ing point for de­sign­ing in­tel­li­gent sys­tems, rather than “op­ti­miz­ing sys­tem” as we pro­pose here. The pre­sent work is strongly in­spired by Garrabrant and Dem­ski’s work. Our hope is to point the way to a view of op­ti­miza­tion and agency that cap­tures re­al­ity suffi­ciently well to avoid the log­i­cal pit­falls iden­ti­fied in the Embed­ded Agency work.
Ex­am­ple: ball in a valley
Con­sider a phys­i­cal ball rol­ling around in a small valley. Ac­cord­ing to our defi­ni­tion of op­ti­miza­tion, this is an op­ti­miz­ing sys­tem:
Con­figu­ra­tion space. The sys­tem we are study­ing con­sists of the phys­i­cal valley plus the ball
Basin of at­trac­tion. The ball could ini­tially be placed any­where in the valley (these are the con­figu­ra­tions com­pris­ing the basin of at­trac­tion)
Tar­get con­figu­ra­tion set. The ball will roll un­til it ends up at the bot­tom of the valley (the set of lo­cal min­ima are the tar­get con­figu­ra­tions)
We can per­turb the ball while it is “in flight”, say by chang­ing its po­si­tion or ve­loc­ity, and the ball will still ul­ti­mately end up at one of the tar­get con­figu­ra­tions. This sys­tem is ro­bust to per­tur­ba­tions along di­men­sions cor­re­spond­ing to the spa­tial po­si­tion and ve­loc­ity of the ball, but there are many more di­men­sions along which this sys­tem is not ro­bust. If we change the shape of the ball to a cube, for ex­am­ple, then the ball will not con­tinue rol­ling to the bot­tom of the valley.

Ex­am­ple: ball in valley with robot
Con­sider now a ball in a valley as above, but this time with the ad­di­tion of an in­tel­li­gent robot hold­ing the goal of en­sur­ing that the ball reaches the bot­tom of the valley.
Con­figu­ra­tion space. The sys­tem we are study­ing now con­sists of the phys­i­cal valley, the ball, and the robot. We con­sider the evolu­tion of and per­tur­ba­tions to this whole joint sys­tem.
Tar­get con­figu­ra­tion set. As be­fore, the tar­get con­figu­ra­tion is the ball be­ing at the bot­tom of the valley
Basin of at­trac­tion. As be­fore, the basin of at­trac­tion con­sists of all the pos­si­ble spa­tial lo­ca­tions that the ball could be placed in the valley.
We can now per­turb the sys­tem along many more di­men­sions than in the case where there was no robot. For ex­am­ple, we could in­tro­duce a bar­rier that pre­vents the ball from rol­ling down­hill past a cer­tain point, and we can then ex­pect a suffi­ciently in­tel­li­gent robot to move the ball over the bar­rier. We can ex­pect a suffi­ciently well-de­signed robot to be able to over­come a wide va­ri­ety of hur­dles that grav­ity would not over­come on its own. There­fore we say that this sys­tem is more ro­bust than the sys­tem with­out the robot.
There is a se­quence of sys­tems span­ning the gap be­tween a ball rol­ling in a valley, which is ro­bust to a nar­row set of per­tur­ba­tions and there­fore we say ex­hibits a weak de­gree of op­ti­miza­tion, up to a robot with a goal of mov­ing a ball around in a valley, which is ro­bust to a much wider set of per­tur­ba­tions, and there­fore we say ex­hibits a stronger de­gree of op­ti­miza­tion. There­fore the differ­ence be­tween sys­tems that do and do not un­dergo op­ti­miza­tion is not a bi­nary dis­tinc­tion but a con­tin­u­ous gra­di­ent of in­creas­ing ro­bust­ness to per­tur­ba­tions.
By in­tro­duc­ing the robot to the sys­tem we have also in­tro­duced new di­men­sions along which the sys­tem is frag­ile: the di­men­sions cor­re­spond­ing to mod­ifi­ca­tions to the robot it­self, and in par­tic­u­lar the di­men­sions cor­re­spond­ing to mod­ifi­ca­tions to the code run­ning on the robot (i.e. phys­i­cal per­tur­ba­tions to the con­figu­ra­tion of the mem­ory cells in which the code is stored). There are two types of per­tur­ba­tion we might con­sider:

Per­tur­ba­tions that de­stroy the robot. There are nu­mer­ous ways we could cut wires or scram­ble com­puter code that would leave the robot com­pletely non-op­er­a­tional. Many of these would be phys­i­cally micro­scopic, such as flip­ping a sin­gle bit in a mem­ory cell con­tain­ing some crit­i­cal com­puter code. In fact there are now more ways to break the sys­tem via micro­scopic per­tur­ba­tions com­pared to when we were con­sid­er­ing a ball in a valley with­out a robot, since there are few ways to cause a ball not to reach the bot­tom of a valley by mak­ing only a micro­scopic per­tur­ba­tion to the sys­tem, but there are many ways to break mod­ern com­puter sys­tems via a micro­scopic per­tur­ba­tion.

Per­tur­ba­tions that change the tar­get con­figu­ra­tions. We could also make phys­i­cally micro­scopic per­tur­ba­tions to this sys­tem that change the robot’s goal. For ex­am­ple we might flip the sign on some crit­i­cal com­pu­ta­tions in the robot’s code such that the robot works to place the ball at the high­est point rather than the low­est. This is still a phys­i­cal per­tur­ba­tion to the valley/​ball/​robot sys­tem: it is one that af­fects the con­figu­ra­tion of the mem­ory cells con­tain­ing the robot’s com­puter code. Th­ese kinds of per­tur­ba­tions may point to a con­cept with some similar­ity to that of an agent. If we have a sys­tem that can be per­turbed in a way that pre­serves the ro­bust­ness of the basin of con­ver­gence but changes the tar­get con­figu­ra­tion to­wards which the sys­tem tends to evolve, and if we can find per­tur­ba­tions that cause the tar­get con­figu­ra­tions to match our own goals, then we have a way to nav­i­gate be­tween con­ver­gence bas­ins.

Ex­am­ple: com­puter perform­ing gra­di­ent descent
Con­sider now a com­puter run­ning an iter­a­tive gra­di­ent de­scent al­gorithm in or­der to solve an op­ti­miza­tion prob­lem. For con­crete­ness let us imag­ine that the ob­jec­tive func­tion be­ing op­ti­mized is globally con­vex, in which case the al­gorithm will cer­tainly reach the global op­ti­mum given suffi­cient time. Let us fur­ther imag­ine that the com­puter stores its cur­rent best es­ti­mate of the lo­ca­tion of the global op­ti­mum (which we will hence­forth call the “op­ti­mizand”) at some known mem­ory lo­ca­tion, and up­dates this af­ter ev­ery iter­a­tion of gra­di­ent de­scent.
Since this is a purely com­pu­ta­tional pro­cess, it may be tempt­ing to define the con­figu­ra­tion space at the com­pu­ta­tional level — for ex­am­ple by tak­ing the con­figu­ra­tion space to be the do­main of the ob­jec­tive func­tion. How­ever, it is of ut­most im­por­tance when an­a­lyz­ing any op­ti­miz­ing sys­tem to ground our anal­y­sis in a phys­i­cal sys­tem evolv­ing ac­cord­ing to the phys­i­cal laws of na­ture, just as we have for all pre­vi­ous ex­am­ples. The rea­son this is im­por­tant is to en­sure that we always study com­plete sys­tems, not just some in­ert part of the sys­tem that is “be­ing op­ti­mized” by some­thing ex­ter­nal to the sys­tem. There­fore we an­a­lyze this sys­tem as fol­lows.
Con­figu­ra­tion space. The sys­tem con­sists of a phys­i­cal com­puter run­ning some code that performs gra­di­ent de­scent. The con­figu­ra­tions of the sys­tem are the phys­i­cal con­figu­ra­tions of the atoms com­pris­ing the com­puter.
Tar­get-con­figu­ra­tion set. The tar­get con­figu­ra­tion set con­sists of the set of phys­i­cal con­figu­ra­tions of the com­puter in which the mem­ory cells that store the cur­rent op­ti­mized state con­tain the true lo­ca­tion of the global op­ti­mum (or the clos­est float­ing point rep­re­sen­ta­tion of it).
Basin of at­trac­tion. The basin of at­trac­tion con­sists of the set of phys­i­cal con­figu­ra­tions in which there is a vi­able com­puter and it is run­ning the gra­di­ent de­scent al­gorithm.
Ex­am­ple: billiard balls
Let us now ex­am­ine a sys­tem that is not an op­ti­miz­ing sys­tem ac­cord­ing to our defi­ni­tion. Con­sider a billiard table with some billiard balls that are cur­rently bounc­ing around in mo­tion. Left alone, the balls will even­tu­ally come to rest in some con­figu­ra­tion. Is this an op­ti­miz­ing sys­tem?
In or­der to qual­ify as an op­ti­miz­ing sys­tem, a sys­tem must (1) have a ten­dency to evolve to­wards a set of tar­get con­figu­ra­tions that are small rel­a­tive to the basin of at­trac­tion, and (2) con­tinue to evolve to­wards the same set of tar­get con­figu­ra­tions if per­turbed.
If we reach in while the billiard balls are bounc­ing around and move one of the balls that is in mo­tion, the sys­tem will now come to rest in a differ­ent con­figu­ra­tion. There­fore this is not an op­ti­miz­ing sys­tem, be­cause there is no set of tar­get con­figu­ra­tions to­wards which the sys­tem evolves de­spite per­tur­ba­tions. A sys­tem does not need to be ro­bust along all di­men­sions in or­der to be an op­ti­miz­ing sys­tem, but a billiard table ex­hibits no such ro­bust di­men­sions at all, so it is not an op­ti­miz­ing sys­tem.
Ex­am­ple: satel­lite in orbit
Con­sider a sec­ond ex­am­ple of a sys­tem that is not an op­ti­miz­ing sys­tem: a satel­lite in or­bit around Earth. Un­like the billiard balls, there is no chaotic ten­dency for small per­tur­ba­tions to lead to large de­vi­a­tions in the sys­tem’s evolu­tion, but nei­ther is there any ten­dency for the sys­tem to come back to some tar­get con­figu­ra­tion when per­turbed. If we per­turb the satel­lite’s ve­loc­ity or po­si­tion, then from that point on it is in a differ­ent or­bit and has no ten­dency to re­turn to its pre­vi­ous or­bit. There is no set of tar­get con­figu­ra­tions to­wards which the sys­tem evolves de­spite per­tur­ba­tions, so this is not an op­ti­miz­ing sys­tem.
Ex­am­ple: a tree
Con­sider a patch of fer­tile ground with a tree grow­ing in it. Is this an op­ti­miz­ing sys­tem?
Con­figu­ra­tion space. For the sake of con­crete­ness let us take a re­gion of space that is sealed off from the out­side world — say 100m x 100m x 100m. This re­gion is filled at the bot­tom with fer­tile soil and at the top with an at­mo­sphere con­ducive to the tree’s growth. Let us say that the re­gion con­tains a sin­gle tree.
We will an­a­lyze this sys­tem in terms of the ar­range­ment of atoms in­side this re­gion of space. Out of all the pos­si­ble con­figu­ra­tions of these atoms, the vast ma­jor­ity con­sist of a uniform hazy gas. An as­tro­nom­i­cally tiny frac­tion of con­figu­ra­tions con­tain a non-triv­ial mass of com­plex biolog­i­cal nu­tri­ents mak­ing up soil. An even tinier frac­tion of con­figu­ra­tions con­tain a vi­able tree.
Tar­get-con­figu­ra­tion set. A tree has a ten­dency to grow taller over time, to sprout more branches and leaves, and so on. Fur­ther­more, trees can only grow so tall due to the physics of trans­port­ing sug­ars up and down the trunk. So we can iden­tify a set of tar­get con­figu­ra­tions in which the atoms in our re­gion of space are ar­ranged into a tree that has grown to its max­i­mum size (has sprouted as many branches and leaves as it can sup­port given the at­mo­sphere, the soil that it is grow­ing in, and the con­straints of its own biol­ogy). There are many topolo­gies in which the tree’s branches could di­vide, many po­si­tions that leaves could sprout in, and so on, so there are many con­figu­ra­tions within the tar­get con­figu­ra­tion set. But this set is still tiny com­pared to all the ways that the same atoms could be ar­ranged with­out the con­straint of form­ing a vi­able tree.
Basin of con­ver­gence. This sys­tem will evolve to­wards the tar­get con­figu­ra­tion set start­ing from any con­figu­ra­tion in which there is a vi­able tree. This in­cludes con­figu­ra­tions in which there is just a seed in the ground, as well as con­figu­ra­tions in which there is a tree of small, medium, or large size. Start­ing from any of these con­figu­ra­tions, if we leave the sys­tem to evolve un­der the nat­u­ral laws of physics then the tree will grow to­wards its max­i­mum size, at which point the sys­tem will be in one of the tar­get con­figu­ra­tions.
Ro­bust­ness to per­tur­ba­tions. This sys­tem is highly ro­bust to per­tur­ba­tions. Con­sider per­turb­ing the sys­tem in any of the fol­low­ing ways:

Mov­ing soil from one place to another

Re­mov­ing some leaves from the tree

Cut­ting a branch off the tree

Th­ese per­tur­ba­tions might change which par­tic­u­lar tar­get con­figu­ra­tion is even­tu­ally reached — the par­tic­u­lar ar­range­ment of branches and leaves in the tree once it reaches its max­i­mum size — but they will not stop the tree from grow­ing taller and evolv­ing to­wards a tar­get con­figu­ra­tion. In fact we could cut the tree right at the base of the trunk and it would con­tinue to evolve to­wards a tar­get con­figu­ra­tion by sprout­ing a new trunk and grow­ing a whole new tree.
Dual­ity. A tree is a non-du­al­is­tic op­ti­miz­ing sys­tem. There is no sub­sys­tem that is re­spon­si­ble for “do­ing” the op­ti­miza­tion, sep­a­rately from that which is “be­ing” op­ti­mized. Yet the tree does ex­hibit a ten­dency to evolve to­wards a set of tar­get con­figu­ra­tions, and can over­come a wide va­ri­ety of per­tur­ba­tions in or­der to do so. There are no man-made sys­tems in ex­is­tence to­day that are ca­pa­ble of gath­er­ing and uti­liz­ing re­sources so flex­ibly as a tree, from so broad a va­ri­ety of en­vi­ron­ments, and there are cer­tainly no man-made sys­tems that can re­cover from be­ing phys­i­cally dis­mem­bered to such an ex­tent that a tree can re­cover from be­ing cut at the trunk.
At this point it may be tempt­ing to say that the en­g­ine of op­ti­miza­tion is nat­u­ral se­lec­tion. But re­call that we are study­ing just a sin­gle tree grow­ing from seed to max­i­mum size. Can you iden­tify a phys­i­cal sub­set of our 100m x 100m x 100m re­gion of space that is this en­g­ine of op­ti­miza­tion, analo­gous to how we iden­ti­fied a phys­i­cal sub­set of the robot-and-ball sys­tem as the en­g­ine of op­ti­miza­tion (i.e. the phys­i­cal robot)? Nat­u­ral se­lec­tion might be the pro­cess by which the ini­tial sys­tem came into ex­is­tence, but it is not the pro­cess that drives the growth of the tree to­wards a tar­get con­figu­ra­tion.
It may then be tempt­ing to say that it is the tree’s DNA that is the en­g­ine of op­ti­miza­tion. It is true that the tree’s DNA ex­hibits some char­ac­ter­is­tics of an en­g­ine of op­ti­miza­tion: it re­mains un­changed through­out the life of the tree, and phys­i­cally micro­scopic per­tur­ba­tions to it can dis­able the tree. But a tree repli­cates its DNA in each of its cells, and per­turb­ing just one or a small num­ber of these is not likely to af­fect the tree’s over­all growth tra­jec­tory. More im­por­tantly, a sin­gle strand of DNA does not re­ally have agency on its own: it re­quires the molec­u­lar ma­chin­ery of the whole cell to syn­the­size pro­teins based on the ge­netic code in the DNA, and the phys­i­cal ma­chin­ery of the whole tree to col­lect and de­ploy en­ergy, wa­ter, and nu­tri­ents. Just as it would be in­cor­rect to iden­tify the mem­ory reg­isters con­tain­ing com­puter code within a robot as the “true” en­g­ine of op­ti­miza­tion sep­a­rate from the rest of the com­put­ing and phys­i­cal ma­chin­ery that brings this code to life, it is not quite ac­cu­rate to iden­tify DNA as an en­g­ine of op­ti­miza­tion. A tree sim­ply does not de­com­pose into en­g­ine and ob­ject of op­ti­miza­tion.
It may also be tempt­ing to ask whether the tree can “re­ally” be said to be un­der­go­ing op­ti­miza­tion in the ab­sence of any “in­ten­tion” to reach one of the tar­get con­figu­ra­tions. But this ex­pec­ta­tion of a cen­tral­ized mind with cen­tral­ized in­ten­tions is re­ally an ar­ti­fact of us pro­ject­ing our view of our self onto the world: we be­lieve that we have a cen­tral­ized mind with cen­tral­ized in­ten­tions, so we fo­cus our at­ten­tion on op­ti­miz­ing sys­tems with a similar struc­ture. But this turns out to be mis­guided on two counts: first, the vast ma­jor­ity of op­ti­miz­ing sys­tems do not con­tain cen­tral­ized minds, and sec­ond, our own minds are ac­tu­ally far less cen­tral­ized than we think! For now we put this ques­tion of whether op­ti­miza­tion re­quires in­ten­tions and in­stead just work within our defi­ni­tion of op­ti­miz­ing sys­tems, which a tree definitely satis­fies.
Ex­am­ple: bot­tle cap
Daniel Filan has pointed out that some defi­ni­tions of op­ti­miza­tion would non­sen­si­cally clas­sify a bot­tle cap as an op­ti­mizer, since a bot­tle cap causes wa­ter molecules in a bot­tle to stay in­side the bot­tle, and the set of con­figu­ra­tions in which the molecules are in­side a bot­tle is much smaller than the set of con­figu­ra­tions in which the molecules are each al­lowed to take a po­si­tion ei­ther in­side or out­side the bot­tle.
In our frame­work we have the fol­low­ing:

The sys­tem con­sists of a bot­tle, a bot­tle cap, and wa­ter molecules. The con­figu­ra­tion space con­sists of all the pos­si­ble spa­tial ar­range­ments of wa­ter molecules, ei­ther in­side or out­side the bot­tle.

The basin of at­trac­tion is the set of con­figu­ra­tions in which the wa­ter molecules are in­side the bottle

The tar­get con­figu­ra­tion set is the same as the basin of attraction


This is not an op­ti­miz­ing sys­tem for two rea­sons.
First, the tar­get con­figu­ra­tion set is no smaller than the basin of at­trac­tion. To be an op­ti­miz­ing sys­tem there must be a ten­dency to evolve from any con­figu­ra­tion within a basin of at­trac­tion to­wards a smaller tar­get con­figu­ra­tion set, but in this case the sys­tem merely re­mains within the set of con­figu­ra­tions in which the wa­ter molecules are in­side the bot­tle. This is no differ­ent from a rock sit­ting on a beach: due to ba­sic chem­istry there is a ten­dency to re­main within the set of con­figu­ra­tions in which the molecules com­pris­ing the rock are phys­i­cally bound to one an­other, but it has no ten­dency to evolve from a wide basin of at­trac­tion to­wards a small set of tar­get con­figu­ra­tion.
Se­cond, the bot­tle cap sys­tem is not ro­bust to per­tur­ba­tions since if we per­turb the po­si­tion of a sin­gle wa­ter molecule so that it is out­side the bot­tle, there is no ten­dency for it to move back in­side the bot­tle. This is re­ally just the first point above restated, since if there were a ten­dency for wa­ter molecules moved out­side the bot­tle to evolve back to­wards a con­figu­ra­tion in which all the wa­ter molecules were in­side the bot­tle, then we would have a basin of at­trac­tion larger than the tar­get con­figu­ra­tion set.
Ex­am­ple: the hu­man liver
Filan also asks whether one’s liver should be con­sid­ered an op­ti­mizer. Sup­pose we ob­serve a hu­man work­ing to make money. If this per­son were de­prived of a liver, or if their liver stopped func­tion­ing, they would pre­sum­ably be un­able to make money. So are we then to view the liver as an op­ti­mizer work­ing to­wards the goal of mak­ing money? Filan asks this ques­tion as a challenge to Yud­kowsky’s defi­ni­tion of op­ti­miza­tion, since it seems ab­surd to view one’s liver as an op­ti­mizer work­ing to­wards the goal of mak­ing money, yet Yud­kowsky’s defi­ni­tion of op­ti­miza­tion might clas­sify it as such.
In our frame­work we have the fol­low­ing:

The sys­tem con­sists of a hu­man work­ing to make money, to­gether with the whole hu­man econ­omy and world.

The basin of at­trac­tion con­sists of the con­figu­ra­tions in which there is a healthy hu­man (with a healthy liver) hav­ing the goal of mak­ing money

The tar­get con­figu­ra­tions are those in which this per­son’s bank bal­ance is high. (In­ter­est­ingly there is no up­per bound here, so there is no fixed point but rather a con­tin­u­ous gra­di­ent.)

We can ex­pect that this per­son is ca­pa­ble of over­com­ing a rea­son­ably broad va­ri­ety of ob­sta­cles in pur­suit of mak­ing money, so we rec­og­nize that this over­all sys­tem (the hu­man to­gether with the whole econ­omy) is an op­ti­miz­ing sys­tem. But Filan would surely agree on this point and his ques­tion is more spe­cific: he is ask­ing whether the liver is an op­ti­mizer.
In gen­eral we can­not ex­pect to de­com­pose op­ti­miz­ing sys­tems into an en­g­ine of op­ti­miza­tion and ob­ject of op­ti­miza­tion. We can see that the sys­tem has the char­ac­ter­is­tics of an op­ti­miz­ing sys­tem, and we may iden­tify parts, in­clud­ing in this case the per­son’s liver, that are nec­es­sary for these char­ac­ter­is­tics to ex­ist, but we can­not in gen­eral iden­tify any crisp sub­set of the sys­tem as that which is do­ing the op­ti­miza­tion. And pick­ing var­i­ous sub­com­po­nents of the sys­tem (such as the per­son’s liver) and ask­ing “is this the part that is do­ing the op­ti­miza­tion?” does not in gen­eral have an an­swer.
By anal­ogy, sup­pose we looked at a planet or­bit­ing a star and asked: “which part here is do­ing the or­bit­ing?” Is it the planet or the star that is the “en­g­ine of or­bit­ing”? Or sup­pose we looked at a car and no­ticed that the fuel pump is a com­plex piece of ma­chin­ery with­out which the car’s lo­co­mo­tion would cease. We might ask: is this fuel pump the true “en­g­ine of lo­co­mo­tion”? Th­ese ques­tions don’t have an­swers be­cause they mis­tak­enly pre­sup­pose that we can iden­tify a sub­sys­tem that is uniquely re­spon­si­ble for the or­bit­ing of the planet or the lo­co­mo­tion of the car. Ask­ing whether a hu­man liver is an “op­ti­mizer” is similarly mis­taken: we can see that the liver is a com­plex piece of ma­chin­ery that is nec­es­sary in or­der for the over­all sys­tem to ex­hibit the char­ac­ter­is­tics of an op­ti­miz­ing sys­tem (ro­bust evolu­tion to­wards a tar­get con­figu­ra­tion set), but be­yond this it makes no more sense to ask whether the liver is a true “lo­cus of op­ti­miza­tion”.
So rather than an­swer­ing Filan’s ques­tion in ei­ther the pos­i­tive or the nega­tive, the ap­pro­pri­ate move is to dis­solve the con­cept of an op­ti­mizer, and in­stead ask whether the over­all sys­tem is an op­ti­miz­ing sys­tem.
Ex­am­ple: the uni­verse as a whole
Con­sider the whole phys­i­cal uni­verse as a sin­gle closed sys­tem. Is this an op­ti­miz­ing sys­tem?
The sec­ond law of ther­mo­dy­nam­ics tells us that the uni­verse is evolv­ing to­wards a max­i­mally di­s­or­dered ther­mo­dy­namic equil­ibrium in which it cy­cles through var­i­ous max­en­tropy con­figu­ra­tion. We might then imag­ine that the uni­verse is an op­ti­miz­ing sys­tem in which the basin of at­trac­tion is all pos­si­ble con­figu­ra­tions of mat­ter and en­ergy, and the tar­get con­figu­ra­tion set con­sists of the max­en­tropy con­figu­ra­tions.
How­ever, this is not quite ac­cu­rate. Out of all pos­si­ble con­figu­ra­tions of the uni­verse, the vast ma­jor­ity of con­figu­ra­tions are at or close to max­i­mum en­tropy. That is, if we sam­ple a con­figu­ra­tion of the uni­verse at ran­dom, we have only an as­tro­nom­i­cally tiny chance of find­ing any­thing other than a close-to-uniform gas of ba­sic par­ti­cles. If we define the basin of at­trac­tion as all pos­si­ble con­figu­ra­tions of mat­ter in the uni­verse and the tar­get con­figu­ra­tion set as the set of max­en­tropy con­figu­ra­tions, then the tar­get con­figu­ra­tion set ac­tu­ally con­tains al­most the en­tirety of the basin of at­trac­tion, with the only con­figu­ra­tions that are in the basin of at­trac­tion but not the tar­get con­figu­ra­tion set be­ing the highly un­usual con­figu­ra­tions of mat­ter con­tain­ing stars, galax­ies, and so on.

For this rea­son the uni­verse as a whole does not qual­ify as an op­ti­miz­ing sys­tem un­der our defi­ni­tion. (Or per­haps it would be more ac­cu­rate to say that it qual­ifies as an ex­tremely weak op­ti­miz­ing sys­tem.)
Power sources and entropy
The sec­ond law of ther­mo­dy­nam­ics tells us that any closed sys­tem will even­tu­ally tend to­wards a max­i­mally di­s­or­dered state in which mat­ter and en­ergy is spread ap­prox­i­mately uniformly through space. So if we were to iso­late one of the sys­tems ex­plore above in­side a sealed cham­ber and leave it for a very long pe­riod then even­tu­ally what­ever power source we put in­side the sealed cham­ber would be­come de­pleted, and then even­tu­ally af­ter that ev­ery com­plex ma­te­rial or com­pound in the sys­tem would de­grade into its base prod­ucts, and then fi­nally we would be left with a cham­ber filled with a uniform gaseous mix­ture of what­ever base el­e­ments we origi­nally put in.
So in this sense there are no op­ti­miz­ing sys­tems at all, since any of the sys­tems above evolve to­wards their tar­get con­figu­ra­tion sets only for a finite pe­riod of time, af­ter which they de­grade and evolve to­wards a max­en­tropy con­figu­ra­tion.
This is not a very se­ri­ous challenge to our defi­ni­tion of op­ti­miza­tion since it is com­mon through­out physics and com­puter sci­ence to study var­i­ous “steady-state” or “fixed point” sys­tems even though the same ob­jec­tion could be made about any of them. We say that a ther­mome­ter can be used to build a heat reg­u­la­tor that will keep the tem­per­a­ture of a house within a de­sired range, and we do not usu­ally need to add the caveat that even­tu­ally the house and reg­u­la­tor will de­grade into a uniform gaseous mix­ture due to the heat death of the uni­verse.
Nev­er­the­less, two pos­si­ble ways to re­fine our defi­ni­tion are:

We could stipu­late that some power source is pro­vided ex­ter­nally to each sys­tem we an­a­lyze, and then perform our anal­y­sis con­di­tional on the ex­is­tence of that power source.

We could spec­ify a finite time hori­zon and say that “a sys­tem is an op­ti­miz­ing sys­tem if it tends to­wards a tar­get con­figu­ra­tion set up to time T”.

Con­nec­tion to dy­nam­i­cal sys­tems theory
The con­cept of “op­ti­miz­ing sys­tem” in this es­say is very close to that of a dy­nam­i­cal sys­tem with one or more at­trac­tors. We offer the fol­low­ing re­marks on this con­nec­tion.

A gen­eral dy­nam­i­cal sys­tem is any sys­tem with a state that evolves over time as a func­tion of the state it­self. This en­com­passes a very broad range of sys­tems in­deed!

In dy­nam­i­cal sys­tem the­ory, an at­trac­tor is the term used for what we have called the tar­get con­figu­ra­tion set. A fixed point at­trac­tor is, in our lan­guage, a tar­get con­figu­ra­tion set with just one el­e­ment, such as when com­put­ing the square root of two. A limit cy­cle is, in our lan­guage, a sys­tem that even­tu­ally sta­bly loops through a se­quence of states all of which are in the tar­get con­figu­ra­tion set, such as a satel­lite in or­bit.

We have dis­cussed sys­tems that evolve to­wards tar­get con­figu­ra­tions along some di­men­sions but not oth­ers (e.g. ball in a valley). We have not yet dis­cov­ered whether dy­nam­i­cal sys­tems the­ory ex­plic­itly stud­ies at­trac­tors that op­er­ate along a sub­set of the sys­tem’s di­men­sions.

There is a con­cept of “well-posed­ness” in dy­nam­i­cal sys­tems the­ory that jus­tifies the iden­ti­fi­ca­tion of a math­e­mat­i­cal model with a phys­i­cal sys­tem. The con­di­tions for a model to be well-posed are (1) that a solu­tion ex­ists (i.e. the model is not self-con­tra­dic­tory), (2) that there is a unique solu­tion (i.e. the model con­tains enough in­for­ma­tion to pick out a sin­gle sys­tem tra­jec­tory), and (3) that the solu­tion changes con­tin­u­ously with the ini­tial con­di­tions (the be­hav­ior of the sys­tem is not too chaotic). This third con­di­tion may pre­sent an in­ter­est­ing av­enue for fu­ture in­ves­ti­ga­tion as it seems re­lated to but not quite equiv­a­lent to our no­tion of ro­bust­ness since ro­bust­ness as we define it ad­di­tion­ally re­quires that the sys­tem con­tinue to evolve to­wards the same at­trac­tor state de­spite per­tur­ba­tions. Ex­plor­ing this con­nec­tion may pre­sent an in­ter­est­ing av­enue for fu­ture in­ves­ti­ga­tion.

Conclusion
We have pro­posed a con­cept that we call “op­ti­miz­ing sys­tems” to de­scribe sys­tems that have a ten­dency to evolve to­wards a nar­row tar­get con­figu­ra­tion set when started from any point within a broader basin of at­trac­tion, and con­tinue to do so de­spite per­tur­ba­tions.
We have an­a­lyzed op­ti­miz­ing sys­tems along three di­men­sions:

Ro­bust­ness, which mea­sures the num­ber of di­men­sions along which the sys­tem is ro­bust to per­tur­ba­tions, and the mag­ni­tude of per­tur­ba­tion along these di­men­sions that the sys­tem can with­stand.

Dual­ity, which mea­sures the ex­tent to which an ap­prox­i­mate “en­g­ine of op­ti­miza­tion” sub­sys­tem can be iden­ti­fied.

Re­tar­getabil­ity, which mea­sures the ex­tent to which the sys­tem can be trans­formed via micro­scopic per­tur­ba­tions into an equally ro­bust op­ti­miz­ing sys­tem but with a differ­ent tar­get con­figu­ra­tion set.

We have ar­gued that the “op­ti­mizer” con­cept rests on an as­sump­tion that op­ti­miz­ing sys­tems can be de­com­posed into en­g­ine and ob­ject of op­ti­miza­tion (or agent and en­vi­ron­ment, or mind and world). We have de­scribed sys­tems that do ex­hibit op­ti­miza­tion yet can­not be de­com­posed this way, such as the tree ex­am­ple. We have also pointed out that, even among those sys­tems that can be de­com­posed ap­prox­i­mately into en­g­ine and ob­ject of op­ti­miza­tion (for ex­am­ple, a robot mov­ing a ball around), we will not in gen­eral be able to mean­ingfully an­swer the ques­tion of whether ar­bi­trary sub­com­po­nents of the agent are an op­ti­mizer not (c.f. the hu­man liver ex­am­ple).
There­fore, while the “op­ti­mizer” con­cept clearly still has much util­ity in de­sign­ing in­tel­li­gent sys­tems, we should be cau­tious about tak­ing it as a prim­i­tive in our un­der­stand­ing of the world. In par­tic­u­lar we should not ex­pect ques­tions of the form “is X an op­ti­mizer?” to always have an­swers.What links here?Cyborgism by NicholasKees (10 Feb 2023 14:47 UTC; 328 points)Why Agent Foun­da­tions? An Overly Ab­stract Explanation by johnswentworth (25 Mar 2022 23:17 UTC; 283 points)The Plan by johnswentworth (10 Dec 2021 23:41 UTC; 253 points)Utility Max­i­miza­tion = De­scrip­tion Length Minimization by johnswentworth (18 Feb 2021 18:04 UTC; 195 points)Matt Botv­inick on the spon­ta­neous emer­gence of learn­ing algorithms by Adam Scholl (12 Aug 2020 7:47 UTC; 153 points)How would a lan­guage model be­come goal-di­rected? by David Mears (EA Forum; 16 Jul 2022 14:50 UTC; 113 points)Vot­ing Re­sults for the 2020 Review by Raemon (2 Feb 2022 18:37 UTC; 108 points)Prizes for the 2020 Review by Raemon (20 Feb 2022 21:07 UTC; 94 points)Un­nat­u­ral Cat­e­gories Are Op­ti­mized for Deception by Zack_M_Davis (8 Jan 2021 20:54 UTC; 89 points)Op­ti­miza­tion at a Distance by johnswentworth (16 May 2022 17:58 UTC; 88 points)Search­ing for Search by NicholasKees (28 Nov 2022 15:31 UTC; 81 points)AI take­off story: a con­tinu­a­tion of progress by other means by Edouard Harris (27 Sep 2021 15:55 UTC; 76 points)Long-Term Fu­ture Fund: July 2021 grant recommendations by abergal (EA Forum; 18 Jan 2022 8:49 UTC; 75 points)2020 Re­view Article by Vaniver (14 Jan 2022 4:58 UTC; 74 points)Liter­a­ture Re­view on Goal-Directedness by adamShimi (18 Jan 2021 11:15 UTC; 74 points)Op­ti­miza­tion Con­cepts in the Game of Life by Vika (16 Oct 2021 20:51 UTC; 74 points)The “Mea­sur­ing Stick of Utility” Problem by johnswentworth (25 May 2022 16:17 UTC; 72 points)Dis­cov­er­ing Agents by zac_kenton (18 Aug 2022 17:33 UTC; 71 points)Ab­stract­ing The Hard­ness of Align­ment: Un­bounded Atomic Optimization by adamShimi (29 Jul 2022 18:59 UTC; 65 points)my cur­rent out­look on AI risk mitigation by Tamsin Leake (3 Oct 2022 20:06 UTC; 63 points)My take on Michael Littman on “The HCI of HAI” by Alex Flint (2 Apr 2021 19:51 UTC; 59 points)Vingean Agency by abramdemski (24 Aug 2022 20:08 UTC; 58 points)Re­view of ‘But ex­actly how com­plex and frag­ile?’ by TurnTrout (6 Jan 2021 18:39 UTC; 54 points)In­ter­pretabil­ity’s Align­ment-Solv­ing Po­ten­tial: Anal­y­sis of 7 Scenarios by Evan R. Murphy (12 May 2022 20:01 UTC; 53 points)Al­gorith­mic In­tent: A Han­so­nian Gen­er­al­ized Anti-Zom­bie Principle by Zack_M_Davis (14 Jul 2020 6:03 UTC; 50 points)AXRP Epi­sode 4 - Risks from Learned Op­ti­miza­tion with Evan Hubinger by DanielFilan (18 Feb 2021 0:03 UTC; 43 points)Mo­saic and Pal­impsests: Two Shapes of Research by adamShimi (12 Jul 2022 9:05 UTC; 39 points)Agency from a causal perspective by tom4everitt (30 Jun 2023 17:37 UTC; 38 points)[ASoT] Con­se­quen­tial­ist mod­els as a su­per­set of mesaoptimizers by leogao (23 Apr 2022 17:57 UTC; 36 points)Selec­tion pro­cesses for subagents by Ryan Kidd (30 Jun 2022 23:57 UTC; 36 points)TurnTrout's comment on But ex­actly how com­plex and frag­ile? by KatjaGrace (6 Jan 2021 18:40 UTC; 35 points)Epistemic Arte­facts of (con­cep­tual) AI al­ign­ment research by Nora_Ammann (19 Aug 2022 17:18 UTC; 30 points)Prob­lems fac­ing a cor­re­spon­dence the­ory of knowledge by Alex Flint (24 May 2021 16:02 UTC; 30 points)adamShimi's comment on Selec­tion vs Control by abramdemski (3 Jan 2021 17:34 UTC; 29 points)The ac­cu­mu­la­tion of knowl­edge: liter­a­ture review by Alex Flint (10 Jul 2021 18:36 UTC; 29 points)Com­pu­ta­tional sig­na­tures of psychopathy by Cameron Berg (19 Dec 2022 17:01 UTC; 28 points)[AN #157]: Mea­sur­ing mis­al­ign­ment in the tech­nol­ogy un­der­ly­ing Copilot by Rohin Shah (23 Jul 2021 17:20 UTC; 28 points)Bits of Op­ti­miza­tion Can Only Be Lost Over A Distance by johnswentworth (23 May 2022 18:55 UTC; 27 points)Bridg­ing Ex­pected Utility Max­i­miza­tion and Optimization by Whispermute (5 Aug 2022 8:18 UTC; 25 points)[AN #105]: The eco­nomic tra­jec­tory of hu­man­ity, and what we might mean by optimization by Rohin Shah (24 Jun 2020 17:30 UTC; 24 points)Epistemic Mo­tif of Ab­stract-Con­crete Cy­cles & Do­main Expansion by Dalcy (10 Oct 2023 3:28 UTC; 23 points)Ramana Kumar's comment on Ngo and Yud­kowsky on al­ign­ment difficulty by Eliezer Yudkowsky (19 Nov 2021 15:48 UTC; 22 points)Con­fu­sions in My Model of AI Risk by peterbarnett (7 Jul 2022 1:05 UTC; 22 points)pro­gram searches by Tamsin Leake (5 Sep 2022 20:04 UTC; 21 points)Pit­falls of the agent model by Alex Flint (27 Apr 2021 22:19 UTC; 20 points)Sun­day July 12 — talks by Scott Garrabrant, Alexflint, alexei, Stu­art_Armstrong by jacobjacob (8 Jul 2020 0:27 UTC; 19 points)Mo­ti­va­tions, Nat­u­ral Selec­tion, and Cur­ricu­lum Engineering by Oliver Sourbut (16 Dec 2021 1:07 UTC; 16 points)What sorts of sys­tems can be de­cep­tive? by Andrei Alexandru (31 Oct 2022 22:00 UTC; 16 points)[AN #164]: How well can lan­guage mod­els write code? by Rohin Shah (15 Sep 2021 17:20 UTC; 13 points)Sun­day Septem­ber 27, 12:00PM (PT) — talks by Alex Flint, Alex Zhu and more by habryka (22 Sep 2020 21:59 UTC; 11 points)johnswentworth's comment on I’m no longer sure that I buy dutch book ar­gu­ments and this makes me skep­ti­cal of the “util­ity func­tion” abstraction by Eli Tyre (22 Jun 2021 17:09 UTC; 11 points)evhub's comment on Risks from Learned Op­ti­miza­tion: Con­clu­sion and Re­lated Work by evhub (26 Jun 2020 19:51 UTC; 10 points)Ramana Kumar's comment on Ngo and Yud­kowsky on al­ign­ment difficulty by Eliezer Yudkowsky (23 Nov 2021 17:28 UTC; 10 points)Mark Xu's comment on Oper­a­tional­iz­ing com­pat­i­bil­ity with strat­egy-stealing by evhub (25 Dec 2020 17:14 UTC; 8 points)A new defi­ni­tion of “op­ti­mizer” by Chantiel (9 Aug 2021 13:42 UTC; 5 points)Gordon Seidoh Worley's comment on Selec­tion The­o­rems: A Pro­gram For Un­der­stand­ing Agents by johnswentworth (29 Sep 2021 14:44 UTC; 4 points)johnswentworth's comment on Gra­di­ent de­scent doesn’t se­lect for in­ner search by Ivan Vendrov (15 Aug 2022 5:41 UTC; 4 points)Rohin Shah's comment on Our take on CHAI’s re­search agenda in un­der 1500 words by Alex Flint (21 Jun 2020 20:28 UTC; 4 points)Dalcy's comment on Dalcy’s Shortform by Dalcy (11 Jan 2023 19:08 UTC; 3 points)RobertKirk's comment on Pos­i­tive val­ues seem more ro­bust and last­ing than prohibitions by TurnTrout (19 Dec 2022 15:46 UTC; 3 points)Alex_Altair's comment on My first year in AI alignment by Alex_Altair (3 Jan 2023 23:44 UTC; 3 points)lifelonglearner's comment on Defin­ing “op­ti­mizer” by Chantiel (17 Apr 2021 16:33 UTC; 2 points)Jan's comment on Ad­ver­sar­ial at­tacks and op­ti­mal control by Jan (29 May 2022 12:42 UTC; 2 points)Alex Flint's comment on Knowl­edge is not just map/​ter­ri­tory resemblance by Alex Flint (26 May 2021 16:41 UTC; 2 points)Edouard Harris's comment on Re-Define In­tent Align­ment? by abramdemski (4 Aug 2021 17:46 UTC; 1 point)DragonGod's comment on DragonGod’s Shortform by DragonGod (19 Dec 2022 21:44 UTC; 1 point)Alex Flint20 Jun 2020 0:38 UTCLW: 241 AF: 9275 commentsLW link1 reviewOptimizationAIWorld ModelingGeneral IntelligenceSelection vs ControlDynamical systemsBest of LessWrongPost permalinkLink without commentsLink without top nav barsLink without comments or top nav barsPart of the sequence:Align­ment & AgencyPrevious: An overview of 11 pro­pos­als for build­ing safe ad­vanced AINext: Search ver­sus designVanessa Kosoy 16 Dec 2021 14:21 UTC LW: 28 AF: 16AFIn this post, the au­thor pro­poses a semifor­mal defi­ni­tion of the con­cept of “op­ti­miza­tion”. This is po­ten­tially valuable since “op­ti­miza­tion” is a word of­ten used in dis­cus­sions about AI risk, and much con­fu­sion can fol­low from sloppy use of the term or from differ­ent peo­ple un­der­stand­ing it differ­ently. While the defi­ni­tion given here is a use­ful per­spec­tive, I have some reser­va­tions about the claims made about its rele­vance and ap­pli­ca­tions.
The key para­graph, which sum­ma­rizes the defi­ni­tion it­self, is the fol­low­ing:

An op­ti­miz­ing sys­tem is a sys­tem that has a ten­dency to evolve to­wards one of a set of con­figu­ra­tions that we will call the tar­get con­figu­ra­tion set, when started from any con­figu­ra­tion within a larger set of con­figu­ra­tions, which we call the basin of at­trac­tion, and con­tinues to ex­hibit this ten­dency with re­spect to the same tar­get con­figu­ra­tion set de­spite per­tur­ba­tions.

In fact, “con­tinues to ex­hibit this ten­dency with re­spect to the same tar­get con­figu­ra­tion set de­spite per­tur­ba­tions” is re­dun­dant: clearly as long as the per­tur­ba­tion doesn’t push the sys­tem out of the basin, the ten­dency must con­tinue.
This is what is known as “at­trac­tor” in dy­nam­i­cal sys­tems the­ory. For com­par­i­son, here is the defi­ni­tion of “at­trac­tor” from the Wikipe­dia:

In the math­e­mat­i­cal field of dy­nam­i­cal sys­tems, an at­trac­tor is a set of states to­ward which a sys­tem tends to evolve, for a wide va­ri­ety of start­ing con­di­tions of the sys­tem. Sys­tem val­ues that get close enough to the at­trac­tor val­ues re­main close even if slightly dis­turbed.

The au­thor ac­knowl­edges this con­nec­tion, al­though he also makes the fol­low­ing re­mark:

We have dis­cussed sys­tems that evolve to­wards tar­get con­figu­ra­tions along some di­men­sions but not oth­ers (e.g. ball in a valley). We have not yet dis­cov­ered whether dy­nam­i­cal sys­tems the­ory ex­plic­itly stud­ies at­trac­tors that op­er­ate along a sub­set of the sys­tem’s di­men­sions.

I find this re­mark con­fus­ing. An at­trac­tor that op­er­ates along a sub­set of the di­men­sion is just an at­trac­tor sub­man­i­fold. This is com­pletely stan­dard in dy­nam­i­cal sys­tems the­ory.
Given that the defi­ni­tion it­self is not es­pe­cially novel, the post’s main claim to value is via the ap­pli­ca­tions. Un­for­tu­nately, some of the pro­posed ap­pli­ca­tions seem to me poorly jus­tified. Speci­fi­cally, I want to talk about two ma­jor ex­am­ples: the claimed re­la­tion­ship to em­bed­ded agency and the claimed re­la­tions to com­pre­hen­sive AI ser­vices.
In both cases, the main short­com­ing of the defi­ni­tion is that there is an es­sen­tial prop­erty of AI that this defi­ni­tion doesn’t cap­ture at all. The au­thor does ac­knowl­edge that “goal-di­rected agent sys­tem” is a dis­tinct con­cept from “op­ti­miz­ing sys­tems”. How­ever, he doesn’t ex­plain how are they dis­tinct.
One way to for­mu­late the differ­ence is as fol­lows: agency = op­ti­miza­tion + learn­ing. An agent is not just ca­pa­ble of steer­ing a par­tic­u­lar uni­verse to­wards a cer­tain out­come, it is ca­pa­ble of steer­ing an en­tire class of uni­verses, with­out know­ing in ad­vance in which uni­verse it was placed. This un­der­lies all of RL the­ory, this is im­plicit in the Shane-Legg defi­ni­tion of in­tel­li­gence and my own[1], this is what Yud­kowsky calls “cross do­main”.
The is­sue of learn­ing is not just nit­pick­ing, it is cru­cial to delineate the bound­ary around “AI risk”, and delineat­ing the bound­ary is cru­cial to con­struc­tively think of solu­tions. If we ig­nore learn­ing and just talk about “op­ti­miza­tion risks” then we will have to in­clude the risk of pan­demics (be­cause bac­te­ria are op­ti­miz­ing for in­fec­tion), the risk of false vac­uum col­lapse in par­ti­cle ac­cel­er­a­tors (be­cause vac­uum bub­bles are op­ti­miz­ing for ex­pand­ing), the risk of run­away global warm­ing (be­cause it is op­ti­miz­ing for in­creas­ing tem­per­a­ture) et cetera. But, these are very differ­ent risks that re­quire very differ­ent solu­tions.
There is an­other, less cen­tral, differ­ence: the au­thor re­quires a par­tic­u­lar set of “tar­get states” whereas in the con­text of agency it is more nat­u­ral to con­sider util­ity func­tions, which means there is a con­tin­u­ous gra­da­tion of states rather than just “good states” and “bad states”. This is re­lated to the differ­ence the au­thor points out be­tween his defi­ni­tion and Yud­kowsky’s:

When dis­cern­ing the bound­ary be­tween op­ti­miza­tion and non-op­ti­miza­tion, we look prin­ci­pally at ro­bust­ness — whether the sys­tem will con­tinue to evolve to­wards its tar­get con­figu­ra­tion set in the face of per­tur­ba­tions — whereas Yud­kowsky looks at the im­prob­a­bil­ity of the fi­nal con­figu­ra­tion.

The im­prob­a­bil­ity of the fi­nal con­figu­ra­tion is a con­tin­u­ous met­ric, whereas just ar­riv­ing or not ar­riv­ing at a par­tic­u­lar set is dis­crete.
Let’s see how this short­com­ing af­fects the con­clu­sions. About em­bed­ded agency, the au­thor writes:

One could view the Embed­ded Agency work as enu­mer­at­ing the many log­i­cal pit­falls one falls into if one takes the “op­ti­mizer” con­cept as the start­ing point for de­sign­ing in­tel­li­gent sys­tems, rather than “op­ti­miz­ing sys­tem” as we pro­pose here.

The cor­rect start­ing point is “agent”, defined in the way I ges­tured at above. If in­stead we start with “op­ti­miz­ing sys­tem” then we throw away the baby with the bath­wa­ter, since the cru­cial as­pect of learn­ing is ig­nored. This is an es­sen­tial prop­erty of the em­bed­ded agency prob­lem: ar­guably the en­tire difficulty is about how can we define learn­ing with­out in­tro­duc­ing un­phys­i­cal du­al­ism (in­deed, I have re­cently ad­dressed this prob­lem, and “op­ti­miz­ing sys­tem” doesn’t seem very helpful there).
About com­pre­hen­sive AI ser­vices:

Our per­spec­tive is that there is a spe­cific class of in­tel­li­gent sys­tems — which we call op­ti­miz­ing sys­tems — that are wor­thy of spe­cial at­ten­tion and study due to their po­ten­tial to re­shape the world. The set of op­ti­miz­ing sys­tems is smaller than the set of all AI ser­vices, but larger than the set of goal-di­rected agen­tic sys­tems.

What is an ex­am­ple of an op­ti­miz­ing AI sys­tem that is not agen­tic? The au­thor doesn’t give such an ex­am­ple and in­stead talks about trees, which are not AIs. I agree that the class of dan­ger­ous sys­tems is sub­stan­tially wider than the class of sys­tems which were ex­plic­itly de­signed with agency in mind. How­ever, this is pre­cisely be­cause agency can arise from such sys­tems even when not ex­plic­itly de­signed, and more­over this is hard to avoid if the sys­tem is to be pow­er­ful enough for pivotal acts. This is not be­cause there is some class of “op­ti­miz­ing AI sys­tems” which are in­ter­me­di­ate be­tween “agen­tic” and “non-agen­tic”.
To sum­ma­rize, I agree with and en­courage the use of tools from dy­nam­i­cal sys­tems the­ory to study AI. How­ever, one must ac­knowl­edge to cor­rect scope of these tools and what they don’t do. More­over, more work is needed be­fore truly novel con­clu­sions can be ob­tained by these means.


↩︎Mo­dulo is­sues with traps which I will not go into atm. 

What links here?Vaniver's comment on 2020 Re­view: The Dis­cus­sion Phase by Vaniver (23 Dec 2021 3:11 UTC; 5 points)Rohin Shah 21 Jun 2020 20:03 UTC LW: 59 AF: 28AFThis is ex­cel­lent, it feels way bet­ter as a defi­ni­tion of op­ti­miza­tion than past at­tempts :) Thanks in par­tic­u­lar for the aca­demic style, speci­fi­cally re­lat­ing it to pre­vi­ous work, it made it much more ac­cessible for me.Let’s try to build up some core AI al­ign­ment ar­gu­ments with this defi­ni­tion.Task: A task is sim­ply an “en­vi­ron­ment” along with a tar­get con­figu­ra­tion set. When­ever I talk about a “task” be­low, as­sume that I mean an “in­ter­est­ing” task, i.e. some­thing like “build a chair”, as op­posed to “have the air molecules be in one of these par­tic­u­lar con­figu­ra­tions”.Solv­ing a task: An ob­ject O solves a task T if adding O to T’s en­vi­ron­ment trans­forms it into an op­ti­miz­ing sys­tem for the T’s tar­get con­figu­ra­tion set.Perfor­mance on the task: If O solves task T, its perfor­mance is quan­tified by how quickly it reaches the tar­get con­figu­ra­tion set, and how ro­bust it is to per­tur­ba­tions.Gen­er­al­ity of in­tel­li­gence: The gen­er­al­ity of O’s in­tel­li­gence is a func­tion of the num­ber and di­ver­sity of tasks T that it can solve, as well as its perfor­mance on those tasks.Op­ti­miz­ing AI: A com­puter pro­gram for which there ex­ists an in­ter­est­ing task such that the com­puter pro­gram solves that task.This isn’t ex­actly right, as it in­cludes e.g. ac­count­ing pro­grams or video games, which when paired with a hu­man form an op­ti­miz­ing sys­tem for cor­rect fi­nan­cials and win­ning the game, re­spec­tively. You might be able to fix this by say­ing that the op­ti­miz­ing sys­tem has to be ro­bust to per­tur­ba­tions in any hu­man be­hav­ior in the en­vi­ron­ment.AGI: An op­ti­miz­ing AI whose gen­er­al­ity of in­tel­li­gence is at least as great as that of hu­mans.Ar­gu­ment for AI risk: As op­ti­miz­ing AIs be­come more and more gen­eral, we will ap­ply them to more eco­nom­i­cally use­ful tasks T. How­ever, they also be­come more and more ro­bust to per­tur­ba­tions, pos­si­bly in­clud­ing per­tur­ba­tions such as “we try to turn off the AI”. As a re­sult, we might even­tu­ally have AIs that form strong op­ti­miz­ing sys­tems for some task T that isn’t the one we ac­tu­ally wanted, which tends to be bad due to frag­ility of value.Deep learn­ing AGI im­plies mesa op­ti­miza­tion: Since deep learn­ing is so sam­ple in­effi­cient, it can­not reach hu­man lev­els of perfor­mance if we ap­ply deep learn­ing di­rectly to each pos­si­ble task T. (For ex­am­ple, it has to re­learn how the world works sep­a­rately for each task T.) As a re­sult, if we do get AGI pri­mar­ily via deep learn­ing, it must be that we used deep learn­ing to cre­ate a new op­ti­miz­ing AI sys­tem, and that sys­tem was the AGI.Ar­gu­ment for mesa op­ti­miza­tion: Due to the com­plex­ity and noise in the real world, most eco­nom­i­cally use­ful tasks re­quire set­ting up a ro­bust op­ti­miz­ing sys­tem, rather than di­rectly cre­at­ing the tar­get con­figu­ra­tion state. (See also the im­por­tance of feed­back for more on this in­tu­ition.) It seems likely that hu­mans will find it eas­ier to cre­ate al­gorithms that then find AGIs that can cre­ate these ro­bust op­ti­miz­ing sys­tems, rather than cre­at­ing an al­gorithm that is di­rectly an AGI.(The pre­vi­ous ar­gu­ment also ap­plies: this is ba­si­cally just a gen­er­al­iza­tion of the pre­vi­ous point to ar­bi­trary AI sys­tems, in­stead of only deep learn­ing.)I want to note that un­der this ap­proach the no­tion of “search” and “mesa ob­jec­tive” are less nat­u­ral, which I see as a pro of this ap­proach (see also here): the ar­gu­ment is that we’ll get a gen­eral in­ner op­ti­miz­ing AI, but it doesn’t say much about what task that AI will be op­ti­miz­ing for (and it could be an op­ti­miz­ing AI that is re­tar­getable by hu­man in­struc­tions).Outer al­ign­ment: ??? Seems hard to for­mal­ize in this frame­work. This makes me feel like outer al­ign­ment is less im­por­tant as a con­cept. (I also don’t par­tic­u­larly like for­mal­iza­tions out­side of this frame­work.)In­ner al­ign­ment: En­sur­ing that (con­di­tional on mesa op­ti­miza­tion oc­cur­ring) the in­ner AGI is al­igned with the op­er­a­tor /​ user, that is, com­bined with the user it forms an op­ti­miz­ing sys­tem for “do­ing what the user wants”. (Note that this is ex­plic­itly not in­tent al­ign­ment, as it is hard to for­mal­ize in­tent al­ign­ment in this frame­work.)In­tent al­ign­ment: ??? As men­tioned above, it’s hard to for­mal­ize in this frame­work, as in­tent al­ign­ment re­ally does re­quire some no­tion of “mo­ti­va­tion”, “goals”, or “try­ing”, which this frame­work ex­plic­itly leaves out. I see this as a con of this frame­work.Ex­pected util­ity max­i­miza­tion: One par­tic­u­lar ar­chi­tec­ture that could qual­ify as an AGI (if the util­ity func­tion is treated as part of the en­vi­ron­ment, and not part of the AGI). I see the fact that EU max­i­miza­tion is no longer high­lighted as a pro of this ap­proach.Wire­head­ing: Spe­cial case of the ar­gu­ment for AI risk with a weird task of “max­i­mize the num­ber in this reg­ister”. Un­nat­u­ral in this fram­ing of the AI risk prob­lem. I see this as a pro of this fram­ing of the prob­lem, though I ex­pect peo­ple dis­agree with me on this point.What links here?In­ter­pretabil­ity’s Align­ment-Solv­ing Po­ten­tial: Anal­y­sis of 7 Scenarios by Evan R. Murphy (12 May 2022 20:01 UTC; 53 points)Re-Define In­tent Align­ment? by abramdemski (22 Jul 2021 19:00 UTC; 28 points)[AN #105]: The eco­nomic tra­jec­tory of hu­man­ity, and what we might mean by optimization by Rohin Shah (24 Jun 2020 17:30 UTC; 24 points)Rohin Shah's comment on The ground of optimization by Alex Flint (21 Jun 2020 20:06 UTC; 7 points)Evan R. Murphy's comment on Re-Define In­tent Align­ment? by abramdemski (9 Apr 2022 0:20 UTC; 3 points)Alex Flint 29 Jun 2020 19:26 UTC LW: 8 AF: 6AFParentThanks for the very thought­ful com­ment Ro­hin. I was on re­treat last week af­ter I pub­lished the ar­ti­cle and upon re­turn­ing to com­puter us­age I was delighted by the en­gage­ment from you and oth­ers.

Gen­er­al­ity of in­tel­li­gence: The gen­er­al­ity of O’s in­tel­li­gence is a func­tion of the num­ber and di­ver­sity of tasks T that it can solve, as well as its perfor­mance on those tasks.

I like this.
We’ll pre­sum­ably need to give O some in­for­ma­tion about the goal /​ tar­get con­figu­ra­tion set for each task. We could say that a robot ca­pa­ble of mov­ing a vase around is a lit­tle bit gen­eral since we can have it solve the tasks of plac­ing the vase at many differ­ent lo­ca­tions by in­putting some lat­i­tude/​lon­gi­tude into some ap­pro­pri­ate mem­ory lo­ca­tion. But this means we’re ac­tu­ally past­ing in a differ­ent ob­ject O for each task T—each of the ob­jects differs in those mem­ory lo­ca­tions into which we’re past­ing the lat­i­tude/​lon­gi­tude. It might be helpful to think of a “agent schema” func­tion that maps goals to ob­jects, so we take the goal part of the task, com­pute the ob­ject O for that goal, then paste this ob­ject into the en­vi­ron­ment.
It’s also im­por­tant that O be able to solve the task for a rea­son­ably broad range of en­vi­ron­ments.

In­ner alignment

Per­haps we could look at it this way: take a sys­tem con­tain­ing a hu­man that is try­ing to get some­thing done. This is pre­sum­ably an op­ti­miz­ing sys­tem as hu­mans of­ten ro­bustly move their en­vi­ron­ment to­wards some de­sired tar­get con­figu­ra­tion set. Then an in­ner-al­igned AI is an ob­ject O such that adding it to this en­vi­ron­ment does not change the tar­get con­figu­ra­tion set, but does change the speed and/​or ro­bust­ness of con­ver­gence to that tar­get con­figu­ra­tion set.

In­tent alignment

Yup very difficult to say much about in­ten­tions us­ing the pure out­side view ap­proach of this frame­work. Per­haps we could say that an in­tent-al­igned AI is an in­ner-al­igned AI mod­ulo less ro­bust­ness. Or per­haps we could say that an in­tent-al­igned AI is an AI that would achieve the goal in a large set of be­nign en­vi­ron­ments, but might not achieve it in the pres­ence of un­likely mis­takes, un­likely en­vi­ron­men­tal con­di­tions, or the pres­ence of other pow­er­ful bas­ins of at­trac­tion.
But this doesn’t re­ally get at the spirit of Paul’s idea, which I think is about re­ally look­ing in­side the AI and un­der­stand­ing its goals.
Rohin Shah 29 Jun 2020 21:23 UTC LW: 4 AF: 3AFParent+1 to all of this.We’ll pre­sum­ably need to give O some in­for­ma­tion about the goal /​ tar­get con­figu­ra­tion set for each task. I was imag­in­ing that the tasks can come equipped with some speci­fi­ca­tion, but some sort of coun­ter­fac­tual also makes sense. This also gets around is­sues of the AI sys­tem not be­ing ap­pro­pri­ately “mo­ti­vated”—e.g. I might be ca­pa­ble of perform­ing the task “lock up pup­pies in cages”, but I wouldn’t do it, and so if you only look at my be­hav­ior you couldn’t say that I was ca­pa­ble of do­ing that task.But this doesn’t re­ally get at the spirit of Paul’s idea, which I think is about re­ally look­ing in­side the AI and un­der­stand­ing its goals.+1 es­pe­cially to thisTurnTrout 22 Jun 2020 23:43 UTC LW: 5 AF: 3AFParentMild op­ti­miza­tion: the eas­iest way to solve hard tasks may be to spec­ify a proxy, which an AI max­i­mizes. The AI steers into con­figu­ra­tions which max­i­mize the proxy func­tion. Sim­ple prox­ies don’t usu­ally have tar­get sets which we like, be­cause hu­man value is com­plex. How­ever, maybe we just want the AI to ran­domly se­lect a con­figu­ra­tion which satis­fies the proxy, in­stead of find­ing the max­i­mally-proxy-ness con­figu­ra­tion, which may be bad due to ex­tremal Good­hart. Quan­tiliza­tion tries to solve this by ran­domly se­lect­ing a tar­get con­figu­ra­tion from some top quan­tile, but this is sen­si­tive to how world states are in­di­vi­d­u­ated. What links here?Rohin Shah's comment on The ground of optimization by Alex Flint (23 Jun 2020 18:55 UTC; 8 points)Rohin Shah 23 Jun 2020 17:54 UTC LW: 8 AF: 5AFParentThis makes sense, but I think you’d need a differ­ent no­tion of op­ti­miz­ing sys­tems than the one used in this post. (In par­tic­u­lar, in­stead of a tar­get con­figu­ra­tion set, you want a con­tin­u­ous no­tion of good­ness, like a util­ity func­tion /​ re­ward func­tion.)TurnTrout 23 Jun 2020 18:15 UTC LW: 2 AF: 1AFParentI’m say­ing the tar­get set for non-mild op­ti­miza­tion is the set of con­figu­ra­tions which max­i­mize proxy-ness. Just take the argmax. By con­trast, we might want to sam­ple uniformly ran­domly from the set of satis­fic­ing con­figu­ra­tions, which is much larger. (This is as­sum­ing a fixed ini­tial state)Rohin Shah 23 Jun 2020 18:49 UTC LW: 4 AF: 3AFParentIt sounds like you’re as­sum­ing that the tar­get con­figu­ra­tion set is built into the AI sys­tem. Ac­cord­ing to me, a ma­jor point of this post /​ frame­work is to avoid that as­sump­tion al­to­gether, and only de­scribe prob­lems in terms of the ac­tual ob­served sys­tem be­hav­ior.(This is why within this frame­work I couldn’t for­mal­ize outer al­ign­ment, and why wire­head­ing and the search /​ mesa-ob­jec­tive split is un­nat­u­ral.)TurnTrout 23 Jun 2020 19:48 UTC LW: 4 AF: 3AFParentI see the ten­sion you’re point­ing at. I think I had in mind some­thing like “an AI is re­li­ably op­ti­miz­ing util­ity func­tion u over the con­figu­ra­tion space (but not nec­es­sar­ily over uni­verse-his­to­ries!) if it re­li­ably moves into high-rated con­figu­ra­tions”, and you could draw differ­ent ep­silon-neigh­bor­hoods of op­ti­mal­ity in con­figu­ra­tion space. It seems like you should be able to talk about dog-max­i­miz­ers with­out re­quiring that the agent ro­bustly end up in the max­i­mum-dog con­figu­ra­tions (and not in max-minus-one-dog con­figs). I’m still con­fused about parts of this.ESRogs 30 Jun 2020 23:59 UTC LW: 4 AF: 3AFParentDeep learn­ing AGI im­plies mesa op­ti­miza­tion: Since deep learn­ing is so sam­ple in­effi­cient, it can­not reach hu­man lev­els of perfor­mance if we ap­ply deep learn­ing di­rectly to each pos­si­ble task T. (For ex­am­ple, it has to re­learn how the world works sep­a­rately for each task T.) As a re­sult, if we do get AGI pri­mar­ily via deep learn­ing, it must be that we used deep learn­ing to cre­ate a new op­ti­miz­ing AI sys­tem, and that sys­tem was the AGI.I don’t quite un­der­stand what this is say­ing.Sup­pose we train a gi­ant deep learn­ing model via self-su­per­vised learn­ing on a ton of real-world data (like GPT-N, but w/​ other sen­sory modal­ities be­sides text), and then we build a sec­ond sys­tem de­signed to provide a nice in­ter­face to the gi­ant model.We’d give task speci­fi­ca­tions to the in­ter­face, and it would have some smarts about how to con­sult the model to figure out what to do. (The in­ter­face might also be learned, via re­in­force­ment or su­per­vised learn­ing, or it might be hand-coded.)It seems plau­si­ble to me that a sys­tem com­pris­ing these two pieces, the model and the in­ter­face, could be an AGI ac­cord­ing to the defi­ni­tion here, in that when com­bined with a very wide va­ri­ety of en­vi­ron­ments (in­clud­ing the task speci­fi­ca­tion in the en­vi­ron­ment), it could perform at least as well as a hu­man.And since most of the smarts seem like they’d be in the model rather than the in­ter­face, I’d count it as get­ting AGI “pri­mar­ily via deep learn­ing”, even if the in­ter­face was hand-coded.But it’s not clear to me whether that would count as us­ing deep learn­ing to “cre­ate a new op­ti­miz­ing AI sys­tem”, which is it­self the AGI. The whole sys­tem is an Op­ti­miz­ing AI, ac­cord­ing to the defi­ni­tion given above, but nei­ther of the two parts is by it­self, and it doesn’t seem to have the fla­vor of mesa-op­ti­miza­tion, as I un­der­stand it. So it seems like a con­tra­dic­tion to the quoted claim.Have I mi­s­un­der­stood what you’re say­ing here, or do you dis­agree with the char­ac­ter­i­za­tion I gave of the hy­po­thet­i­cal model + in­ter­face sys­tem? (Or have I per­haps mi­s­un­der­stood mesa-op­ti­miza­tion?)Rohin Shah 1 Jul 2020 17:34 UTC LW: 5 AF: 4AFParentThe whole sys­tem is an Op­ti­miz­ing AI, ac­cord­ing to the defi­ni­tion given above, but nei­ther of the two parts is by itselfYeah, I’m talk­ing about the whole sys­tem.it doesn’t seem to have the fla­vor of mesa-optimizationYeah, I agree it doesn’t fit the ex­pla­na­tion /​ defi­ni­tion in Risks from Learned Op­ti­miza­tion. I don’t like that defi­ni­tion, and usu­ally mean some­thing like “run­ning the model pa­ram­e­ters in­stan­ti­ates a com­pu­ta­tion that does ‘rea­son­ing’”, which I think does fit this ex­am­ple. I men­tioned this a bit later in the com­ment:I want to note that un­der this ap­proach the no­tion of “search” and “mesa ob­jec­tive” are less nat­u­ral, which I see as a pro of this ap­proach [...]: the ar­gu­ment is that we’ll get a gen­eral in­ner op­ti­miz­ing AI, but it doesn’t say much about what task that AI will be op­ti­miz­ing for (and it could be an op­ti­miz­ing AI that is re­tar­getable by hu­man in­struc­tions).matthewp 31 Mar 2022 13:09 UTC 1 pointParentThanks for the ad­di­tions here. I’m also un­sure how to gel this defi­ni­tion (which I quite like) with the in­ner/​outer/​mesa ter­minol­ogy. Here is my knuckle drag­ging model of the post’s im­pli­ca­tion:
target_set = f(env, agent)
So if we plug in a bunch of val­ues for agent and hope for the best, the target_set we get might might not be what we de­sired. This would be mis­al­ign­ment. Whereas the al­ign­ment task is more like to fix target_set and env and solve for agent.
The stuff about mesa op­ti­misers mainly sounds like in­ad­e­quate (nar­row) mod­el­ling of what env, agent and target_set are. Usu­ally fix­at­ing on some frac­tion of the prob­lem (win the bat­tle, lose the war prob­lem).Aryeh Englander 1 Jul 2020 15:46 UTC LW: 32 AF: 12AFI shared this es­say with a col­league where I work (Johns Hop­kins Univer­sity Ap­plied Physics Lab). Here are her com­ments, which she asked me to share:This es­say pro­poses a very in­ter­est­ing defi­ni­tion of op­ti­miza­tion as the man­i­fes­ta­tion of a par­tic­u­lar be­hav­ior of a closed, phys­i­cal sys­tem. I haven’t finished think­ing this over, but I sus­pect it will be (as is sug­gested in the es­say) a use­ful con­struct. The rea­son­ing lead­ing to the defi­ni­tion is clearly laid out (thank you!), with ex­am­ples that are very use­ful in un­der­stand­ing the con­cept. The down­side of be­ing clearly laid out, how­ever, is that it makes cri­tique eas­ier. I have a few thoughts about the rea­son­ing in the es­say.The first thing I will note is that the es­say gives three defi­ni­tions for an op­ti­miz­ing sys­tem. Th­ese defi­ni­tions are close, but not ex­actly equiv­a­lent. The nu­ances can be im­por­tant. For ex­am­ple, that the tar­get con­figu­ra­tion set and the basin of at­trac­tion can­not be equal is ob­vi­ous; that is made ex­plicit in defi­ni­tion 3, but only im­plied in defi­ni­tions 1 and 2. A big­ger is­sue is that there are no crite­ria or ra­tio­nale for their ex­tent and rel­a­tive size. For ex­am­ple, the es­say offers two rea­sons why the poster­child of non-op­ti­miz­ers—the bot­tle with a cap—is not an op­ti­miz­ing sys­tem; they both arise from the rather ar­bi­trary defi­ni­tion of the basin of at­trac­tion as equal to the tar­get con­figu­ra­tion set. I see no nec­es­sary rea­son why the basin of at­trac­tion couldn’t be defined as the set of all con­figu­ra­tions of wa­ter molecules both in­side and out­side the bot­tle. That way, the defi­ni­tional re­quire­ment of a tar­get con­figu­ra­tion set smaller than the ba­sis of at­trac­tion is met. The im­por­tant point is: will wa­ter molecules in this new, larger basin of at­trac­tion tend to the tar­get con­figu­ra­tion set? Let’s sup­pose that capped bot­tle is in a sealed room (not nec­es­sary but eas­ier to think about), and that the cap is made of a spe­cial ma­te­rial that al­lows wa­ter molecules to pass through it in only one di­rec­tion: from out­side the bot­tle to in­side. The wa­ter molecules in­side the bot­tle stay in­side the bot­tle, as for any cap. The wa­ter molecules in­side the room, but out­side the bot­tle, are zoom­ing about (ther­mo­dy­namic en­ergy), bounc­ing off the walls, each other, and the bot­tle. Although it will take some time, sooner or later all the molecules out­side the bot­tle will hit the bot­tle cap, go through, and be trapped in the bot­tle. Voila! Origi­nally, the bot­tle-with-a-cap sys­tem was a non-op­ti­miz­ing sys­tem by defi­ni­tion; the bot­tle cap type was ir­rele­vant and could have been the rather spe­cial one I de­scribed. Sim­ply by chang­ing the defi­ni­tion of the basin of at­trac­tion, we could turn it into an op­ti­miz­ing sys­tem. Fur­ther, the origi­nal, “non-op­ti­miz­ing” sys­tem (with the origi­nal defi­ni­tions of the basin of at­trac­tion and tar­get set) would have be­haved ex­actly the same as my op­ti­miz­ing sys­tem. On the other hand, chang­ing the bot­tle cap from our spe­cial one to a reg­u­lar cap will change the sys­tem into a non-op­ti­miz­ing sys­tem, re­gard­less of the defi­ni­tions of the basin of at­trac­tion and the tar­get con­figu­ra­tion set. Per­haps, we should in­sist that a prop­erly formed sys­tem de­scrip­tion has a basin of at­trac­tion that is larger than the tar­get set, and count on the sys­tem be­hav­ior to make the op­ti­miz­ing/​non-op­ti­miz­ing dis­tinc­tion.Defi­ni­tions 1 and 2 both con­tain the phrase “a small set of tar­get con­figu­ra­tions” which im­plies that the tar­get set << than the basin of at­trac­tion. This is a prob­lem for the no­tion of the uni­verse as a sys­tem with max­i­mum en­tropy as the tar­get con­figu­ra­tion set be­cause the tar­get set is most of the pos­si­ble con­figu­ra­tions. For this rea­son, the es­say’s au­thor con­cludes that uni­verse-with-en­tropy sys­tem is not an op­ti­miz­ing sys­tem, or at best, a weak one. Stars, galax­ies, black holes – there are strong forces that pull mat­ter into these struc­tures. I would say that any sys­tem that has suc­ceeded in get­ting nearly ev­ery­thing within the basin of at­trac­tion into the tar­get con­figu­ra­tion is a strong op­ti­mizer! Re­gard­less of the way we chose to think about strong or weak, the uni­verse is a sys­tem that tends to a set of con­figu­ra­tions smaller than the set of pos­si­ble con­figu­ra­tions de­spite per­tur­ba­tions (the oc­ca­sional house-build­ing pro­ject for ex­am­ple!). Per­son­ally, I see no value in a defi­ni­tional limi­ta­tion. The be­hav­ior of the sys­tem (tend­ing to­ward a smaller set of con­figu­ra­tions out of a larger set) should gov­ern the defi­ni­tion of an op­ti­miz­ing sys­tem, re­gard­less of rel­a­tive sizes of the sets.Between the uni­verse-with-en­tropy and bot­tle-with-a-cap sys­tems, I ques­tion the util­ity of the “all con­figu­ra­tions >= basin of at­trac­tion >> tar­get set con­figu­ra­tion” struc­ture in the defi­ni­tion of op­ti­miz­ing sys­tems. I be­lieve it is worth think­ing about what the nec­es­sary re­la­tion­ships among these con­figu­ra­tions are, and how they are cho­sen.The ex­am­ple of the billiards sys­tem raised an­other (to me) in­ter­est­ing ques­tion. The es­say did not offer a sys­tem de­scrip­tion but says “Con­sider a billiard table with some billiard balls that are cur­rently bounc­ing around in mo­tion. Left alone, the balls will even­tu­ally come to rest in some con­figu­ra­tion…. If we reach in while the billiard balls are bounc­ing around and move one of the balls that is in mo­tion, the sys­tem will now come to rest in a differ­ent con­figu­ra­tion. There­fore this is not an op­ti­miz­ing sys­tem, be­cause there is no set of tar­get con­figu­ra­tions to­wards which the sys­tem evolves de­spite per­tur­ba­tions.” This ex­am­ple has some odd fea­tures. Fric­tion be­tween the balls and the table sur­face, along with the loss of en­ergy dur­ing non-elas­tic col­li­sions, cause the balls to slow down and stop. The minu­tia of their trav­els de­ter­mines where they stop. The fi­nal ar­range­ment is un­pre­dictable (ok, it could be mod­eled given com­plete in­for­ma­tion, but let’s skip that as beside the point), and any ar­range­ment is as likely as an­other. This sug­gests that the billiards sys­tem is a non-op­ti­miz­ing sys­tem even with­out the pro­posed per­tur­ba­tion of mov­ing the balls around while the balls are in mo­tion. Looked at an­other way, billiards sys­tem does tend to a cer­tain tar­get con­figu­ra­tion set, while fric­tion and the non-elas­tic­ity of the col­li­sions are per­tur­ba­tions. If we make the sur­face fric­tion­less and the col­li­sions perfectly elas­tic, the balls will bounce around the table with­out stop­ping. Much like the wa­ter molecules in the bot­tle-with-a-cap ex­am­ple, each will even­tu­ally fall into one pocket or an­other dur­ing its trav­els. Once in the pocket, the ball can­not get out, and thus even­tu­ally all will end up in the pock­ets. So, this sys­tem tends to a tar­get con­figu­ra­tion set of all balls in pock­ets. Ad­ding back in the per­turb­ing fric­tion and en­ergy loss does not mean that this sys­tem is not tend­ing to the tar­get con­figu­ra­tion set. Reach­ing in and mov­ing a ball to a differ­ent point, or even redi­rect­ing any ball head­ing for a pocket, will not keep this sys­tem from tend­ing to­wards the tar­get con­figu­ra­tion. It seems as though the billiards sys­tem was an op­ti­miz­ing sys­tem all along! The larger point is that it seems, by defi­ni­tion, an op­ti­miz­ing sys­tem is an op­ti­miz­ing sys­tem even if there are a set of per­tur­ba­tions that pre­vent it from ever reach­ing the tar­get con­figu­ra­tion! “Tend­ing to­ward”, not “reach­ing”, a tar­get con­figu­ra­tion set is in all three defi­ni­tions. It is worth think­ing about an op­ti­miz­ing sys­tem that never ac­tu­ally op­ti­mizes. This may have some bear­ing on the AGI ques­tion.[And for you read­ers who, like me, would say, whoa—it is pos­si­ble that the balls will en­ter some re­peat­ing pat­tern of mo­tion where some do not en­ter pock­ets. Maybe we need a robot to move the balls around ran­domly if they seem stuck, just like the ball-in-valley+robot sys­tem where the robot moves the ball over bar­ri­ers. I main­tain that the point is the same.]The satel­lite sys­tem illus­trates (per­haps an ob­vi­ous point) that the defi­ni­tion of the tar­get con­figu­ra­tion set can change a sin­gle sys­tem from op­ti­miz­ing and to non-op­ti­miz­ing. What is a lit­tle more sub­tle is that the defi­ni­tion of the sys­tem bound­aries is es­sen­tial to the char­ac­ter­i­za­tion of the sys­tem as op­ti­miz­ing or non-op­ti­miz­ing, even if the be­hav­ior of the sys­tem is the same un­der both defi­ni­tions. In par­tic­u­lar, what we con­sider to be part of the sys­tem and what is con­sid­ered to be a per­tur­ba­tion can flip a sys­tem be­tween char­ac­ter­i­za­tions. [This lat­ter point is illus­trated by the billiards sys­tem as well, as I will ex­plain be­low.] The es­say says that a satel­lite in or­bit is a non-op­ti­miz­ing sys­tem be­cause if its po­si­tion or ve­loc­ity is per­turbed, it has no ten­dency to re­turn to its origi­nal or­bit; that is, the au­thor defines the tar­get con­figu­ra­tion as a par­tic­u­lar or­bit. With re­spect to an­other tar­get con­figu­ra­tion that may be de­scribed as “a scorched pile of junk on the sur­face of the Earth”, a satel­lite in or­bit is an op­ti­miz­ing sys­tem ex­actly like a ball in a valley. As soon as the launch rocket stops firing, a satel­lite starts fal­ling to the cen­ter of the earth be­cause at­mo­spheric drag and so­lar ra­di­a­tion pres­sure con­tin­u­ously de­crease the com­po­nent of the satel­lite’s ve­loc­ity per­pen­dicu­lar to the force of grav­ity. So, un­less a per­tur­ba­tion is big enough to send it out of or­bit al­to­gether, a satel­lite tends to­wards a tar­get con­figu­ra­tion of junk lo­cated on Earth’s sur­face.Since a par­tic­u­lar or­bit is usu­ally the de­sired tar­get con­figu­ra­tion (!), many satel­lites in­cor­po­rate a rocket sys­tem to force them to stay in a cho­sen or­bit. If a rocket sys­tem is in­cluded in the sys­tem defi­ni­tion, then the satel­lite is an op­ti­miz­ing sys­tem rel­a­tive to the de­sired or­bit. What is a lit­tle more in­ter­est­ing, with re­spect to the junk-on-the-Earth tar­get set, drag and so­lar pres­sure are the part of the op­ti­miz­ing sys­tem; an or­bit cor­rec­tion sys­tem is a per­tur­ba­tion. If the tar­get set is the par­tic­u­lar or­bit the satel­lite started in, these defi­ni­tions swap. This ob­ser­va­tion has bear­ing on the billiards sys­tem ex­am­ple. If we in­clude drag and non-elas­tic col­li­sions as part of the billiards sys­tem, then the sys­tem is non-op­ti­miz­ing. If we see them as per­tur­ba­tions out­side the sys­tem, then the billiards sys­tem is op­ti­miz­ing. I find this flex­i­bil­ity as a lit­tle cu­ri­ous, al­though I haven’t com­pletely thought through the im­pli­ca­tions.A com­pletely differ­ent sort of ques­tion is sug­gested by the sec­tion on Drexler. There the es­say sets out a hi­er­ar­chy of all AI sys­tems, op­ti­miz­ing sys­tems, and goal-di­rected agent sys­tems. This makes sense with re­spect to AI sys­tems, but I do not see how op­ti­miz­ing sys­tems, as defined, can be wholly con­tained within the cat­e­gory of AI sys­tems, un­less you define AI sys­tems pretty broadly. For ex­am­ple, I think that pretty much any con­trol sys­tem is an op­ti­miz­ing sys­tem by the defi­ni­tion in the es­say. If we ac­cept this defi­ni­tion of op­ti­miz­ing sys­tem, and hold that all op­ti­miz­ing sys­tems are a sub­set of AI sys­tems, do we have to ac­cept our ther­mostats as AI sys­tems? What about the pro­gram that de­ter­mined the square root of 2? Is that AI? Is this an is­sue for this defi­ni­tion, or does its broad­ness mat­ter in an AI con­text?And a nit­pick: The first ex­am­ple of an op­ti­miz­ing sys­tem offered in the es­say is a pro­gram calcu­lat­ing the square root of 2. It meets the defi­ni­tion of an op­ti­miz­ing sys­tem, but it seems to con­tra­dict the ear­lier as­ser­tion that “… op­ti­miz­ing sys­tems are not some­thing that are de­signed but are dis­cov­ered.” The al­gorithm and the pro­gram were both de­signed. I’m not sure why this point is nec­es­sary. Either I do not un­der­stand some­thing fun­da­men­tal, or the only pur­pose of the state­ment of dis­cov­ery is to give peo­ple like me some­thing to ar­gue about!In sum­mary, the defi­ni­tion in the es­say sug­gests a few ques­tions that could have a bear­ing on its ap­pli­ca­tion:How do we choose the ba­sis of at­trac­tion rel­a­tive to the tar­get con­figu­ra­tion set, if our choice can change the sta­tus of the sys­tem from op­ti­miz­ing to non-op­ti­miz­ing and vice versa?Is it an is­sue that an op­ti­miz­ing sys­tem may never ac­tu­ally op­ti­mize? How do we choose what is part of the sys­tem ver­sus a per­tur­ba­tion out­side the sys­tem when our choice changes the sta­tus of the sys­tem as op­ti­miz­ing or non-op­ti­miz­ing?All con­trol sys­tems are op­ti­miz­ing sys­tems by the defi­ni­tion, but are all con­trol sys­tems AI sys­tems? Does it mat­ter? If it does mat­ter, how do we tell the differ­ence?For any of these, how do they af­fect our think­ing for AI?Fi­nally, it might be bet­ter to have one, con­sis­tent defi­ni­tion that cov­ers all the pos­si­bil­ities, in­clud­ing (in my opinion) that per­tur­ba­tions may be con­fined to cer­tain di­men­sions.Aryeh Englander 2 Jul 2020 17:44 UTC LW: 4 AF: 2AFParentThis was ac­tu­ally part of a con­ver­sa­tion I was hav­ing with this col­league re­gard­ing whether or not evolu­tion can be viewed as an op­ti­miza­tion pro­cess. Here are some fol­low-up com­ments to what she wrote above re­lated to the evolu­tion an­gle:We could define the nat­u­ral se­lec­tion sys­tem as:All con­figu­ra­tions = all ar­range­ments of mat­ter on a planet (both ar­range­ments that are liv­ing and those that are non-liv­ing) Ba­sis of at­trac­tion = all ar­range­ments of mat­ter on a planet that meet the defi­ni­tion of a liv­ing thingTar­get con­figu­ra­tion set = all ar­range­ments of liv­ing things where the type and num­ber of liv­ing things re­mains ap­prox­i­mately sta­ble.I think that this sys­tem meets the defi­ni­tion of an op­ti­miz­ing sys­tem given in the Ground for Op­ti­miza­tion es­say. For ex­am­ple, preda­tor and prey co-evolve to be about “equal” in sur­vival abil­ity. If a preda­tor be­come so much bet­ter than its prey that it eats them all, the preda­tor will die out along with its prey; the re­main­ing an­i­mals will be in bal­ance. I think this works for cli­mate per­tur­ba­tions, etc. too.HOWEVER, it should be clear that there are nu­mer­ous ways in which this can hap­pen – like the ball on bumpy sur­face with a lot of con­vex “valleys” (lo­cal min­ima), there is not just one way that liv­ing things can be in bal­ance. So, to say that “nat­u­ral se­lec­tion op­ti­mized for in­tel­li­gence” is quite not right – it just fell into a “valley” where in­tel­li­gence hap­pened. FURTHER, it’s not clear that we have reached the lo­cal min­i­mum! Hu­mans may be that preda­tor that is go­ing to fall “prey” to its own suc­cess. If that hap­pened (and any in­tel­li­gent an­i­mals re­main at all), I guess we could say that nat­u­ral se­lec­tion op­ti­mized for less-than-hu­man in­tel­li­gence!Fur­ther, this defi­ni­tion of op­ti­miza­tion has no con­no­ta­tion of “best” or even bet­ter – just equal to a defined set. The word “op­ti­mize” is loaded. And its use in con­nec­tion with nat­u­ral se­lec­tion has led to a lot of trou­ble in terms of hu­man races, and hu­mans v. an­i­mal rights.Fi­nally, in the es­say’s defi­ni­tion, there is no im­per­a­tive that the tar­get set be reached. As long as the set of liv­ing things is “tend­ing” to­ward in­tel­li­gence, then the sys­tem is op­ti­miz­ing. So even if nat­u­ral se­lec­tion was op­ti­miz­ing for in­tel­li­gence there is no guaran­tee that it will be achieved (in its high­est man­i­fes­ta­tion). Like a billiards sys­tem where the table is slick (but not fric­tion­less) and the col­li­sions are close to elas­tic, the balls may come to rest with some of the balls out­side the pock­ets. The rea­son I think this is im­por­tant for AI re­search, es­pe­cially AGI and ASI, is per­haps we should be look­ing for those per­tur­ba­tions to pre­vent us from ever reach­ing what we may think of as the tar­get con­figu­ra­tion, de­spite our best efforts. johnswentworth 22 Jun 2020 22:05 UTC LW: 24 AF: 14AFMy biggest ob­jec­tion to this defi­ni­tion is that it in­her­ently re­quires time. At a bare min­i­mum, there needs to be an “ini­tial state” and a “fi­nal state” within the same state space, so we can talk about the sys­tem go­ing from out­side the tar­get set to in­side the tar­get set.One class of cases which definitely seem like op­ti­miza­tion but do not satisfy this prop­erty at all: one-shot non-iter­a­tive op­ti­miza­tion. For in­stance, I could write a con­vex func­tion op­ti­mizer which works by sym­bol­i­cally differ­en­ti­at­ing the ob­jec­tive func­tion and then alge­braically solv­ing for a point at which the gra­di­ent is zero.Is there an ar­gu­ment that I should not con­sider this to be an op­ti­mizer?Alex Flint 29 Jun 2020 3:18 UTC LW: 24 AF: 9AFParent
My biggest ob­jec­tion to this defi­ni­tion is that it in­her­ently re­quires time

Fas­ci­nat­ing—but why is this an ob­jec­tion? Is it just the in­el­e­gance of not be­ing able to look at a sin­gle time slice and an­swer the ques­tion of whether op­ti­miza­tion is hap­pen­ing?

One class of cases which definitely seem like op­ti­miza­tion but do not satisfy this prop­erty at all: one-shot non-iter­a­tive op­ti­miza­tion.

Yes this is a fas­ci­nat­ing case! I’d like to write a whole post about it. Here are my thoughts:
First, just as a fun fact, not that it’s ac­tu­ally ex­tremely rare to see any non-iter­a­tive op­ti­miza­tion in prac­ti­cal us­age. When we solve lin­ear equa­tions, we could use gaus­sian elimi­na­tion but it’s so un­sta­ble that in prac­tice we use, most likely, the SVD, which is iter­a­tive. When we solve a sys­tem of polyno­mial equa­tion we could use some­thing like a Grob­ner ba­sis or the re­sul­tant, but it’s so un­sta­ble that in prac­tice we some­thing like a com­pan­ion ma­trix method, which comes down to an eigen­value de­com­po­si­tion, which is again iter­a­tive.Con­sider find­ing the roots of a sim­ple quadratic equa­tion (ie solv­ing a cu­bic op­ti­miza­tion prob­lem). We can use the quadratic equa­tion to do this. But ul­ti­mately this comes down to com­put­ing a square root, which is typ­i­cally (though not nec­es­sar­ily) solved with an iter­a­tive method.That these meth­ods (for solv­ing lin­ear sys­tems, polyno­mial sys­tems, and quadratic equa­tions) have at their heart an iter­a­tive op­ti­miza­tion al­gorithm is not ac­ci­den­tal. The iter­a­tive meth­ods in­volved are not some small or sideline part of what’s go­ing on. In fact when you solve a sys­tem of polyno­mial equa­tions us­ing a com­pan­ion ma­trix, you spend a lot of en­ergy re­ar­rang­ing the sys­tem into a form where it can be solved via an eigen­value de­com­po­si­tion, and then the eigen­value de­com­po­si­tion it­self is very much op­er­at­ing on the full prob­lem. It’s not some unim­por­tant side op­er­a­tion. I find this fas­ci­nat­ing.Nev­er­the­less it is pos­si­ble to solve lin­ear sys­tems, polyno­mial sys­tems etc with non-iter­a­tive meth­ods.Th­ese meth­ods are definitely con­sid­ered “op­ti­miza­tion” by any nor­mal use of that term. So in this way my defi­ni­tion doesn’t quite line up with the com­mon lan­guage use of the word “op­ti­miza­tion”.But these non-iter­a­tive meth­ods ac­tu­ally do not have the core prop­erty that I de­scribed in the square-root-of-two ex­am­ple. If I reach in and flip a bit while a Guas­sian elimi­na­tion is run­ning, the al­gorithm does not in any sense re­cover. Since the al­gorithm is just perform­ing a lin­ear se­quence of steps, the er­ror just grows and grows as the com­pu­ta­tion un­folds. This is the op­po­site of what hap­pens if I reach in and flip a bit while an SVD is be­ing com­puted: in this case the er­ror will be driven back to zero by the iter­a­tive op­ti­miza­tion al­gorithm.You might say that my fo­cus on er­ror-cor­rec­tion sim­ply doesn’t cap­ture the com­mon lan­guage use of the term op­ti­miza­tion, as demon­strated by the fact that non-iter­a­tive op­ti­miza­tion al­gorithms do not have this er­ror-cor­rect­ing prop­erty. You would be cor­rect!But per­haps my real re­sponse is that fun­da­men­tally I’m in­ter­ested in these pro­cesses that some­what mys­te­ri­ously drive the state of the world to­wards a tar­get con­figu­ra­tion, and keep do­ing so de­spite per­tur­ba­tions. I think these are cen­tral to what AI and agency are. The term “op­ti­miz­ing sys­tem” might not be quite right, but it seems close enough to be com­pel­ling.
Thanks for the ques­tion—I clar­ified my own think­ing while writ­ing up this re­sponse.
johnswentworth 29 Jun 2020 4:50 UTC LW: 4 AF: 2AFParentAnother big thing to note in ex­am­ples like e.g. iter­a­tively com­put­ing a square root for the quadratic for­mula or iter­a­tively com­put­ing eigen­val­ues to solve a ma­trix: the op­ti­miza­tion prob­lems we’re solv­ing are sub­prob­lems, not the origi­nal full prob­lem. Th­ese cru­cially differ from most of the ex­am­ples in the OP in that the sys­tem’s ob­jec­tive func­tion (in your sense) does not match the ob­jec­tive func­tion (in the usual in­tu­itive sense). They’re iter­a­tively op­ti­miz­ing a sub­prob­lem’s ob­jec­tive, not the “full” prob­lem’s ob­jec­tive.That’s po­ten­tially an is­sue for think­ing about e.g. AI as an op­ti­mizer: if it’s us­ing iter­a­tive op­ti­miza­tion on sub­prob­lems, but us­ing those re­sults to perform some higher-level op­ti­miza­tion in a non-iter­a­tive man­ner, then al­ign­ing the sobprob­lem-op­ti­miz­ers may not be syn­ony­mous with al­ign­ing the full AI. In­deed, I think a lot of rea­son­ing works very much like this: we de­com­pose a high-di­men­sional prob­lem into cou­pled low-di­men­sional sub­prob­lems (i.e. “gears”), then ap­ply iter­a­tive op­ti­miz­ers to the sub­prob­lems. That’s ex­actly how eigen­value al­gorithms work, for in­stance: we de­com­pose the full prob­lem into a se­ries of op­ti­miza­tion sub­prob­lems in nar­rower and nar­rower sub­spaces, while the “high-level” part of the al­gorithm (i.e. out­side the sub­prob­lems) doesn’t look like iter­a­tive op­ti­miza­tion.johnswentworth 30 Jun 2020 17:04 UTC LW: 3 AF: 2AFParentFas­ci­nat­ing—but why is this an ob­jec­tion? Is it just the in­el­e­gance of not be­ing able to look at a sin­gle time slice and an­swer the ques­tion of whether op­ti­miza­tion is hap­pen­ing?No, the is­sue is that the usual defi­ni­tion of an op­ti­miza­tion prob­lem (e.g. maxx f(x)) has no built-in no­tion of time, and the in­tu­itive no­tion of op­ti­miza­tion (e.g. “the sys­tem makes Y big”) has no built-in no­tion of time (or at least lin­ear time). It’s this re­ally fun­da­men­tal thing that isn’t pre­sent in the “origi­nal prob­lem”, so to speak; it would be very sur­pris­ing and in­ter­est­ing if time had to be in­volved when it’s not pre­sent from the start. If I speci­fi­cally try to brain­storm things-which-look-like-op­ti­miza­tion-but-don’t-in­volve-ob­jec­tive-im­prove­ment-over-time, then it’s not hard to come up with ex­am­ples:Rather than a func­tion-value “im­prov­ing” along lin­ear time, I could think about a func­tion value im­prov­ing along some tree or DAG—e.g. in a heap data struc­ture, we have a tree where the “func­tion value” always “im­proves” as we move from any leaf to­ward the root. There, any path from a leaf to the root could be con­sid­ered “time” (but the whole set of nodes at the “same level” can’t be con­sid­ered a time-slice, be­cause we don’t have a mean­ingful way to com­pare whole sets of val­ues; we could in­vent one, but it wouldn’t ac­tu­ally re­flect the tree struc­ture).The ex­am­ple from the ear­lier com­ment: a one-shot non-iter­a­tive optimizerA dis­tributed op­ti­mizer: the sys­tem fans out, tests a whole bunch of pos­si­ble choices in par­allel, then se­lects the best of those.Var­i­ous fla­vors of con­straint prop­a­ga­tion, e.g. the sim­plex al­gorithm (and mar­kets more gen­er­ally)Davidmanheim 24 Jun 2020 17:15 UTC LW: 4 AF: 2AFParentI think this is cov­ered in my view of op­ti­miza­tion via se­lec­tion, where “di­rect solu­tion” is the third op­tion. Any one-shot op­ti­mizer is im­plic­itly rely­ing on an in­ter­nal model com­pletely for de­ci­sion mak­ing, rather than iter­at­ing, as I ex­plain there. I think that is com­pat­i­ble with the model here, but it needs to be ex­tended slightly to cover what I was try­ing to say there.newstorkcity@gmail.com 23 Jun 2020 17:41 UTC 1 pointParentThis model is ex­plic­itly re­quiring that you deal only with phys­i­cal pro­cesses, so your con­vex func­tion solver would re­quire time to get from the start­ing state to the end state. If it is hap­pen­ing non-iter­a­tively then it would cease to be an op­ti­miz­ing sys­tem af­ter it has com­pleted the func­tion, since there is no longer a tar­get con­figu­ra­tion.johnswentworth 23 Jun 2020 18:12 UTC 2 pointsParentI’m not sure what you’re try­ing to say here. What’s the state space (in which both the start and end state of the op­ti­mizer live), what’s the basin of at­trac­tion (i.e. set of al­lowed ini­tial con­di­tions), and what’s the tar­get re­gion within the state space? And re­mem­ber, the tar­get re­gion needs to be a sub­set of the al­lowed ini­tial con­di­tions.newstorkcity 24 Jun 2020 14:27 UTC 1 pointParentThis end state state is the solu­tion to the con­vex func­tion be­ing stored in some phys­i­cal reg­isters. The ini­tial state is those reg­isters con­tain­ing ar­bi­trary data to be over­writ­ten. It’s not par­tic­u­larly in­ter­est­ing as op­ti­miza­tion prob­lems go (not a very large basin of at­trac­tion) but it fulfills the ba­sic crite­ria.The unique thing about your ex­am­ple is that it solves once and then it is done (rel­a­tive to the ex­am­ples in the post), so it ceases to be an op­ti­miz­ing sys­tem once it finishes com­put­ing the solu­tion to your con­vex func­tion.With a slight mod­ifi­ca­tion, you could be re­peat­ing this al­gorithm in a loop so it con­stantly re­calcu­lates a new func­tion. Now the ini­tial state can be some value in the re­sult and in­put reg­isters, and the tar­get re­gion is the set of in­put equa­tions and ap­pro­pri­ate solu­tion in the out­put reg­isters. It widens the basin of at­trac­tion to both the in­put and out­put reg­isters rather than just the out­put.johnswentworth 24 Jun 2020 15:34 UTC 2 pointsParentOk, two prob­lems with this:There’s no rea­son why that tar­get set would be smaller than the basin of at­trac­tion. Given one such op­ti­miza­tion prob­lem, there are no ob­vi­ous per­tur­ba­tions we could make which would leave the re­sult in the tar­get re­gion.The tar­get re­gion is not a sub­set of the basin of at­trac­tion. The sys­tem doesn’t evolve from a larger re­gion to a smaller sub­set (as in the Venn-di­a­gram vi­su­als in the OP), it just evolves from one set to an­other.The first prob­lem ex­plic­itly vi­o­lates the OP’s defi­ni­tion of an op­ti­mizer, and the sec­ond prob­lem vi­o­lates one of the un­spo­ken as­sump­tions pre­sent in all of the OP’s ex­am­ples.newstorkcity 24 Jun 2020 17:42 UTC 1 pointParentI don’t be­lieve that ei­ther of these points are true. In your origi­nal ex­am­ple, there is one cor­rect solu­tion for any con­vex func­tion. I will as­sume there is a sin­gle hard-coded func­tion for the fol­low­ing, but it can be ex­tended to work for an ar­bi­trary func­tion. The out­put reg­ister hav­ing the cor­rect solu­tion is the tar­get set.The out­put reg­ister hav­ing any state is the basin of at­trac­tion.Clearly any spe­cific num­ber (or rather sin­gle­ton of that num­ber) is a sub­set of all num­bers, so the tar­get is a sub­set of the basin. And fur­ther, be­cause “all num­bers” has more than one el­e­ment, the tar­get set is smaller than the basin.johnswentworth 24 Jun 2020 17:44 UTC 2 pointsParentThis ar­gu­ment ap­plies to liter­ally any de­ter­minis­tic pro­gram with nonempty out­put. Are you say­ing that ev­ery pro­gram is an op­ti­mizer?newstorkcity 24 Jun 2020 18:03 UTC 1 pointParentPretty much, yes, ac­cord­ing to defi­ni­tion given. Like I said, not a par­tic­u­larly in­ter­est­ing op­ti­miza­tion but an op­ti­miza­tion none the less.To ex­tend on this, the basin of op­ti­miza­tion is not any smaller than an iter­a­tive pro­cess act­ing on a sin­gle reg­ister (and if you loop the pro­gram, then the time hori­zon is the same). In both cases your basin is any­thing in that reg­ister and the tar­get state is one par­tic­u­lar num­ber in that reg­ister. As far as I can tell the defi­ni­tion doesn’t have any way of say­ing that one is “more of an op­ti­mizer” than the other. If any­thing, the fixed out­put is more op­ti­mized be­cause it ar­rives more quickly.johnswentworth 24 Jun 2020 19:42 UTC 2 pointsParentOk, well, it seems like the one-shot non-iter­a­tive op­ti­mizer is an op­ti­mizer in a MUCH stronger sense than a ran­dom pro­gram, and I’d still ex­pect a defi­ni­tion of op­ti­miza­tion to say some­thing about the sense in which that holds.Richard_Ngo 22 Jun 2020 8:52 UTC LW: 19 AF: 7AFThis seems great, I’ll read and com­ment more thor­oughly later. Two quick com­ments:It didn’t seem like you defined what it meant to evolve to­wards the tar­get con­figu­ra­tion set. So it seems like ei­ther you need to com­mit to the sys­tem ac­tu­ally reach­ing one of the tar­get con­figu­ra­tions to call it an op­ti­miser, or you need some sort of met­ric over the con­figu­ra­tion space to tell whether it’s get­ting closer to or fur­ther away from the tar­get con­figu­ra­tion set. But if you’re rank­ing all con­figu­ra­tions any­way, then I’m not sure it adds any­thing to draw a bi­nary dis­tinc­tion be­tween tar­get con­figu­ra­tions and all the oth­ers. In other words, can’t you keep the defi­ni­tion in terms of a util­ity func­tion, but just add per­tur­ba­tions?Also, you don’t cite Den­nett here, but his defi­ni­tion has some im­por­tant similar­i­ties. In par­tic­u­lar, he defines sev­eral differ­ent types of per­tur­ba­tion (such as ran­dom per­tur­ba­tions, ad­ver­sar­ial per­tur­ba­tions, etc) and says that a sys­tem is more agen­tic when it can with­stand more types of per­tur­ba­tions. Can’t re­mem­ber ex­actly where this is from—per­haps The In­ten­tional Stance?What links here?Bridg­ing Ex­pected Utility Max­i­miza­tion and Optimization by Whispermute (5 Aug 2022 8:18 UTC; 25 points)Rohin Shah 23 Jun 2020 18:55 UTC LW: 8 AF: 5AFParentIt didn’t seem like you defined what it meant to evolve to­wards the tar­get con­figu­ra­tion set.+1 for swap­ping out the tar­get con­figu­ra­tion set with a util­ity func­tion, and look­ing for a ro­bust ten­dency for the util­ity func­tion to in­crease. This would also let you ex­press mild op­ti­miza­tion (see this thread).TurnTrout 23 Jun 2020 19:50 UTC LW: 2 AF: 1AFParentWould this work for highly non-mono­tonic util­ity func­tions? Richard_Ngo 23 Jun 2020 20:28 UTC LW: 2 AF: 1AFParentIt would work at least as well as the origi­nal pro­posal, be­cause your util­ity func­tion could just be what­ever met­ric of “get­ting closer to the tar­get states” would be used in the origi­nal pro­posal.
Ben Pace 22 Jun 2020 22:28 UTC LW: 17 AF: 10AFCu­rated. Come on dude, stop writ­ing so many awe­some posts so quickly, it’s too much.This is a cen­tral ques­tion in the sci­ence of agency and op­ti­miza­tion. The pro­posal is sim­ple, you con­nected it to other ideas from Drexler and Dem­ski+Garrabrant, and you gave a ton of ex­am­ples of how to ap­ply the idea. I gen­er­ally get scared by the aca­demic style, wor­ried that the au­thors will fill out the text and make it re­ally hard to read, but this was all highly read­able, and set its own con­text (re-ex­plain­ing the ba­sic ideas at the start). I’m look­ing for­ward to you dis­cussing it in the com­ments with Ricraz, Ro­hin and John.Please keep writ­ing these posts!Alex Flint 29 Jun 2020 2:31 UTC LW: 7 AF: 3AFParentThank you Ben. Read­ing this re­ally filled me with joy and gives me en­ergy to write more. Thank you for your cu­ra­tion work—it’s a huge part of why there is this place for such high qual­ity dis­cus­sion of top­ics like this, for which I’m very grate­ful.
Ben Pace 29 Jun 2020 3:09 UTC LW: 3 AF: 2AFParentYou’re wel­come :-)SoerenMind 22 Jun 2020 23:42 UTC 2 pointsParentSe­conded that the aca­demic style re­ally helped, par­tic­u­larly dis­cussing the prob­lem and prior work early on. One clas­sic in­tro­duc­tion para­graph that I was miss­ing is “what have prior works left un­ad­dressed?”.AdamGleave 31 Jul 2020 1:03 UTC LW: 16 AF: 8AFThanks for the post, this is my favourite for­mal­i­sa­tion of op­ti­mi­sa­tion so far!One con­cern I haven’t seen raised so far, is that the defi­ni­tion seems very sen­si­tive to the choice of con­figu­ra­tion space. As an ex­treme ex­am­ple, for any given sys­tem, I can always aug­ment the con­figu­ra­tion space with an ar­bi­trary num­ber of dummy di­men­sions, and choose the dy­nam­ics such that these dummy di­men­sions always get set to all zero af­ter each time step. Now, I can make the basin of at­trac­tion ar­bi­trar­ily large, while the tar­get con­figu­ra­tion set re­mains a fixed size. This can then make any such dy­nam­i­cal sys­tem seem to be an ar­bi­trar­ily pow­er­ful op­ti­miser.This could per­haps be solved by de­mand­ing the con­figu­ra­tion space be se­lected ac­cord­ing to Oc­cam’s ra­zor, but I think the out­come still ends up be­ing prior de­pen­dent. It’d be nice for two ob­servers who model op­ti­mis­ing sys­tems in a sys­tem­at­i­cally differ­ent way to always agree within some con­stant fac­tor, akin to Kol­mogorov com­plex­ity’s in­var­i­ance the­o­rem, al­though this may well be im­pos­si­ble.As a less face­tious ex­am­ple, con­sider a com­puter pro­gram that re­peat­edly sets a vari­able to 0. It seems again we can make the op­ti­mis­ing power ar­bi­trar­ily large by mak­ing the vari­able’s size ar­bi­trar­ily large. But this doesn’t quite map onto the in­tu­itive no­tion of the “difficulty” of an op­ti­mi­sa­tion prob­lem. Per­haps in­clud­ing some no­tion of how many other op­ti­mis­ing sys­tems would have the same tar­get set would re­solve this.What links here?Bridg­ing Ex­pected Utility Max­i­miza­tion and Optimization by Whispermute (5 Aug 2022 8:18 UTC; 25 points)Richard_Ngo 27 Jun 2020 12:11 UTC LW: 16 AF: 5AFTwo ex­am­ples which I’d be in­ter­ested in your com­ments on:1. Con­sider adding a big black hole in the mid­dle of a galaxy. Does this turn the galaxy into a sys­tem op­ti­mis­ing for a re­ally big black hole in the mid­dle of the galaxy? (Credit for the ex­am­ple goes to Ra­mana Ku­mar).2. Imag­ine that I have the goal of trav­el­ling as fast as pos­si­ble. How­ever, there is no set of states which you can point to as the “tar­get states”, since what­ever state I’m in, I’ll try to go even faster. This is an­other ar­gu­ment for, as I ar­gue be­low, defin­ing an op­ti­mis­ing sys­tem in terms of in­creas­ing some util­ity func­tion (rather than mov­ing to­wards tar­get states).What links here?Ramana Kumar's comment on Op­ti­miza­tion Con­cepts in the Game of Life by Vika (26 Oct 2021 9:31 UTC; 6 points)Ben Pace 29 Jun 2020 3:27 UTC LW: 3 AF: 1AFParentOn the topic of the black hole...There’s a way of view­ing the world as a se­ries of ”forces”, each try­ing to con­trol the fu­ture. Eukary­otic life is one. Black holes are an­other. We build many things, hu­mans, from chairs to planes to AIs. Of those three, turn­ing on the AI feels the most like “a new force has en­tered the game”. All these forces are fight­ing over the fu­ture, and while it’s odd to think of a black hole as an agent, some­times when I look at it it does feel nat­u­ral to think of physics as an­other op­ti­mi­sa­tion force that’s play­ing the game with us.Alex Flint 27 Jun 2020 19:22 UTC LW: 2 AF: 1AFParentGreat ex­am­ples! Thank you.

Con­sider adding a big black hole in the mid­dle of a galaxy. Does this turn the galaxy into a sys­tem op­ti­mis­ing for a re­ally big black hole in the mid­dle of the galaxy?

Yes this would qual­ify as an op­ti­miz­ing sys­tem by my defi­ni­tion. In fact just plac­ing a large planet close to a bunch of smaller planets would qual­ify as an op­ti­miz­ing sys­tem if the even­tual re­sult is to col­lapse the mass of the smaller planets into the larger planet.
This seems to me to be a lot like a ball rol­ling down a hill: a black hole doesn’t seem al­ive or agen­tic, and it doesn’t re­ally re­spond in any mean­ingful way to hur­dles put in its way, but yes it does qual­ify as an op­ti­miz­ing sys­tem. For this rea­son my defi­ni­tion isn’t yet a very good defi­ni­tion of what agency is, or what post-agency con­cept we should adopt. I like Ro­hin’s com­ment on how we might view agency in this frame­work.

Imag­ine that I have the goal of trav­el­ling as fast as pos­si­ble. How­ever, there is no set of states which you can point to as the “tar­get states”, since what­ever state I’m in, I’ll try to go even faster. This is an­other ar­gu­ment for, as I ar­gue be­low, defin­ing an op­ti­mis­ing sys­tem in terms of in­creas­ing some util­ity func­tion (rather than mov­ing to­wards tar­get states).

Yes it’s true that us­ing a set of tar­get states rather than an or­der­ing over states means that we can’t han­dle cases where there is a di­rec­tion of op­ti­miza­tion but not a “des­ti­na­tion”. But if we use an or­der­ing over states then we run into the fol­low­ing prob­lem: how can we say whether a sys­tem is ro­bust to per­tur­ba­tions? Is it just that the sys­tem con­tinues to climb the prefer­ence gra­di­ent de­spite per­tur­ba­tions? But now ev­ery sys­tem is an op­ti­miz­ing sys­tem, be­cause we can always come up with some prefer­ence or­der­ing that ex­plains a sys­tem as an op­ti­miz­ing sys­tem. So then we can say “well it should be an or­der­ing over states with a com­pact rep­re­sen­ta­tion” or “it should be more com­pact than com­pet­ing ex­pla­na­tions”. This may be okay but it seems quite dicey to me.
It ac­tu­ally seems quite im­por­tant to me that the defi­ni­tion point to sys­tems that “get back on track” even when you push them around. It may be pos­si­ble to do this with an or­der­ing over states and I’d love to dis­cuss this more.
Richard_Ngo 28 Jun 2020 17:27 UTC LW: 8 AF: 4AFParentBut now ev­ery sys­tem is an op­ti­miz­ing sys­tem, be­cause we can always come up with some prefer­ence or­der­ing that ex­plains a sys­tem as an op­ti­miz­ing sys­tem.Hmmm, I’m a lit­tle un­cer­tain about whether this is the case. E.g. sup­pose you have a box with a rock in it, in an oth­er­wise empty uni­verse. Noth­ing hap­pens. You per­turb the sys­tem by mov­ing the rock out­side the box. Noth­ing else hap­pens in re­sponse. How would you de­scribe this as an op­ti­mis­ing sys­tem? (I’m as­sum­ing that we’re rul­ing out the triv­ial case of a con­stant util­ity func­tion; if not, we should analo­gously in­clude the triv­ial case of all states be­ing tar­get states).As a more gen­eral com­ment: I sus­pect that what starts to hap­pen af­ter you start dig­ging into what “per­tur­ba­tion” means, and what counts as a small or big per­tur­ba­tion, is that you run into the prob­lem that a *tiny* per­tur­ba­tion can trans­form a highly op­ti­mis­ing sys­tem to a non-op­ti­mis­ing sys­tem (e.g. flick­ing the switch to turn off the AGI). In or­der to quan­tify size of per­tur­ba­tions in an in­ter­est­ing way, you need the pre-ex­ist­ing con­cept of which sub­sys­tems are do­ing the op­ti­mi­sa­tion.My preferred solu­tion to this is just to stop try­ing to define op­ti­mi­sa­tion in terms of *out­comes*, and start defin­ing it in terms of *com­pu­ta­tion* done by sys­tems. E.g. a first at­tempt might be: an agent is an op­ti­miser if it does plan­ning via ab­strac­tion to­wards some goal. Then we can zoom in on what all these words mean, or what else we might need to in­clude/​ex­clude (in this case, we’ve ruled out evolu­tion, so we prob­a­bly need to broaden it). The broad philos­o­phy here is that it’s bet­ter to be vaguely right than pre­cisely wrong. Un­for­tu­nately I haven’t writ­ten much about this ap­proach pub­li­cly—I briefly defend it in a com­ment thread on this post though.ESRogs 1 Jul 2020 7:43 UTC 2 pointsParentI briefly defend it in a com­ment thread on this post though (https://​​www.less­wrong.com/​​posts/​​9px­cekdNjE7oNwvcC/​​goal-di­rect­ed­ness-is-be­hav­ioral-not-struc­tural )FYI: I think some­thing got messed up with this link. The text of the link is a valid url, but it links to a man­gled one (s.t. if you click it you get a 404 er­ror).Richard_Ngo 1 Jul 2020 8:34 UTC 4 pointsParentThat’s weird; thanks for the catch. Fixed.Alex Flint 28 Jun 2020 23:27 UTC LW: 2 AF: 1AFParent
sup­pose you have a box with a rock in it, in an oth­er­wise empty uni­verse [...]

Yes you’re right, this sys­tem would be de­scribed by a con­stant util­ity func­tion, and yes this is analo­gous to the case where the tar­get con­figu­ra­tion set con­tains all con­figu­ra­tions, and yes this should not be con­sid­ered op­ti­miza­tion. In the tar­get set for­mu­la­tion, we can mea­sure the de­gree of op­ti­miza­tion by the size of the tar­get set rel­a­tive to the size of the basin of at­trac­tion. In your rock ex­am­ple, the sets have the same size, so it would make sense to say that the de­gree of op­ti­miza­tion is zero.
This dis­cus­sion is up­dat­ing me in the di­rec­tion that a prefer­ence or­der­ing for­mu­la­tion is pos­si­ble, but that we need some anal­ogy for “de­gree of op­ti­miza­tion” that cap­tures how “tight” or “con­strained” the sys­tem’s evolu­tion is rel­a­tive to the size of the basin of at­trac­tion. We need a way to say that a con­stant util­ity func­tion cor­re­sponds to a de­gree of op­ti­miza­tion equal to zero. We also need a way to han­dle the case where our util­ity func­tion as­signs util­ity pro­por­tional to en­tropy, so again we can de­scribe all phys­i­cal sys­tems as op­ti­miz­ing sys­tems and ther­mo­dy­nam­ics en­sures that we are cor­rect. This util­ity func­tion would be ex­tremely flat and wide, with most con­figu­ra­tions re­ceiv­ing near-iden­ti­cal util­ity (since the high en­tropy con­figu­ra­tions con­sti­tute the vast ma­jor­ity of all pos­si­ble con­figu­ra­tions). I’m sure there is some way to quan­tify this—do you know of any ap­pro­pri­ate mea­sure?
The challenge here is that in or­der to ac­tu­ally deal with the case you men­tioned origi­nally—the goal of mov­ing as fast as pos­si­ble—we need a mea­sure that is not based on the size or cur­va­ture of some lo­cal max­ima of the util­ity func­tion. If we are work­ing with lo­cal max­ima then we are re­ally still work­ing with sys­tems that evolve to­wards a spe­cific des­ti­na­tion (al­though there still may be ad­van­tages to think­ing this way rather than in terms of a bi­nary set).

My preferred solu­tion to this is just to stop try­ing to define op­ti­mi­sa­tion in terms of out­comes, and start defin­ing it in terms of com­pu­ta­tion done by systems

Nice—I’d love to hear more about this
ESRogs 1 Jul 2020 7:37 UTC 2 pointsParentBut if we use an or­der­ing over states then we run into the fol­low­ing prob­lem: how can we say whether a sys­tem is ro­bust to per­tur­ba­tions? Is it just that the sys­tem con­tinues to climb the prefer­ence gra­di­ent de­spite per­tur­ba­tions? But now ev­ery sys­tem is an op­ti­miz­ing sys­tem, be­cause we can always come up with some prefer­ence or­der­ing that ex­plains a sys­tem as an op­ti­miz­ing sys­tem. So then we can say “well it should be an or­der­ing over states with a com­pact rep­re­sen­ta­tion” or “it should be more com­pact than com­pet­ing ex­pla­na­tions”. This may be okay but it seems quite dicey to me.Doesn’t the set-of-tar­get-states ver­sion have just the same is­sue (or an analo­gous one)?For what­ever be­hav­ior the sys­tem ex­hibits, I can always say that the states it ends up in were part of its set of tar­get states. So you have to count on com­pact­ness (or nat­u­ral­ness of de­scrip­tion, which is ba­si­cally the same thing) of the set of tar­get states for this con­cept of an op­ti­miz­ing sys­tem to be mean­ingful. No?Alex Flint 1 Jul 2020 23:53 UTC 2 pointsParentWell most sys­tem don’t have a ten­dency to evolve to­wards any small set of tar­get states de­spite per­tur­ba­tions. Most sys­tems, if you per­turb then, just go off in some differ­ent di­rec­tion. For ex­am­ple, if you per­turb most run­ning com­puter pro­grams by mod­ify­ing some vari­able with a de­bug­ger, they do not self-cor­rect. Same with the satel­lite and billiard balls ex­am­ple. Most sys­tems just don’t have this “at­trac­tor” dy­namic.
ESRogs 2 Jul 2020 1:42 UTC 2 pointsParentHmm, I see what you’re say­ing, but there still seems to be an anal­ogy to me here with ar­bi­trary util­ity func­tions, where you need the set of tar­get states to be small (as you do say). Other­wise I could just say that the set of tar­get states is all the di­rec­tions the sys­tem might fly off in if you per­turb it.So you might say that, for this ver­sion of op­ti­miza­tion to be mean­ingful, the set of tar­get states has to be small (how­ever that’s quan­tified), and for the util­ity max­i­miza­tion ver­sion to be mean­ingful, you need the util­ity func­tion to be sim­ple (how­ever that’s quan­tified).EDIT: And ac­tu­ally, maybe the two con­cepts are sort of dual to each other. If you have an agent with a sim­ple util­ity func­tion, then you could con­sider all its lo­cal op­tima to be a (small) set of tar­get states for an op­ti­miz­ing sys­tem. And if you have an op­ti­miz­ing sys­tem with a small set of tar­get states, then you could eas­ily con­vert that into a sim­ple util­ity func­tion with a gra­di­ent to­wards those states.And if your util­ity func­tion isn’t sim­ple, maybe you wouldn’t get a small set of tar­get states when you do the con­ver­sion, and vice versa?Alex Flint 2 Jul 2020 1:54 UTC 4 pointsParentI’d say the util­ity func­tion needs to con­tain one or more lo­cal op­tima with large bas­ins of at­trac­tion that con­tain the ini­tial state, not that the util­ity func­tion needs to be sim­ple. The sim­plest pos­si­ble util­ity func­tion is a con­stant func­tion, which al­lows the sys­tem to wan­der aim­lessly and cer­tainly not “cor­rect” in any way for per­tur­ba­tions.
ESRogs 2 Jul 2020 5:01 UTC 2 pointsParentAh, good points!clwainwright 23 Jun 2020 0:22 UTC LW: 11 AF: 3AFThis seems like a good defi­ni­tion of op­ti­miza­tion for al­gorith­mic sys­tems, but I don’t see how it works for phys­i­cal sys­tems. Go­ing by the pri­mary defi­ni­tion,An op­ti­miz­ing sys­tem is a sys­tem that has a ten­dency to evolve to­wards one of a set of con­figu­ra­tions that we will call the tar­get con­figu­ra­tion set, when started from any con­figu­ra­tion within a larger set of con­figu­ra­tions, which we call the basin of at­trac­tion.But in the phys­i­cal world, there are liter­ally zero closed sys­tems with this prop­erty. En­tropy always in­creases*, and the tar­get con­figu­ra­tion set will never be smaller than the basin of at­trac­tion. The dirt-plus-seed-plus-sun­light sys­tem has a vastly smaller con­figu­ra­tion space than the dirt-plus-tree-plus-heat sys­tem. Per­haps one could ob­ject that one should dis­count the in­com­ing sun­light and out­go­ing heat since the sys­tem isn’t re­ally closed, but then con­sider a very similar sys­tem con­sist­ing of only dirt, air, and fun­gal spores. Surely if a grow­ing tree is an op­ti­miz­ing sys­tem, then a grow­ing mush­room in a closed sys­tem is an op­ti­mizer too. But the en­tropy in­crease in the lat­ter case is un­am­bigu­ous: the num­ber of ways to ar­range atoms into a fully grown mush­room is again vastly larger than the num­ber of ways to con­figure atoms into dirt with­out mush­rooms but with the nu­tri­ents to grow them.It may be pos­si­ble to get around this by re­defin­ing con­figu­ra­tion spaces that bet­ter match our in­tu­ition (it does seem like a mush­room is more spe­cial than dirt), but I don’t see any way to do this rigor­ously.*or, at least, en­tropy always tends to in­crease.Thomas Kwa 26 Mar 2021 21:25 UTC 3 pointsParentI agree that closed phys­i­cal sys­tems aren’t op­ti­miz­ing sys­tems. It seems like the first patch given by the au­thor works when worded more care­fully: “We could stipu­late that some [low-en­tropy] power source [and some en­tropy sink] is pro­vided ex­ter­nally to each sys­tem we an­a­lyze, and then perform our anal­y­sis con­di­tional on the ex­is­tence of that power source.”
Then an op­ti­miz­ing sys­tem with X bits of “op­ti­miza­tion power” (which is log(tar­get states /​ basin of at­trac­tion size) or some­thing) has to sink at least X bits, and this seems like it works. Maybe it gets hard to rigor­ously define the ex­act form of the power source and en­tropy sink though? Dis­claimer: I don’t know statis­ti­cal me­chan­ics.Davidmanheim 24 Jun 2020 17:12 UTC LW: 10 AF: 5AFI think this is great. I would want to re­late it to a few key points out which I tried to ad­dress in a few ear­lier posts. Prin­ci­pally, I dis­cussed se­lec­tion ver­sus con­trol, which is about the differ­ence be­tween what op­ti­miza­tion does ex­ter­nally, and how it uses mod­els and test­ing. This re­lated strongly to your con­cep­tion of an op­ti­miz­ing sys­tem, but fo­cused on how much of the op­ti­miza­tion pro­cess oc­curs in the sys­tem ver­sus in the agent it­self. This is prin­ci­pally im­por­tant be­cause of how it re­lates to mis­al­ign­ment and Good­hart­ing of var­i­ous types.I had hopes to fur­ther ap­ply that con­cep­tual model to meas-op­ti­miza­tion, but I was a bit un­sure how to think about it, and have been work­ing on other pro­jects. At this point, I think your dis­cus­sion is prob­a­bly a bet­ter con­cep­tual model than the one I was try­ing to build there—it just needs to be slightly ex­tended to cover the points I was try­ing to work out in those posts. I’d like to think about how it re­lates to mesa-op­ti­miza­tion as well, but I’m un­likely to ac­tu­ally work on thatStuart_Armstrong 31 Jul 2020 15:48 UTC LW: 8 AF: 4AFVery good. A lot of po­ten­tial there, I feel.johnswentworth 22 Jun 2020 21:39 UTC LW: 7 AF: 4AFThis is ex­cel­lent! Very well done, I would love to see more work like this.I have a whole bunch of things to say along sep­a­rate di­rec­tions so I’ll break them into sep­a­rate com­ments. This first one is just a cou­ple minor notes:For the uni­verse sec­tion, the uni­verse doesn’t push “to­ward” max­ent, it just wan­ders around and usu­ally ends up in max­ent states be­cause that’s most of the states. The basin of at­trac­tion in­cludes all states.Re­gard­ing “whether dy­nam­i­cal sys­tems the­ory ex­plic­itly stud­ies at­trac­tors that op­er­ate along a sub­set of the sys­tem’s di­men­sions”, I be­lieve there’s an old the­o­rem that the long-term be­hav­ior of dy­nam­i­cal sys­tems on a com­pact space is always er­godic on some man­i­fold within the space. That man­i­fold has a name which I don’t re­mem­ber, which is prob­a­bly what you want to look for.ryan_b 23 Jun 2020 21:09 UTC 2 pointsParentDoes “er­godic on some man­i­fold” here mean it ap­proaches ev­ery point within the man­i­fold, as in the er­god­ic­ity as­sump­tion, or does it mean de­scribed by an er­godic func­tion? I re­al­ize the lat­ter im­plies the former, but what I am driv­ing at is the be­hav­ior vs. the for­mal­ism.johnswentworth 23 Jun 2020 22:30 UTC 2 pointsParentNot sure.Rohin Shah 21 Jun 2020 20:06 UTC LW: 7 AF: 4AFPlanned sum­mary for the Align­ment Newslet­ter:Many ar­gu­ments about AI risk de­pend on the no­tion of “op­ti­miz­ing”, but so far it has eluded a good defi­ni­tion. One nat­u­ral ap­proach is to say that an op­ti­mizer causes the world to have higher val­ues ac­cord­ing to some rea­son­able util­ity func­tion, but this seems in­suffi­cient, as then a <@bot­tle cap would be an op­ti­mizer@>(@Bot­tle Caps Aren’t Op­ti­misers@) for keep­ing wa­ter in the bot­tle.This post pro­vides a new defi­ni­tion of op­ti­miza­tion, by tak­ing a page from <@Embed­ded Agents@> and an­a­lyz­ing a sys­tem as a whole in­stead of sep­a­rat­ing the agent and en­vi­ron­ment. An **op­ti­miz­ing sys­tem** is then one which tends to evolve to­ward some spe­cial con­figu­ra­tions (called the **tar­get con­figu­ra­tion set**), when start­ing any­where in some larger set of con­figu­ra­tions (called the **basin of at­trac­tion**), _even if_ the sys­tem is per­turbed.For ex­am­ple, in gra­di­ent de­scent, we start with some ini­tial guess at the pa­ram­e­ters θ, and then con­tinu­ally com­pute loss gra­di­ents and move θ in the ap­pro­pri­ate di­rec­tion. The tar­get con­figu­ra­tion set is all the lo­cal min­ima of the loss land­scape. Such a pro­gram has a very spe­cial prop­erty: while it is run­ning, you can change the value of θ (e.g. via a de­bug­ger), and the pro­gram will prob­a­bly _still work_. This is quite im­pres­sive: cer­tainly most pro­grams would not work if you ar­bi­trar­ily changed the value of one of the vari­ables in the mid­dle of ex­e­cu­tion. Thus, this is an op­ti­miz­ing sys­tem that is ro­bust to per­tur­ba­tions in θ. Of course, it isn’t ro­bust to ar­bi­trary per­tur­ba­tions: if you change any other vari­able in the pro­gram, it will prob­a­bly stop work­ing. In gen­eral, we can quan­tify how pow­er­ful an op­ti­miz­ing sys­tem is by how ro­bust it is to per­tur­ba­tions, and how small the tar­get con­figu­ra­tion set is.The bot­tle cap ex­am­ple is _not_ an op­ti­miz­ing sys­tem be­cause there is no broad basin of con­figu­ra­tions from which we get to the bot­tle be­ing full of wa­ter. The bot­tle cap doesn’t cause the bot­tle to be full of wa­ter when it didn’t start out full of wa­ter.Op­ti­miz­ing sys­tems are a su­per­set of goal-di­rected agen­tic sys­tems, which re­quire a sep­a­ra­tion be­tween the op­ti­mizer and the thing be­ing op­ti­mized. For ex­am­ple, a tree is cer­tainly an op­ti­miz­ing sys­tem (the tar­get is to be a fully grown tree, and it is ro­bust to per­tur­ba­tions of soil qual­ity, or if you cut off a branch, etc). How­ever, it does not seem to be a goal-di­rected agen­tic sys­tem, as it would be hard to sep­a­rate into an “op­ti­mizer” and a “thing be­ing op­ti­mized”.This does mean that we can no longer ask “what is do­ing the op­ti­miza­tion” in an op­ti­miz­ing sys­tem. This is a fea­ture, not a bug: if you ex­pect to always be able to an­swer this ques­tion, you typ­i­cally get con­fus­ing re­sults. For ex­am­ple, you might say that your liver is op­ti­miz­ing for mak­ing money, since with­out it you would die and fail to make money.The full post has sev­eral other ex­am­ples that help make the con­cept clearer.Planned opinion:I’ve <@pre­vi­ously ar­gued@>(@In­tu­itions about goal-di­rected be­hav­ior@) that we need to take gen­er­al­iza­tion into ac­count in a defi­ni­tion of op­ti­miza­tion or goal-di­rected be­hav­ior. This defi­ni­tion achieves that by pri­mar­ily an­a­lyz­ing the ro­bust­ness of the op­ti­miz­ing sys­tem to per­tur­ba­tions. While this does rely on a no­tion of coun­ter­fac­tu­als, it still seems sig­nifi­cantly bet­ter than any pre­vi­ous at­tempt to ground op­ti­miza­tion.I par­tic­u­larly like that the con­cept doesn’t force us to have a sep­a­rate agent and en­vi­ron­ment, as that dis­tinc­tion does seem quite leaky upon close in­spec­tion. I gave a shot at ex­plain­ing sev­eral other con­cepts from AI al­ign­ment within this frame­work in this com­ment, and it worked quite well. In par­tic­u­lar, a com­puter pro­gram is a goal-di­rected AI sys­tem if there is an en­vi­ron­ment such that adding the com­puter pro­gram to the en­vi­ron­ment trans­forms it into a op­ti­miz­ing sys­tem for some “in­ter­est­ing” tar­get con­figu­ra­tion states (with one caveat ex­plained in the com­ment).Andrew_Critch 15 Dec 2020 1:06 UTC LW: 6 AF: 4AFThis post re­minds me of think­ing from 1950s when peo­ple tak­ing in­spira­tion from Wiener’s work on cy­ber­net­ics tried to op­er­a­tional­ize “pur­pose­ful be­hav­ior” in terms of ro­bust con­ver­gence to a goal state: https://​​heinon­line.org/​​HOL/​​Page?col­lec­tion=jour­nals&han­dle=hein.jour­nals/​​josf29&id=48&men_tab=srchresults> When an op­ti­miz­ing sys­tem de­vi­ates be­yond its own rim, we say that it dies. An ex­is­ten­tial catas­tro­phe is when the op­ti­miz­ing sys­tem of life on Earth moves be­yond its own outer rim.I ap­pre­ci­ate the di­rect at­ten­tion to this pro­cess as an im­por­tant in­stance of op­ti­miza­tion. The first talk I ever gave in the EECS de­part­ment at UC Berkeley (to the full EECS fac­ulty) in­cluded a di­a­gram of Earth drift­ing out of the re­gion of phase spare where hu­mans would ex­ist. Need­less to say, I’d like to see more ex­plicit con­sid­er­a­tion of this type of sce­nario.johnswentworth 22 Jun 2020 21:54 UTC LW: 6 AF: 3AFThe set of op­ti­miz­ing sys­tems is smaller than the set of all AI ser­vices, but larger than the set of goal-di­rected agen­tic sys­tems....A tree is an op­ti­miz­ing sys­tem but not a goal-di­rected agent sys­tem.I’m not sure this is true, at least not in the sense that we usu­ally think about “goal-di­rected agent sys­tems”.You make a case that there’s no dis­tinct sub­sys­tem of the tree which is “do­ing the op­ti­miz­ing”, but this isn’t ob­vi­ously rele­vant to whether the tree is agenty. For in­stance, the tree pre­sum­ably still needs to model its en­vi­ron­ment to some ex­tent, and “make de­ci­sions” to op­ti­mize its growth within the en­vi­ron­ment—e.g. new branches/​leaves grow­ing to­ward sun­light and roots grow­ing to­ward wa­ter, or the tree “pre­dict­ing” when the sea­sons are turn­ing and grow­ing/​drop­ping leaves ac­cord­ingly.One to think about whether “the set of op­ti­miz­ing sys­tems is smaller than the set of all AI ser­vices, but larger than the set of goal-di­rected agen­tic sys­tems” is that it’s equiv­a­lent to Scott’s (open) ques­tion does agent-like be­hav­ior im­ply agent-like ar­chi­tec­ture?johnswentworth 22 Jun 2020 21:46 UTC LW: 6 AF: 3AFAt first I par­tic­u­larly liked the idea of iden­ti­fy­ing sys­tems with “an op­ti­mizer” as those which are ro­bust to changes in the ob­ject of op­ti­miza­tion, but brit­tle with re­spect to changes in the en­g­ine of op­ti­miza­tion.On re­flec­tion, it seems like a use­ful heuris­tic but not a re­li­able defi­ni­tion. A coun­terex­am­ple: sup­pose we do man­age to build a ro­bust AI which max­i­mizes some util­ity func­tion. One de­sir­able prop­erty of such an AI is that it’s ro­bust to e.g. one of its servers go­ing down or cor­rupted data on a hard drive; the AI it­self should be ro­bust to as many in­ter­ven­tions as pos­si­ble. Ideally it would even be ro­bust to minor bugs in its own source code. Yet it still seems like the AI is the “en­g­ine”, and it op­ti­mizes the rest of the world.Alex Flint 27 Jun 2020 19:44 UTC LW: 4 AF: 2AFParentYeah I agree that du­al­ity is not a good mea­sure of whether a sys­tem con­tains some­thing like an AI. There is one kind of AI that we can build that is highly du­al­is­tic. Most pre­sent-day AI sys­tems are quite du­al­is­tic, be­cause they are pred­i­cated on hav­ing some ro­bust com­pute in­fras­truc­ture that is sep­a­rate from and mostly un­per­turbed by the world around it. But there is ev­ery rea­son to go be­yond these du­al­is­tic de­signs, for pre­cisely the rea­son you point to: such sys­tems do tend to be some­what brit­tle.
I think it’s quite fea­si­ble to build highly ro­bust AI sys­tems, al­though do­ing so will likely re­quire more than just hard­en­ing (mak­ing it re­ally un­likely for the sys­tem to be per­turbed). What we re­ally want is an AI sys­tem where the core AI it­self tends to evolve back to a sta­ble con­figu­ra­tion de­spite per­tur­ba­tions to its core in­fras­truc­ture. My sense is that this will ac­tu­ally re­quire a sig­nifi­cant shift in how we think about AI—speci­fi­cally mov­ing from the agent model to some­thing that cap­tures what is good and helpful in the agent model but dis­cards the du­al­is­tic view of things.
Chantiel 17 Aug 2021 20:31 UTC 3 points
An op­ti­miz­ing sys­tem is a sys­tem that has a ten­dency to evolve to­wards one of a set of con­figu­ra­tions that we will call the tar­get con­figu­ra­tion set, when started from any con­figu­ra­tion within a larger set of con­figu­ra­tions, which we call the basin of at­trac­tion, and con­tinues to ex­hibit this ten­dency with re­spect to the same tar­get con­figu­ra­tion set de­spite per­tur­ba­tions.

First, I want to say that I think your defi­ni­tion says some­thing im­por­tant.
That said, I’m con­cerned that the above defi­ni­tion would have some po­ten­tially prob­le­matic false nega­tives. I’m a lit­tle un­clear what counts as a per­tur­ba­tion, though, but I haven’t been able to find a way to clar­ify it that doesn’t re­sult in false nega­tives.
Speci­fi­cally, con­sider a com­puter pro­gram that performs hill-climb­ing. This would nor­mally be con­sid­ered an op­ti­miz­ing sys­tem. When do­ing hill-climb­ing, it doesn’t seem like there is any­thing that would count as a per­tur­ba­tion un­less some ex­ter­nal sys­tem mod­ified the pro­gram’s state. I mean, dur­ing a nor­mal, undis­turbed ex­e­cu­tion, the hill-climb­ing al­gorithm would just go right to the top of its near­est hill and then stop; that doesn’t seem to in­clude any per­tur­ba­tions.
But sup­pose the pro­gram checked for any ex­ter­nal per­tur­ba­tions, that is, mod­ifi­ca­tions, of its code or pro­gram mem­ory and would im­me­di­ately halt ex­e­cu­tion if it found any. For ex­am­ple, sup­pose the pro­gram would si­mul­ta­neously run thou­sands of iden­ti­cal in­stances of a hill-climb­ing al­gorithm and would im­me­di­ately halt ex­e­cu­tion if any of the in­stances of the op­ti­miza­tion al­gorithm failed to ex­actly match any other one. That way, if some ex­ter­nal force mod­ified one of the in­stances, for ex­am­ple, by mod­ify­ing one of the can­di­date solu­tions, it would fail to match with all the other in­stances and the en­tire pro­gram would halt.
Now, there are some ex­ter­nal per­tur­ba­tions of the sys­tem that would make it still reach its tar­get state, for ex­am­ple by mak­ing the ex­act same ex­ter­nal mod­ifi­ca­tion to ev­ery in­stance of the op­ti­miza­tion pro­ce­dure. But still al­most all per­tur­ba­tions would re­sult in the pro­gram failing to reach its tar­get con­figu­ra­tion of hav­ing found the lo­cal max­i­mum or min­i­mum. So it doesn’t re­ally seem to tend to reach its tar­get con­figu­ra­tion de­spite per­tur­ba­tions. So it doesn’t seem it would be clas­sified as an op­ti­mizer ac­cord­ing to the given defi­ni­tion.
This could be prob­le­matic if the defi­ni­tion is used to pre­vent mesaop­ti­miza­tion. If the above would in­deed not be con­sid­ered an op­ti­mizer by your defi­ni­tion, then it could po­ten­tially al­low for pow­er­ful mesaop­ti­miz­ers to be cre­ated with­out match­ing the given defi­ni­tion of “op­ti­mizer”.What links here?Chantiel's comment on Chantiel’s Shortform by Chantiel (21 Aug 2021 17:25 UTC; 3 points)David Cato 23 Jun 2020 12:10 UTC LW: 3 AF: 1AFTruly a joy to read! Thank you.To what ex­tent can we iden­tify sub­sets of the sys­tem cor­re­spond­ing to  “that which is be­ing op­ti­mized” and “that which is do­ing the  op­ti­miza­tion”?The in­for­ma­tion the­o­retic mea­sure of in­di­vi­d­u­al­ity at­tempts to an­swer ex­actly this type of ques­tion.From this view, a set of com­po­nents (the sys­tem) is de­com­posed into two sub­sets (sub­sys­tem + en­vi­ron­ment). The pro­posed sub­sys­tem is as­signed a de­gree of in­di­vi­d­u­al­ity by mea­sur­ing the amount of in­for­ma­tion it shares with its fu­ture state, op­tion­ally con­di­tioned on its en­vi­ron­ment. This leads to 2 types of in­di­vi­d­u­al­ity. The first type says that a pro­posed sub­sys­tem is in­di­vi­d­u­al­is­tic to the de­gree that the sub­sys­tem is pre­dic­tive of its fu­ture state af­ter ac­count­ing for the in­for­ma­tion in the en­vi­ron­ment. The sec­ond type cap­tures the no­tion of in­sep­a­ra­bil­ity by as­sign­ing a high de­gree of in­di­vi­d­u­al­ity to sub­sys­tems that are strongly cou­pled with their en­vi­ron­ment in such a way that nei­ther the sub­sys­tem nor en­vi­ron­ment alone are pre­dic­tive of the next state of the sub­sys­tem.For ex­am­ple, con­sid­er­ing the set of atoms mak­ing up the space con­tain­ing the robot-op­ti­mizer and vase, the set of robot-atoms re­tains the de­sired prop­er­ties of an op­ti­mizer, and is also highly in­di­vi­d­u­al­is­tic in the first sense since know­ing the state of the robot atoms tells you a lot about their next state, but know­ing about the set of non-robot atoms tells you very lit­tle about the state of the robot. On the other hand, con­sid­er­ing the set of atoms mak­ing up the tree, the sys­tem as a whole is an op­ti­miz­ing sys­tem, but no in­di­vi­d­ual sub­set of atoms ac­com­plishes the tar­get of the larger op­ti­miz­ing sys­tem.Alex Flint 27 Jun 2020 18:50 UTC LW: 2 AF: 1AFParentThank you for the poin­ter to this ter­minol­ogy. It seems rele­vant and I wasn’t aware of the ter­minol­ogy be­fore.
Vivek Hebbar 8 Sep 2021 0:23 UTC LW: 2 AF: 2AFIs a metal bar an op­ti­mizer? Look­ing at the tem­per­a­ture dis­tri­bu­tion, there is a clear set of tar­get states (states of uniform tem­per­a­ture) with a much larger basin of at­trac­tion (all tem­per­a­ture dis­tri­bu­tions that don’t va­por­ize the bar).I sup­pose we could con­sider the sec­ond law of ther­mo­dy­nam­ics to be the true op­ti­mizer in this case. The con­se­quence is that any* closed phys­i­cal sys­tem is triv­ially an op­ti­miz­ing sys­tem to­wards higher en­tropy.In gen­eral, it seems like this op­ti­miza­tion crite­rion is very easy to satisfy if we don’t spec­ify what ex­actly we care about as a mean­ingful as­pect of the sys­tem. Even the bot­tle cap ‘op­ti­mizes’ for triv­ial things like main­tain­ing its shape (against the per­tur­ba­tion of elas­tic de­for­ma­tion).  Do you think this will be­come a prob­lem when us­ing this defi­ni­tion for AI? For ex­am­ple, we might find that a par­tic­u­lar pro­gram in­ci­den­tally tends to ‘op­ti­mize’ cer­tain sim­ple mea­sures such as the av­er­age mag­ni­tude of net­work weights, or some other func­tions of weights, loss, policy, etc. to a set point/​range. We may then find slightly more com­plex things be­ing op­ti­mized that look like sub-goals (which could in a cer­tain con­text be un­wanted or dan­ger­ous).  How would we know where to draw the line? It seems like the defi­ni­tion would clas­sify lots of things as op­ti­miza­tion, and it would be up to us to de­cide which kinds are in­ter­est­ing or con­cern­ing and which ones are as triv­ial as the bot­tle cap main­tain­ing its shape.That be­ing said, I re­ally like this defi­ni­tion. I just think it should be ex­tended to clas­sify the in­ter­est­ing­ness of a given op­ti­miza­tion. An AI agent which com­pe­tently pur­sues com­plex goals is a much more in­ter­est­ing op­ti­mizer than a metal bar, even though the bar seems more ro­bust (delet­ing a tiny piece of metal won’t stop it from con­duct­ing; delet­ing a tiny piece in the AI’s com­puter could to­tally dis­able it).Also a nit­pick on the sec­tion about whether the uni­verse is an op­ti­miz­ing sys­tem:I don’t think it is cor­rect to say that the tar­get space is al­most as big as the basin of at­trac­tion. Either:We use area to rep­re­sent the num­ber of macro­scopic states—in this case, the tar­get space is ex­tremely small (one state only(?) -- an ul­tra-low-den­sity bath of par­ti­cles with uniform tem­per­a­ture).  The uni­verse is an ex­tremely pow­er­ful op­ti­mizer from this per­spec­tive, with the caveat that it takes al­most for­ever to achieve its tar­get.We use area to rep­re­sent the num­ber of micro­scopic states (as I think you in­tended).  In this case, I think the tar­get space is ex­actly iden­ti­cal to the basin of at­trac­tion. Low en­tropy microstates are not any less likely than high en­tropy microstates—there just hap­pen to be as­tro­nom­i­cally fewer of them. There is no ‘op­ti­miz­ing force’ push­ing the uni­verse out of these states. From the microstate per­spec­tive, there is no rea­son to ex­clude them from the tar­get zone, since any small and un­re­mark­able sub­set of the tar­get space will dis­play the prop­erty that the sys­tem tends to stum­ble out of it at ran­dom.I would say that the first lens is al­most always bet­ter than the sec­ond, since macro-states are what we ac­tu­ally care about and how we nat­u­rally di­vide the con­figu­ra­tion space of a sys­tem.Fi­nally, just want to say this is an amaz­ing post! I love the style as well as the con­tent. The di­a­grams make it re­ally easy to get an in­tu­itive pic­ture.*Un­sure about the ex­is­tence of ex­cep­tions (can an iso­lated sys­tem be con­trived that fails to reach the global max for en­tropy?)DanielFilan 18 Aug 2020 18:21 UTC LW: 2 AF: 1AF
But Filan would surely agree on this point and his ques­tion is more spe­cific: he is ask­ing whether the liver is an op­ti­mizer.

FYI, it seems pretty clear to me that a liver should be con­sid­ered an op­ti­miser: as an or­gan in the hu­man body, it performs var­i­ous tasks mostly re­li­ably, achieves home­osta­sis, etc. The ques­tion I was rhetor­i­cally ask­ing was whether it is an op­ti­miser of one’s in­come, and the an­swer (I claim) is ‘no’.Pattern 20 Jun 2020 18:22 UTC LW: 2 AF: 1AF the ex­act same an­swer it would have out­put with­out the per­tur­ba­tion. It always gives the same an­swer for the last digit?Alex Flint 27 Jun 2020 18:53 UTC LW: 2 AF: 1AFParentWell we could always just set the last digit to 0 as a post-pro­cess­ing step to en­sure perfect re­peata­bil­ity. But point taken, you’re right that most nu­mer­i­cal al­gorithms are not quite as perfectly sta­ble as I claimed.
mattmacdermott 9 May 2023 9:59 UTC 1 pointAn in­ter­est­ing point about the agency-as-re­tar­getable-op­ti­mi­sa­tion idea is that it seems like you can make the per­tur­ba­tion in var­i­ous places up­stream of the agent’s de­ci­sion-mak­ing, but not down­stream, i.e. you can re­tar­get an agent by per­turb­ing its sen­sors more eas­ily than its ac­tu­a­tors.
For ex­am­ple, to change a ther­mo­stat-con­trol­led heat­ing sys­tem to op­ti­mise for a higher tem­per­a­ture, the most nat­u­ral per­tur­ba­tion might be to turn the tem­per­a­ture dial up, but you could also tam­per with its ther­mis­tor so that it re­ports lower tem­per­a­tures. On the other hand, mak­ing its heat­ing el­e­ment more pow­er­ful wouldn’t af­fect the fi­nal tem­per­a­ture.
I won­der if this sug­gests that an agent’s goal lives in the last place in a causal chain of things you can per­turb to change the set of tar­get states of the sys­tem.Chantiel 15 Aug 2021 19:55 UTC 1 pointAFYou said your defi­ni­tion would not clas­sify a bot­tle cap with wa­ter in it as an op­ti­mizer. This might be re­ally nit-picky, but I’m not sure it’s gen­er­ally true.
I say this be­cause the wa­ter in the bot­tle cap could evap­o­rate. Thus, sup­pos­ing there is no rain, from a wide range of pos­si­ble states of the bot­tle cap, it would tend to­wards no longer hav­ing wa­ter in it.
I know you said you make an ex­cep­tion for ten­den­cies to­wards in­creased en­tropy be­ing con­sid­ered op­ti­miz­ers. How­ever, this does not in­crease the en­tropy of the bot­tle­cap, It could po­ten­tially in­crease the en­tropy of the wa­ter that was in the bot­tle cap, but this is not nec­es­sar­ily the case. For ex­am­ple, if the bot­tle cap is kept in a sealed con­tainer, the wa­ter va­por could po­ten­tially con­dense into a small pud­dle with the same en­tropy as it had in the bot­tle cap.
If my mem­ory of physics is cor­rect, wa­ter evap­o­rat­ing would still in­creases the to­tal en­tropy of the to­tal sys­tem in which the bot­tle cap is lo­cated, by virtue of re­leas­ing some heat into the en­vi­ron­ment . How­ever, note that hu­mans and robots also, merely by do­ing me­chan­i­cal work and thus form­ing heat which is then dis­persed into the en­vi­ron­ment, re­sult in in­creased en­tropy of the sys­tem they’re in. So you can’t rule out any sys­tem that makes its en­vi­ron­ment tend to­wards in­creased en­tropy from be­ing an op­ti­mizer, be­cause that’s what hu­mans and robots do, too.
That said, if you clar­ify that the bot­tle cap is not in any such con­tained sys­tem, I think the wa­ter would re­sult in a higher-en­tropy state.Alex Flint 16 Aug 2021 17:17 UTC LW: 2 AF: 1AFParentThank you for this com­ment Chantiel. Yes, a con­tainer that en­g­ineered to evap­o­rate wa­ter poured any­where into it and con­dense it into a cen­tral area would be an op­ti­miz­ing sys­tem by my defi­ni­tion. That is a bit like a ball rol­ling down a hill, which is also an op­ti­miz­ing sys­tem and also has noth­ing re­sem­bling agency. I am
The bot­tle cap ex­am­ple was ac­tu­ally about putting a bot­tle cap onto a bot­tle and ask­ing whether, since the wa­ter now stays in­side the bot­tle, it should be con­sid­ered an op­ti­mizer. I pointed out that this would not qual­ify as an op­ti­miz­ing sys­tem be­cause if you moved a wa­ter molecule from the bot­tle and place it out­side the bot­tle, the bot­tle cap would not act to put it back in­side.Chantiel 14 May 2021 23:17 UTC 1 pointAn op­ti­miz­ing sys­tem is a sys­tem that has a ten­dency to evolve to­wards one of a set of con­figu­ra­tions that we will call the tar­get con­figu­ra­tion set, when started from any con­figu­ra­tion within a larger set of con­figu­ra­tions, which we call the basin of at­trac­tion, and con­tinues to ex­hibit this ten­dency with re­spect to the same tar­get con­figu­ra­tion set de­spite per­tur­ba­tions.If I’m rea­son­ing cor­rectly, I think this defi­ni­tion could clas­sify just about any­thing as an op­ti­mizer.Con­sider inan­i­mate biolog­i­cal sub­stances, like a leaf. From a wide range of ini­tial con­figu­ra­tions of a leaf, effec­tively all make the leaf evolve to­wards be­ing dirt, be­cause leafs de­com­pose even­tu­ally. Are leaves op­ti­miz­ers?Peo­ple tend to get older and wrin­klier when ag­ing. From a wide range of states, peo­ple would tend to “evolve” to­wards be­ing aged. Are peo­ple op­ti­miz­ers for ag­ing?If the rock is hot­ter than the sur­round­ing aid, vir­tu­ally any ini­tial con­figu­ra­tion of the rock would tend to­wards the rock be­ing some­where around the tem­per­a­ture of the sur­round­ing air. Are rocks op­ti­miz­ers?Sup­pose you have a pro­gram with that shows the user a wel­come and in­for­ma­tion blurb the first time they run the pro­gram, and then won’t show it again. Con­sider the tar­get con­figu­ra­tion to be “pro­gram does not show the wel­come blurb”. The pro­gram would evolve into such a con­figu­ra­tion from any other con­figu­ra­tion. Are wel­come blurbs op­ti­miz­ers?Let us now ex­am­ine a sys­tem that is not an op­ti­miz­ing sys­tem ac­cord­ing to our defi­ni­tion. Con­sider a billiard table with some billiard balls that are cur­rently bounc­ing around in mo­tion. Left alone, the balls will even­tu­ally come to rest in some con­figu­ra­tion. Is this an op­ti­miz­ing sys­tem?In or­der to qual­ify as an op­ti­miz­ing sys­tem, a sys­tem must (1) have a ten­dency to evolve to­wards a set of tar­get con­figu­ra­tions that are small rel­a­tive to the basin of at­trac­tion, and (2) con­tinue to evolve to­wards the same set of tar­get con­figu­ra­tions if per­turbed.If we reach in while the billiard balls are bounc­ing around and move one of the balls that is in mo­tion, the sys­tem will now come to rest in a differ­ent con­figu­ra­tion. There­fore this is not an op­ti­miz­ing sys­tem, be­cause there is no set of tar­get con­figu­ra­tions to­wards which the sys­tem evolves de­spite per­tur­ba­tions. A sys­tem does not need to be ro­bust along all di­men­sions in or­der to be an op­ti­miz­ing sys­tem, but a billiard table ex­hibits no such ro­bust di­men­sions at all, so it is not an op­ti­miz­ing sys­tem.What about tak­ing the tar­get con­figu­ra­tion to be any state in which all the billiard balls are sta­tion­ary? A wide range of states of billiards bounc­ing around on a table would re­sult in all the balls end­ing up sta­tion­ary, so I don’t see how it wouldn’t be clas­sified as an op­ti­miza­tion pro­cess.Also, I’ve made my own at­tempt at defin­ing “op­ti­mizer” here, in case you’re in­ter­ested.Alex Flint 15 May 2021 16:07 UTC 2 pointsParentThank you for this com­ment Chantiel.

Con­sider inan­i­mate biolog­i­cal sub­stances, like a leaf. From a wide range of ini­tial con­figu­ra­tions of a leaf, effec­tively all make the leaf evolve to­wards be­ing dirt, be­cause leafs de­com­pose even­tu­ally. Are leaves op­ti­miz­ers?
Peo­ple tend to get older and wrin­klier when ag­ing. From a wide range of states, peo­ple would tend to “evolve” to­wards be­ing aged. Are peo­ple op­ti­miz­ers for ag­ing?

It’s a good ques­tion. How­ever, the de­com­po­si­tion of a leaf and of the body are both ex­am­ples of in­creases in en­tropy over time, but ac­tu­ally if you look at the size of the “tar­get con­figu­ra­tion set” you find that it’s al­most as big as the whole con­figu­ra­tion space, be­cause most of the con­figu­ra­tions of a sys­tem are high en­tropy con­figu­ra­tions. So I don’t think a leaf or an ag­ing body qual­ify as op­ti­miz­ing sys­tems ac­cord­ing to the defi­ni­tion in this post. See also this sec­tion.

If the rock is hot­ter than the sur­round­ing aid, vir­tu­ally any ini­tial con­figu­ra­tion of the rock would tend to­wards the rock be­ing some­where around the tem­per­a­ture of the sur­round­ing air. Are rocks op­ti­miz­ers?

Well you re­ally have to look at the whole sys­tem. It’s true that if you have a sys­tem that con­sists of a hot part and cold part, the sys­tem over­all will evolve to­wards con­figu­ra­tions in which the parts are the same tem­per­a­ture. But this is again an ex­am­ple of en­tropy in­creas­ing. Most of the con­figu­ra­tions of the joint rock+en­vi­ron­ment sys­tem have the rock and the en­vi­ron­ment at ap­prox­i­mately the same tem­per­a­ture, since if you ran­domly sam­ple a tem­per­a­ture for each par­ti­cle, the large num­ber of par­ti­cles in the rock and the en­vi­ron­ment mean that the av­er­age tem­per­a­ture of all the par­ti­cles in the rock will be very similar to the av­er­age tem­per­a­ture of all the par­ti­cles in the en­vi­ron­ment, with high prob­a­bil­ity.

What about tak­ing the tar­get con­figu­ra­tion to be any state in which all the billiard balls are sta­tion­ary? A wide range of states of billiards bounc­ing around on a table would re­sult in all the balls end­ing up sta­tion­ary, so I don’t see how it wouldn’t be clas­sified as an op­ti­miza­tion pro­cess.

Yes, just like a ball rol­ling down a hill qual­ifies as an op­ti­miz­ing sys­tem, a table with with billiard balls qual­ifies as an op­ti­miz­ing sys­tem in the sense that you point out.
But the whole point of this post is to get past the no­tion of “op­ti­mizer” and “op­ti­miza­tion” to the ex­tent that these con­cepts im­ply that there is some “agent” perform­ing op­ti­miza­tion, and some thing “be­ing op­ti­mized”, which sneaks the agent model into all our think­ing and leads to a very con­fused pic­ture of things.

Also, I’ve made my own at­tempt at defin­ing “op­ti­mizer” here, in case you’re in­ter­ested.

Thank you for the poin­ter!paulfchristiano 15 May 2021 16:20 UTC 2 pointsParentYes, just like a ball rol­ling down a hill qual­ifies as an op­ti­miz­ing sys­tem, a table with with billiard balls qual­ifies as an op­ti­miz­ing sys­tem in the sense that you point out.Both of these ex­am­ples also seem like in­creases in en­tropy if you con­sider the full sys­tem.With a fixed amount of en­ergy, there are a tiny num­ber of ways to use it to make the ball move (or to spend en­ergy putting it some­where other than the bot­tom of the hill) but an ex­po­nen­tially vast num­ber of ways to use it to in­crease the tem­per­a­ture of the billiard ball and table (since there are billions of billions of micro­scopic de­grees of free­dom that could be vibrat­ing or what­ever). Chantiel 15 May 2021 20:46 UTC 1 pointParentThanks for the re­sponse.A lot of the ex­am­ples I pointed out can end up tend­ing to­wards in­creas­ing en­tropy, but I think there are a lot of things that would be con­sid­ered op­ti­mizer that don’t in­crease en­tropy.For ex­am­ple, con­sider a leaf out in the sun, dry­ing out and go­ing from a green­ish color to a yel­low one. Pretty much all con­figu­ra­tions of the leaf would re­sult in the leaf get­ting more yel­low over time. Is the leaf op­ti­miz­ing for yel­low-ness?What about a knife that is be­ing used and never sharp­ened? From a wide range of con­figu­ra­tions the knife would tend to­wards get­ting dul­ler. Is it op­ti­miz­ing dul­l­ness?What about a space­ship leav­ing Earth? Is it op­ti­miz­ing for the dis­tance from Earth?I sup­pose we could con­sider these things op­ti­miz­ers if you re­ally want to. But I’m con­cerned that a defi­ni­tion that in­clude leaves, knives, billiard balls, and rocket ships is overly broad.More gen­er­ally, it seems like this defi­ni­tion clas­sifies a lot of things that change in some way over time as an op­ti­mizer. In gen­eral, if some­thing tends to be differ­ent in some ways when it’s young than old, then I think you can say the sys­tem is an op­ti­mizer op­ti­miz­ing for what­ever char­ac­ter­is­tics cor­re­late with old­ness. Joe_Collman 30 Jun 2020 18:04 UTC 1 pointGreat post.I’m not keen on the re­quire­ment that the basin of at­trac­tion be strictly larger than the tar­get con­figu­ra­tion set. I don’t think this buys you much, and seems to need­lessly rule out goals based on nar­row main­te­nance of some sta­tus-quo. Switch­ing to a util­ity func­tion as sug­gested by oth­ers im­proves things, I think.For ex­am­ple: a highly ca­pa­ble AI whose only goal is to main­tain a chess set in a par­tic­u­lar po­si­tion for as long as pos­si­ble, but not to care about it af­ter it’s dis­turbed.Here the tar­get set is iden­ti­cal to the basin of at­trac­tion: states con­tain­ing the chess set in the par­tic­u­lar po­si­tion (or his­to­ries where it’s re­mained undis­turbed).This doesn’t tell us any­thing about what the AI will do in pur­su­ing this goal. It may not do much un­til some­thing ap­proaches the board; it may re-ar­range the galaxy to min­imise the chances that a piece will be moved (but ar­bi­trar­ily small en­vi­ron­men­tal changes might have it take very differ­ent ac­tions, so in gen­eral we can’t say it’s op­ti­mis­ing for some par­tic­u­lar con­figu­ra­tion of the galaxy).I want to say that this sys­tem is op­ti­mis­ing to keep the chess set undis­turbed.With util­ity you can eas­ily rep­re­sent this goal, and all you need to do is com­pare un­per­turbed util­ity with the util­ity un­der var­i­ous per­tur­ba­tions.Some­thing like: The sys­tem S op­ti­mises U 𝛿-ro­bustly to per­tur­ba­tion x if E[U(S)] - E[U(x(S))] < 𝛿Back to top","






A research workflow with Zotero and Org mode | mkbehr.com



















Skip to main content





Toggle navigation





mkbehr.com






About me


Archive


Tags


RSS feed


Github














Source










A research workflow with Zotero and Org mode


                    Michael Behr
            
September 19, 2015

Comments

Source






Any research project is going to involve a literature search: reading
through a bunch of papers that might be relevant to your topic in
order to get a sense of what the field already knows. Now, maybe
there's some magic technique for picking out the information that
matters, passing over the rest, and writing out a single, coherent
story in one pass through all the papers you can find. If that
technique exists, I have no idea what it is.
So when every paper brings up ten new questions and twenty papers to
start answering them, I need a system to keep my notes organized. I
need notes that let me jump back and forth between papers without
losing my place, draw links between papers, and store lists of
citations to come back to. Here's how I do it.

Storing papers with Zotero
The first tool I use is Zotero, a reference
manager. Zotero's job is to store all the actual papers I come across,
along with information like data on how to cite the papers and any
tags they might have been published with. It can grab that information
from my web browser, whether from a journal's website or someplace
like Google Scholar or PubMed. It's also great for quickly putting
together a bibliography, using bibtex or similar programs, when I want
to write up some results.



Zotero stores the papers I want to
read and reference. I scaled up the font size here to make it readable
in a tiny blog image.
Zotero isn't the only choice for reference managers.
Mendeley is another popular choice, and
there are a
whole bunch more
out there. I picked Zotero arbitrarily a few years ago, but it's
working out well because of its emacs integration.
Keeping notes with Emacs and Org mode
You see, Zotero has some note-taking functions, and I used to keep my
notes there, but there were some problems. Notes are stored as
separate files for each paper, but I want to cross-reference notes
from a lot of different papers at once. And while the editor has some
rich-text capabilities (e.g. bold and italic text), it's missing
important things I need in my notes, like the ability to typeset
equations.
That's where Emacs and its
extension Org mode come in. To borrow a term
from Perl enthusiasts, Org mode is the swiss army chainsaw of text
document formats. Org mode documents have a lot of features, and it's
way beyond this post's scope to describe them all. For the purpose of
research notes, the most useful things it lets me do are:

I can store my notes in a hierarchical tree structure, and I can
  hide parts of the tree from view in order to focus on other parts.
I can put hyperlinks into my notes, including links to papers,
  websites, or other parts of the file.
I can put math in my notes using Latex, and view the typeset
  equations right in my Emacs buffer.




A sample from my notes file. You
can see the tree structure of the file, some links to papers, and a
little bit of inline math, using Latex.
Gluing it all together with zotxt
Now, see those links to papers in my notes buffer? I didn't have to
copy and paste them from anywhere. I inserted them with just three
keystrokes each. So far, I've just described some useful pieces of
software, but the interesting part of my workflow is how they fit
together.
zotxt is an extension that lets
other programs talk to Zotero, and Emacs has a package to talk to it.
It's even structured specifically to work with Org mode documents.
With zotxt, my workflow looks like this:

I find a paper I want to look at somewhere on the internet.
I use Zotero's browser plugin to save it to Zotero. Hopefully it
  grabs the paper itself and this happens in one click; if the site
  doesn't play along, I spend a minute grabbing a pdf and feeding it
  to Zotero.
I insert a link to the Zotero entry into my notes file in Emacs. I
  can do this with the key chords C-c "" "". I don't need to further
  specify what paper I want to grab: the browser plugin leaves the
  paper selected in Zotero, and zotxt can grab the selected paper.
When I want to read the paper, I go to the link and tell Emacs to
  open the paper in my system PDF viewer. The key chords for this are
  C-c "" a, and then selecting the PDF attachment from the Helm
  window that appears (usually I just type pdf RET).
When I'm reading a paper and see a citation that might be useful, I
  look it up on the internet and repeat this process to store a note
  linking to it.

It took me a while to get it set up to my liking, so here's how I did
it:

First, install zotxt. If you're
  using Zotero as a firefox extension, you just need to install zotxt
  as another extension. If you're using the standalone Zotero client,
  you can still do it: download the extension file from that link,
  then go to the Add-Ons Manager under the Tools menu and find the
  option to install an add-on from a file.




The menu option looks like
this.

Next, install the zotxt package in emacs. If your
  package manager is set up, you
  can just type M-x package-install RET zotxt RET.
Now, when org-zotxt-mode is active, you can use its functions in
  your org-mode buffers. You can search for papers and insert them
  with C-c "" i, insert the currently-selected paper in Zotero with
  C-u C-c "" i, and open a paper's PDF or other related files by
  moving the cursor to a link and typing C-c "" a. However, you might
  want a little bit more setup to deal with some annoyances.
You probably want to have org-zotxt-mode automatically activated
  in all your org-mode documents. To make that happen, you can add
  some code to your .emacs file to start up this mode on all your
  org-mode buffers - see below this list for the .emacs
  configuration I use.
If you want to insert a link to the currently-selected item a lot,
  C-u C-c "" i is an awkward sequence to type. I rebound it to C-c ""
  "".
You might notice that when you insert a link to a paper, the text of
  that link is a full citation. That might be what you want, but I
  just want the authors, paper name, and year. It took me a bit of
  hacking to get around that: it's possible to tell the emacs zotxt
  interface to use a different citation format than the default, but I
  had to throw together a little XML file to give it a shorter format
  than a full citation. (This may not be the easiest or cleanest way
  to do it, but it works!)
  That XML file is here. To use it, go into
  your Zotero preferences and select Cite -> Styles, and add the file.
  It should appear in the menu as ""mkbehr's short reference format"".
  Then add the last two lines in the .emacs snippet below, and you
  should get shorter citations.
You probably want to install the
  Helm package, to make zotxt's
  search interface easier to navigate. That link should tell you
  everything you need to know.

Here's that .emacs setup code:
;; Activate org-zotxt-mode in org-mode buffers
(add-hook 'org-mode-hook (lambda () (org-zotxt-mode 1)))
;; Bind something to replace the awkward C-u C-c "" i
(define-key org-mode-map
  (kbd ""C-c \"" \"""") (lambda () (interactive)
                      (org-zotxt-insert-reference-link '(4))))
;; Change citation format to be less cumbersome in files.
;; You'll need to install mkbehr-short into your style manager first.
(eval-after-load ""zotxt""
'(setq zotxt-default-bibliography-style ""mkbehr-short""))

Of course, I'm not done tinkering to make my workflow better. I hear
good things about the org-ref
and helm-bibtex
packages - if only I can keep an up-to-date bibtex file as I add papers
to my library, I can associate links with not only a paper's pdf, but
also that paper's section of my notes file. And I haven't found a
smooth way to take a paper and pull up the papers it cites in my
browser. But until then, I'm pretty happy with this setup.
Happy researching!



emacs
research



Previous post


Next post

Comments

Please enable JavaScript to view the comments powered by Disqus.

Comments powered by Disqus




            Contents © 2015         Michael Behr - Powered by         Nikola






"
9,"









Rent Seeking - Econlib

































































Liberty Fund Network

Econlib
Liberty Fund
OLL
Adam Smith Works
Law & Liberty















EconLog

Blog
Browse by Author
Browse by Topic
Search EconLog
RSS
Subscribe


EconTalk

Latest Episodes
Browse by Date
Browse by Guest
Browse by Category
Browse Extras
Search EconTalk
RSS Feeds


Articles

Latest Articles
Liberty Classics
Browse by Author
Browse by Date
Search Articles


Books

Books
Bios
Books by Date
Books by Author
Search Books


Encyclopedia

Index
Browse by Author
Browse by Title
Biographies
Search Encyclopedia


Guides

Index
#ECONLIBREADS
College Topics
High School Topics
Subscribe to QuickPicks
Search Guides


Videos

Index
Search Videos


Liberty Fund Network

Econlib
OLL
Adam Smith Works
Library of Law & Liberty














Home  /  








ECONLIB CEE


                                Government Policy                            











Rent Seeking

							By David R. Henderson						




								Categories:
																	     Government Policy


	
								By David R. Henderson, 
															


                                    SHARE
                                    
                                    POST:
                                













“
Rent seeking” is one of the most important insights in the last fifty years of economics and, unfortunately, one of the most inappropriately labeled. Gordon Tullock originated the idea in 1967, and Anne Krueger introduced the label in 1974. The idea is simple but powerful. People are said to seek rents when they try to obtain benefits for themselves through the political arena. They typically do so by getting a subsidy for a good they produce or for being in a particular class of people, by getting a tariff on a good they produce, or by getting a special regulation that hampers their competitors. Elderly people, for example, often seek higher Social Security payments; steel producers often seek restrictions on imports of steel; and licensed electricians and doctors often lobby to keep regulations in place that restrict competition from unlicensed electricians or doctors.



But why do economists use the term “rent”? Unfortunately, there is no good reason. David Ricardo introduced the term “rent” in economics. It means the payment to a factor of production in excess of what is required to keep that factor in its present use. So, for example, if I am paid $150,000 in my current job but I would stay in that job for any salary over $130,000, I am making $20,000 in rent. What is wrong with rent seeking? Absolutely nothing. I would be rent seeking if I asked for a raise. My employer would then be free to decide if my services are worth it. Even though I am seeking rents by asking for a raise, this is not what economists mean by “rent seeking.” They use the term to describe people’s lobbying of government to give them special privileges. A much better term is “privilege seeking.”


It has been known for centuries that people lobby the government for privileges. Tullock’s insight was that expenditures on lobbying for privileges are costly and that these expenditures, therefore, dissipate some of the gains to the beneficiaries and cause inefficiency. If, for example, a steel firm spends one million dollars lobbying and advertising for restrictions on steel imports, whatever money it gains by succeeding, presumably more than one million, is not a net gain. From this gain must be subtracted the one-million-dollar cost of seeking the restrictions. Although such an expenditure is rational from the narrow viewpoint of the firm that spends it, it represents a use of real resources to get a transfer from others and is therefore a pure loss to the economy as a whole.


Krueger (1974) independently discovered the idea in her study of poor economies whose governments heavily regulated their people’s economic lives. She pointed out that the regulation was so extensive that the government had the power to create “rents” equal to a large percentage of national income. For India in 1964, for example, Krueger estimated that government regulation created rents equal to 7.3 percent of national income; for Turkey in 1968, she estimated that rents from import licenses alone were about 15 percent of Turkey’s gross national product. Krueger did not attempt to estimate what percentage of these rents were dissipated in the attempt to get them. Tullock (1993) tentatively maintained that expenditures on rent-seeking in democracies are not very large.



About the Author

David R. Henderson is the editor of this encyclopedia. He is a research fellow with Stanford University’s Hoover Institution and an associate professor of economics at the Naval Postgraduate School in Monterey, California. He was formerly a senior economist with President Ronald Reagan’s Council of Economic Advisers.




Further Reading
 
Krueger, Anne O. “The Political Economy of the Rent-Seeking Society.” American Economic Review 64 (1974): 291–303.
Tullock, Gordon. Rent Seeking. Brookfield, Vt.: Edward Elgar, 1993.
Tullock, Gordon. “The Welfare Costs of Tariffs, Monopolies and Theft.” Western Economic Journal 5 (1967): 224–232.
 






RELATED
                                    CONTENT 
Don Boudreaux on Public Choice

Don Boudreaux of George Mason University talks with EconTalk host Russ Roberts about public choice: the application of economics to the political process. Boudreaux argues that political competition is a blunt instrument that works less effectively than economic competition. One reason for this bluntness is the voting process itself--where intensity does not matter, only whether a voter prefers one candidate to the other. A second reason is that political outcomes tend to be one-size-fits-all, w...

Read This
                                    Article








                            SHARE
                            
                            POST:
                        









Enter your email address to subscribe to our monthly newsletter:














RELATED
                                    CONTENT 
Don Boudreaux on Public Choice

Don Boudreaux of George Mason University talks with EconTalk host Russ Roberts about public choice: the application of economics to the political process. Boudreaux argues that political competition is a blunt instrument that works less effectively than economic competition. One reason for this bluntness is the voting process itself--where intensity does not matter, only whether a voter prefers one candidate to the other. A second reason is that political outcomes tend to be one-size-fits-all, w...

Read This
                                    Article






COLLECTION: GOVERNMENT POLICY




                                        The article you’re reading is part of Econlib’s Government Policy                                        collection. Explore other
                                        Government Policy                                        articles:
                                    



Feb 5 2018
Hoover's Economic Policies

Steven Horwitz                                                



Feb 5 2018
Unemployment Insurance

David Francis                                                



Feb 5 2018
Third World Debt

Kenneth Rogoff                                                



Feb 5 2018
Trucking Deregulation

Thomas Gale Moore                                                












Econlib


The Library of Economics and Liberty
Liberty Fund, Inc.
11301 N. Meridian Street
Carmel, IN 46032-4564, USA
econlib@libertyfund.org


About
About Us
Contact Us
Privacy Policy
 

Publications
Books
Articles
EconTalk
EconLog
Videos
 

Resources
Quickpicks
CEE Encyclopedia
College Guides
High School Guides
 


Sign up for our newsletter
Enter your email address to subscribe to the Econlib monthly newsletter.





»




Liberty Fund, Inc.
11301 N. Meridian Street
Carmel, IN 46032-4564, USA
info@libertyfund.org






 





© 2023 Econlib, Inc. All Rights Reserved. Part of the Liberty Fund Network.


























","




Off the Convex Path – Off the convex path






























About

Subscribe









Off the Convex Path


Contributors

Sanjeev Arora
Nisheeth Vishnoi
Nadav Cohen

Former contributors:

Moritz Hardt

Mission statement
The notion of convexity underlies a lot of beautiful mathematics. When combined with computation, it gives rise to the area of convex optimization that has had a huge impact on understanding and improving the world we live in. However, convexity does not provide all the answers. Many procedures in statistics, machine learning and nature at large—Bayesian inference, deep learning, protein folding—successfully solve non-convex problems that are NP-hard, i.e., intractable on worst-case instances. Moreover, often nature or humans choose methods that are inefficient in the worst case to solve problems in P.
Can we develop a theory to resolve this mismatch between reality and the predictions of worst-case analysis?  Such a theory could identify structure in natural inputs that helps sidestep worst-case complexity.
This blog is dedicated to the idea that optimization methods—whether created by humans or nature, whether convex or nonconvex—are exciting objects of study and, often lead to useful algorithms and insights into nature. This study can be seen as an extension of classical mathematical fields such as dynamical systems and differential equations among others, but with the important addition of the notion of computational efficiency.
We will report on interesting research directions and open problems, and highlight progress that has been made. We will write articles ourselves as well as encourage others to contribute. In doing so, we hope to generate an active dialog between theorists, scientists and practitioners and to motivate a generation of young researchers to work on these important problems.
Contributing an article
If you’re writing an article for this blog, please follow these guidelines.








      Theme available on Github.
    








"
